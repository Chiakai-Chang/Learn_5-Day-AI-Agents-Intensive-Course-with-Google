### 🎙️ 第 18 頁：Agent Ops：指標、開發與偵錯

#### 【本頁重點摘要】
*   **衡量關鍵指標**：專注於能反映真實商業價值的 KPI，而不僅是技術正確性。
*   **讓大型語言模型（LM）擔任裁判**：使用 LM 根據預定義的標準，自動化地評估代理人的輸出品質。
*   **指標驅動開發**：依據自動化品質分數，建立部署新版本的「Go/No-Go」決策流程。
*   **使用追蹤日誌（Traces）偵錯**：利用 OpenTelemetry 記錄代理人的完整執行路徑，以診斷問題根源。
*   **珍視人類回饋**：將使用者的回饋轉化為永久性的測試案例，持續強化系統。

---

#### 【逐字講稿】

(開場白)
好的，我們剛剛了解了 Agent Ops 是一種管理「不可預測性」的全新營運哲學。現在，讓我們深入探討支撐這個理念的五個核心實踐，也就是從「是什麼」進到「該怎麼做」。

##### ① 首先，衡量真正重要的事 (Measure What Matters)
在 Agent Ops 的世界裡，我們不能只問「代理人有沒有壞掉？」。我們必須像在做 A/B 測試一樣，問自己：**它是否真的創造了價值？**

> 這意味著我們的關鍵績效指標 (KPI) 必須超越技術層面，去衡量真實世界的影響。例如：**目標完成率**、**用戶滿意度分數**、任務處理的**延遲時間**、每次互動的**營運成本**，以及最重要的，它對公司營收或客戶留存率帶來了什麼改變。

##### ② 第二，讓大型語言模型擔任裁判 (Use an "LM as Judge")
傳統軟體的「通過／不通過」測試在這裡行不通，因為語言是充滿彈性的。所以，我們轉而評估「品質」。

我們使用一個強大的語言模型，就像一位公正的法官，根據一份預先定義好的「評分標準」來評估代理人的表現。這份標準會問：它的回答正確嗎？它的論述是否**基於事實**？它是否遵循了所有指示？這個自動化評估流程會在一套「黃金標準資料集」上運行，為我們提供一個穩定、一致的品質分數。

##### ③ 第三，指標驅動開發 (Metrics-Driven Development)
一旦我們有了自動化的品質分數，開發流程就不再是憑感覺了。

這個過程很簡單：當我們要推出新版本時，就讓它在整個評估資料集上跑一遍，然後將它的分數與現有的正式版本直接比較。這個系統消除了所有猜測，讓我們對每一次部署都充滿信心。為了追求最高的安全性，我們通常還會搭配 **A/B 部署**，逐步推出新版本，並同時比較真實世界的指標和模擬分數。

##### ④ 第四，用追蹤日log (Traces) 進行偵錯
當你的指標下滑，或使用者回報一個 bug，你最需要回答的問題就是「為什麼？」。

> OpenTelemetry 的「Trace」就是解答。它就像一個高解析度的飛行紀錄器，一步步記錄下代理人完整的執行軌跡。

透過 Trace，你可以看到模型收到的**確切提示**、它內部的推理過程、它選擇呼叫的**特定工具**、為該工具生成的**精確參數**，以及工具回傳的原始數據。所有細節一覽無遺，讓你能夠精準診斷問題的根源。

##### ⑤ 最後一點，也最重要的一點：珍視人類的回饋
人類的回饋不是待辦清單上的麻煩事；它是你最寶貴、資訊密度最高的資源。

當使用者按下「不喜歡」的按鈕時，他們其實是送給你一份禮物：一個你的自動化測試腳本沒有涵蓋到的、真實世界的「邊界案例」。我們的任務就是「閉合這個循環」：捕捉這些回饋、重現問題，然後將這個場景轉化為一個新的、**永久性的測試案例**。

> 這就像是為你的系統**接種疫苗**，確保它不僅修復了眼前的 bug，更能對這一整類的錯誤永遠免疫。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講完這五點後，可以稍微停頓一下，讓資訊沉澱。這五個實踐是 Agent Ops 的核心工作循環。
*   **強化比喻**：「為系統接種疫苗」這個比喻非常有力，可以加強語氣來強調「從錯誤中學習並永久免疫」的概念。
*   **轉場橋樑 (Bridge)**：我們已經建立了一個高品質、可偵錯、且能透過回饋持續進步的代理人。但它並非孤立存在。下一步，就是將它與更廣闊的世界連結起來。
    > 那麼，一個成熟的代理人該如何與人類、其他代理人，甚至是金融系統進行互動呢？下一頁，我們將探討「代理人的互操作性」。