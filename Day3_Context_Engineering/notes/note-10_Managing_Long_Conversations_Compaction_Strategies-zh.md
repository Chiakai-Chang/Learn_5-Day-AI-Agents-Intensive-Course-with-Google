```markdown
### 🎙️ 第 10 頁：Managing_Long_Conversations_Compaction_Strategies

#### 【本頁重點摘要】
*   **問題核心**：隨著對話變長，歷史紀錄會超出模型的上下文限制，並導致成本、延遲和雜訊增加。
*   **核心比喻**：這就像打包行李箱，成功不在於你能帶多少，而在於你只帶了你需要的東西。
*   **解決策略**：介紹三種主要的對話歷史壓縮方法：保留最近 N 輪、基於 Token 的截斷、以及遞歸式摘要。

---

#### 【逐字講稿】

(開場白)
好，我們剛剛談完了 Session，也就是對話的「工作台」。但如果一場對話持續很久，這個工作台上就會堆滿越來越多的資料。這會引發一個非常現實的問題：我們要如何管理這些不斷增長的對話歷史呢？

這頁投影片的比喻非常貼切，大家可以想像一下，**管理一個長對話，就像一位聰明的旅行者在為長途旅行打包行李箱。**

> 如果你試圖把所有東西都塞進去，行李箱會變得又重又亂，很難快速找到你需要​​的東西——這就像一個超載的上下文視窗，會增加處理成本並減慢回應時間。
>
> 但反過來說，如果你打包得太少，你可能會漏掉像護照或外套這樣的重要物品，導致整個旅程出問題——這就像一個 Agent 失去了關鍵的上下文，給出不相關或錯誤的答案。

所以，無論是旅行者還是 AI Agent，成功的關鍵都不在於你能攜帶多少東西，而在於 **你只攜帶你真正需要的東西**。這就是「壓縮 (Compaction)」的核心思想。

那麼，為什麼我們需要這麼費心去壓縮對話歷史呢？主要有四個原因：

##### ① Context Window 限制
每個 LLM 都有它能一次處理的最大文字量，也就是「上下文視窗」。如果對話歷史超過這個限制，API 呼叫就會直接失敗。

##### ② API 成本
大多數 LLM 服務是按 Token 數量收費的。更短的歷史意味著更少的 Token，也意味著每一輪對話的成本更低。

##### ③ 延遲與速度
傳送給模型的文字越多，處理時間就越長，用戶等待回應的時間也就越久。壓縮能讓我們的 Agent 感覺更敏捷、反應更快速。

##### ④ 回應品質
最後，當 Token 數量增加時，模型可能會因為上下文中的「雜訊」過多而表現變差，這種現象有時被稱為「上下文腐爛 (context rot)」，模型的注意力會被分散，難以聚焦在最重要的資訊上。

為了解決這些問題，我們有幾種核心的壓縮策略：

*   **第一種，最簡單的，就是「保留最近 N 輪對話」**。這就像一個滑動的視窗，我們只保留最近的幾次互動，然後把更早的內容全部丟掉。
*   **第二種，是「基於 Token 的截斷」**。在每次發送請求前，系統會從最新的訊息開始往回計算 Token 數量，直到達到一個預設的上限（比如 4000 個 Token）。超出這個範圍的舊訊息就會被直接切掉。
*   **第三種，也是最精密的，叫做「遞歸式摘要」**。這個方法會用另一個 LLM 呼叫，將對話中較早的部分生成一個摘要。隨著對話越來越長，系統會定期地把最舊的訊息「總結」起來，用這個濃縮後的摘要來代替原本冗長的對話紀錄。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在講完「打包行李箱」的比喻後，可以稍微停頓一下，確保聽眾理解了這個核心概念。
*   **補充案例**：如果時間允許，可以提到 `source_file` 中有 ADK 的程式碼範例，展示了如何用一個簡單的插件來實現「保留最近 N 輪對話」的功能，這能讓概念更具體。
*   **轉場橋樑 (Bridge)**：
    > 我們現在知道了如何聰明地「打包行李箱」，透過壓縮策略來管理對話的長度。但問題來了：那些被我們篩選出來、認為「至關重要」的資訊，該如何被長期保存與利用呢？這就引出了我們 Context Engineering 的第二個核心組件——**Memory**。
```