<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PPTPlaner Guide</title>
    <style>
        body { font-family: sans-serif; line-height: 1.6; margin: 0; padding: 20px; background-color: #f4f4f4; color: #333; }
        .container { max-width: 1200px; margin: auto; background: white; padding: 20px; box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 8px; }
        .page { display: flex; border-bottom: 2px solid #eee; padding: 20px 0; }
        .slide { flex: 1; padding-right: 20px; border-right: 1px solid #ddd; min-width: 0; } /* Fix: Added min-width */
        .notes { flex: 1; padding-left: 20px; min-width: 0; } /* Fix: Added min-width */
        h1, h2 { border-bottom: 1px solid #ddd; padding-bottom: 10px; color: #444; }
        pre { background: #f9f9f9; padding: 15px; border-radius: 5px; white-space: pre-wrap; word-wrap: break-word; border: 1px solid #eee; font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace; }
        
        .note-toggle { margin-bottom: 15px; }
        .toggle-btn { padding: 8px 15px; border: 1px solid #ccc; background-color: #f0f0f0; cursor: pointer; border-radius: 5px; margin-right: 5px; }
        .toggle-btn.active { background-color: #007bff; color: white; border-color: #007bff; }

        /* Generic styles for rendered markdown content */
        .rendered-content h1, .rendered-content h2, .rendered-content h3, .rendered-content h4, .rendered-content h5, .rendered-content h6 { border-bottom: none; padding-bottom: 5px; margin-top: 20px; }
        .rendered-content ul, .rendered-content ol { padding-left: 25px; }
        .rendered-content code { background-color: #eee; padding: 2px 5px; border-radius: 3px; font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace; }
        .rendered-content blockquote { border-left: 4px solid #ccc; padding-left: 15px; color: #666; margin-left: 0; }
        .rendered-content table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .rendered-content th, .rendered-content td { border: 1px solid #ddd; padding: 8px; }
        .rendered-content th { background-color: #f2f2f2; }
        .rendered-content img { max-width: 100%; height: auto; display: block; margin: 1em 0; }
    </style>
</head>
<body>
    <div class="container">
        <h1>PPTPlaner Guide</h1>
        <p>This guide displays the generated slides and notes side-by-side. Use the buttons to toggle between rendered and source view for the notes.</p>
        <hr>
        
        <div class="page" id="page-1">
            <div class="slide">
                <h2>Slide 01</h2>
                <div class="rendered-content">
                    <h1>Context Engineering: Sessions, and Memory</h1>
<p><strong>Authors:</strong> Kimberly Milam and Antonio Gulli</p>
<p><strong>A deep-dive into building stateful, intelligent LLM agents.</strong></p>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 01</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(1, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(1, 'raw')">Source</button>
                </div>
                <div id="note-rendered-1" class="note-content rendered-content">
                    <h3>🎙️ 第 01 頁：Title_Context_Engineering,_Sessions,_and_Memory</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>點出當前 AI 的核心困境：大型語言模型 (LLM) 天生是「無狀態」的，無法記憶互動。</li>
<li>提出解決方案的總稱：「情境工程 (Context Engineering)」，這是打造智慧型代理人的關鍵。</li>
<li>預告本次深潛的兩大核心主題：對話管理 (Sessions) 與長期記憶 (Memory)。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(各位好，歡迎大家。在我們開始之前，我想先問一個問題：大家有沒有覺得，跟現在的 AI 聊天，常常像是在跟一隻金魚對話？你每次都得重頭介紹自己，解釋前因後果。這就是因為，當今絕大多數的 AI，天生都是「失憶」的。)</p>
<h5>① 介紹主題與願景</h5>
<p>我們今天要探討的，正是如何解決這個問題。如何賦予 AI <strong>記憶</strong>與<strong>智慧</strong>，讓它們從一個個健忘的工具，進化成能夠記住你、理解你、並且持續學習的「個人化智慧代理人」(Intelligent Agent)。</p>
<blockquote>
<p>這一切的起點，都源自於一個核心概念，也就是我們今天的主題：<strong>情境工程 (Context Engineering)</strong>。</p>
</blockquote>
<h5>② 介紹作者與深度剖析</h5>
<p>本次的分享，是基於 Kimberly Milam 與 Antonio Gulli 的權威白皮書。我們將會進行一場「<strong>深度剖析</strong>」，帶領各位了解，如何透過「<strong>對話 (Sessions)</strong>」與「<strong>記憶 (Memory)</strong>」這兩大支柱，來打造真正具有狀態、能夠個人化的 AI 體驗。這是一趟通往更強大、更具持續性 AI 體驗的旅程。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：在開場提問後，可以稍微停頓 1-2 秒，讓聽眾思考並產生共鳴。</li>
<li><strong>補充案例</strong>：可以簡單舉例，例如「你每次都要重新告訴 AI 你的專案背景」或「你希望 AI 能記住你的寫作風格」，讓問題更具體。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>我們剛剛提到了 AI 的「失憶症」。那麼，這個問題到底有多根本？為什麼說打造「有狀態的 AI」是下一個世代的挑戰？下一頁，我們就來深入探討這個需求的核心。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-1" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 01 頁：Title_Context_Engineering,_Sessions,_and_Memory

#### 【本頁重點摘要】
*   點出當前 AI 的核心困境：大型語言模型 (LLM) 天生是「無狀態」的，無法記憶互動。
*   提出解決方案的總稱：「情境工程 (Context Engineering)」，這是打造智慧型代理人的關鍵。
*   預告本次深潛的兩大核心主題：對話管理 (Sessions) 與長期記憶 (Memory)。

---

#### 【逐字講稿】

(各位好，歡迎大家。在我們開始之前，我想先問一個問題：大家有沒有覺得，跟現在的 AI 聊天，常常像是在跟一隻金魚對話？你每次都得重頭介紹自己，解釋前因後果。這就是因為，當今絕大多數的 AI，天生都是「失憶」的。)

##### ① 介紹主題與願景
我們今天要探討的，正是如何解決這個問題。如何賦予 AI **記憶**與**智慧**，讓它們從一個個健忘的工具，進化成能夠記住你、理解你、並且持續學習的「個人化智慧代理人」(Intelligent Agent)。

> 這一切的起點，都源自於一個核心概念，也就是我們今天的主題：**情境工程 (Context Engineering)**。

##### ② 介紹作者與深度剖析
本次的分享，是基於 Kimberly Milam 與 Antonio Gulli 的權威白皮書。我們將會進行一場「**深度剖析**」，帶領各位了解，如何透過「**對話 (Sessions)**」與「**記憶 (Memory)**」這兩大支柱，來打造真正具有狀態、能夠個人化的 AI 體驗。這是一趟通往更強大、更具持續性 AI 體驗的旅程。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在開場提問後，可以稍微停頓 1-2 秒，讓聽眾思考並產生共鳴。
*   **補充案例**：可以簡單舉例，例如「你每次都要重新告訴 AI 你的專案背景」或「你希望 AI 能記住你的寫作風格」，讓問題更具體。
*   **轉場橋樑 (Bridge)**：
    > 我們剛剛提到了 AI 的「失憶症」。那麼，這個問題到底有多根本？為什麼說打造「有狀態的 AI」是下一個世代的挑戰？下一頁，我們就來深入探討這個需求的核心。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-2">
            <div class="slide">
                <h2>Slide 02</h2>
                <div class="rendered-content">
                    <h1>The Need for Stateful AI</h1>
<ul>
<li>
<p><strong>Problem:</strong> Large Language Models (LLMs) are inherently stateless. Their awareness is confined to the context window of a single API call.</p>
</li>
<li>
<p><strong>Challenge:</strong> How can we build agents that remember, learn, and personalize interactions over time?</p>
</li>
<li>
<p><strong>Solution:</strong> <strong>Context Engineering</strong>—the dynamic assembly and management of information within an LLM's context window.</p>
</li>
<li>
<p><strong>Core Components:</strong></p>
<ul>
<li><strong>Sessions:</strong> Manage the immediate, turn-by-turn state of a single conversation.</li>
<li><strong>Memory:</strong> Provides long-term persistence, capturing key information across multiple sessions.</li>
</ul>
</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 02</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(2, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(2, 'raw')">Source</button>
                </div>
                <div id="note-rendered-2" class="note-content rendered-content">
                    <h3>🎙️ 第 02 頁：Introduction_The_Need_for_Stateful_AI</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li><strong>核心問題</strong>：大型語言模型 (LLM) 天生是「無狀態」的，它們的記憶僅限於單次的 API 呼叫，無法記住過去的對話。</li>
<li><strong>面臨挑戰</strong>：我們該如何打造能夠記憶、學習並提供個人化互動的 AI 代理人 (Agent)？</li>
<li><strong>解決方案</strong>：答案是「脈絡工程」(Context Engineering)，一種動態組合與管理 LLM 脈絡視窗資訊的方法。</li>
<li><strong>兩大基石</strong>：這個方案主要由兩個核心組件構成——「會話 (Sessions)」和「記憶 (Memory)」。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
大家好，我們今天探討的主題，是如何讓 AI 從一個健忘的工具，進化成一個能記住你、了解你的智慧夥伴。這一切，都始於我們需要「有狀態的 AI」。</p>
<h5>① 核心困境：大型語言模型的「失憶症」</h5>
<p>首先，我們必須理解一個根本問題：現今所有的大型語言模型，本質上都是<strong>無狀態 (stateless)</strong> 的。這代表什麼呢？</p>
<blockquote>
<p>這意味著，除了訓練資料之外，它們的推理和意識，完全被限制在單次 API 呼叫的「脈絡視窗 (context window)」之內。</p>
</blockquote>
<p>簡單來說，每一次你跟它互動，都是一次全新的開始。它不記得你五分鐘前問過什麼，也不記得你的偏好。這就是為什麼有時候跟 AI 聊天，會感覺像在跟一個有「失憶症」的對象說話。這是一個根本性的障礙，限制了我們打造真正智慧體驗的可能性。</p>
<h5>② 挑戰與解方：從「無狀態」到「有狀態」</h5>
<p>所以，我們面臨的<strong>挑戰</strong>非常明確：我們該如何打造一個能夠<strong>記憶</strong>、<strong>學習</strong>，並隨著時間推移<strong>個人化</strong>互動的代理人呢？</p>
<p>答案，就是我們今天將深入探討的核心概念——<strong>脈絡工程 (Context Engineering)</strong>。</p>
<p>這不是一個花俏的術語，而是一套具體的方法論。根據白皮書的定義，脈絡工程是「在 LLM 的脈絡視窗內，動態地組合與管理資訊，以實現有狀態、智慧化的代理人。」</p>
<h5>③ 兩大核心組件：會話 (Sessions) 與記憶 (Memory)</h5>
<p>而要實現脈絡工程，我們需要依賴兩個關鍵的組件：</p>
<ul>
<li>第一個是 <strong>會話 (Sessions)</strong>：它負責管理當下這一次對話的即時狀態，就像是我們對話中的短期記憶。</li>
<li>第二個是 <strong>記憶 (Memory)</strong>：它提供長期的持久性，能夠跨越多次不同的對話，捕捉並儲存關鍵資訊，形成長期記憶。</li>
</ul>
<p>這兩者共同構成了打造有狀態 AI 的基石。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：在講完「LLM 的失憶症」後，可以稍微停頓一下，讓聽眾思考一下自己與 AI 互動時是否也遇過類似的體驗，加深共鳴。</li>
<li><strong>補充案例</strong>：可以舉例，像是「你每次都要重新告訴 ChatGPT 你的角色和需求」，這就是無狀態的具體表現。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>了解了我們面臨的根本問題和解決方案的名稱後，大家可能會好奇，「脈絡工程」到底是什麼？它跟我們常聽到的「提示工程 (Prompt Engineering)」有什麼不同？下一頁，我們就來深入解析這個概念。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-2" class="note-content note-raw" style="display: none;">
                    <pre>```markdown
### 🎙️ 第 02 頁：Introduction_The_Need_for_Stateful_AI

#### 【本頁重點摘要】
*   **核心問題**：大型語言模型 (LLM) 天生是「無狀態」的，它們的記憶僅限於單次的 API 呼叫，無法記住過去的對話。
*   **面臨挑戰**：我們該如何打造能夠記憶、學習並提供個人化互動的 AI 代理人 (Agent)？
*   **解決方案**：答案是「脈絡工程」(Context Engineering)，一種動態組合與管理 LLM 脈絡視窗資訊的方法。
*   **兩大基石**：這個方案主要由兩個核心組件構成——「會話 (Sessions)」和「記憶 (Memory)」。

---

#### 【逐字講稿】

(開場白)
大家好，我們今天探討的主題，是如何讓 AI 從一個健忘的工具，進化成一個能記住你、了解你的智慧夥伴。這一切，都始於我們需要「有狀態的 AI」。

##### ① 核心困境：大型語言模型的「失憶症」
首先，我們必須理解一個根本問題：現今所有的大型語言模型，本質上都是**無狀態 (stateless)** 的。這代表什麼呢？

> 這意味著，除了訓練資料之外，它們的推理和意識，完全被限制在單次 API 呼叫的「脈絡視窗 (context window)」之內。

簡單來說，每一次你跟它互動，都是一次全新的開始。它不記得你五分鐘前問過什麼，也不記得你的偏好。這就是為什麼有時候跟 AI 聊天，會感覺像在跟一個有「失憶症」的對象說話。這是一個根本性的障礙，限制了我們打造真正智慧體驗的可能性。

##### ② 挑戰與解方：從「無狀態」到「有狀態」
所以，我們面臨的**挑戰**非常明確：我們該如何打造一個能夠**記憶**、**學習**，並隨著時間推移**個人化**互動的代理人呢？

答案，就是我們今天將深入探討的核心概念——**脈絡工程 (Context Engineering)**。

這不是一個花俏的術語，而是一套具體的方法論。根據白皮書的定義，脈絡工程是「在 LLM 的脈絡視窗內，動態地組合與管理資訊，以實現有狀態、智慧化的代理人。」

##### ③ 兩大核心組件：會話 (Sessions) 與記憶 (Memory)
而要實現脈絡工程，我們需要依賴兩個關鍵的組件：
*   第一個是 **會話 (Sessions)**：它負責管理當下這一次對話的即時狀態，就像是我們對話中的短期記憶。
*   第二個是 **記憶 (Memory)**：它提供長期的持久性，能夠跨越多次不同的對話，捕捉並儲存關鍵資訊，形成長期記憶。

這兩者共同構成了打造有狀態 AI 的基石。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在講完「LLM 的失憶症」後，可以稍微停頓一下，讓聽眾思考一下自己與 AI 互動時是否也遇過類似的體驗，加深共鳴。
*   **補充案例**：可以舉例，像是「你每次都要重新告訴 ChatGPT 你的角色和需求」，這就是無狀態的具體表現。
*   **轉場橋樑 (Bridge)**：
    > 了解了我們面臨的根本問題和解決方案的名稱後，大家可能會好奇，「脈絡工程」到底是什麼？它跟我們常聽到的「提示工程 (Prompt Engineering)」有什麼不同？下一頁，我們就來深入解析這個概念。
```</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-3">
            <div class="slide">
                <h2>Slide 03</h2>
                <div class="rendered-content">
                    <h1>What is Context Engineering?</h1>
<p><strong>Context Engineering is the dynamic assembly and management of information for an LLM.</strong></p>
<ul>
<li>
<p><strong>Evolution of Prompt Engineering:</strong> It goes beyond crafting static system instructions to address the entire payload, dynamically constructing a state-aware prompt.</p>
</li>
<li>
<p><strong>The Goal:</strong> Ensure the model has no more and no less than the most relevant information to complete its task.</p>
</li>
<li>
<p><strong>Analogy: <em>Mise en Place</em></strong></p>
<ul>
<li>A chef gathers and prepares all ingredients <em>before</em> cooking.</li>
<li>Context Engineering ensures the agent has the right, high-quality information, tools, and instructions to reliably produce an excellent, customized result.</li>
</ul>
</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 03</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(3, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(3, 'raw')">Source</button>
                </div>
                <div id="note-rendered-3" class="note-content rendered-content">
                    <h3>🎙️ 第 03 頁：What is Context Engineering</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li><strong>核心定義</strong>：上下文工程 (Context Engineering) 是為大型語言模型 (LLM) 動態組裝與管理資訊的過程。</li>
<li><strong>超越提示工程</strong>：它不僅是設計靜態指令，而是動態建構一個包含使用者、歷史紀錄與外部數據的「具備狀態意識」的完整情境。</li>
<li><strong>最終目標</strong>：確保模型不多不少，只獲得完成任務所需的最相關資訊。</li>
<li><strong>生動比喻</strong>：就像法式料理中的 <em>Mise en Place</em> (備料)，在烹飪前備妥所有高品質的食材與工具，才能穩定產出最佳結果。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好，在上一頁我們提到了打造「有狀態」AI 的核心挑戰，以及兩個關鍵元件：Session 和 Memory。而要將這一切串連起來，我們必須先理解一個更宏觀的概念，那就是<strong>上下文工程 (Context Engineering)</strong>。</p>
<p>各位可能對「提示工程 (Prompt Engineering)」這個詞更熟悉，但我要說，提示工程只是這幅巨大藍圖中的一小部分。</p>
<h5>① 什麼是上下文工程？</h5>
<p>簡單來說，投影片上定義了：<strong>上下文工程，就是為大型語言模型動態組裝和管理資訊的整個過程。</strong></p>
<p>LLM 天生是健忘的，它的世界觀僅限於我們在單次 API 請求中提供給它的「上下文視窗」。所以，要讓 AI 變得聰明、能記憶、能個人化，我們就必須在每一次互動中，為它精心建構這個上下文。這就是上下文工程的核心——它是一個<strong>動態的、有策略的建構過程</strong>。</p>
<h5>② 它如何超越提示工程？</h5>
<p>這也是它與傳統提示工程最大的區別。提示工程，更專注於打造一份完美的、通常是<strong>靜態的</strong>系統說明書或食譜。</p>
<p>但上下文工程處理的是<strong>整個交付給模型的 payload</strong>。它會根據當下的使用者是誰、對話歷史是什麼、以及需要從哪些外部資料庫檢索資訊，來<strong>動態地</strong>建構一個具備「狀態感知」的完整情境。它的目標是策略性地選擇、總結、並注入不同類型的資訊，最大化相關性，同時最小化雜訊。</p>
<blockquote>
<p>正如我們在白皮書中強調的：「上下文工程的目標，是確保模型不多不少，只獲得完成其任務所需的最相關資訊。」</p>
</blockquote>
<h5>③ 最好的比喻：法式料理的 <em>Mise en Place</em></h5>
<p>為了讓大家更好理解，我想請各位想像一位米其林星級大廚。</p>
<p>「上下文工程」就像這位大廚在烹飪前的<strong>備料 (Mise en Place)</strong> 過程。這是一個法文術語，指的是在開始烹飪前，把所有食材、醬料、工具全部準備妥當、井然有序地擺放好。</p>
<p>如果我們只給大廚一份食譜（也就是<strong>提示</strong>），他或許能用手邊隨便的食材做出一道還行的菜。但是，如果我們事先確保他擁有一流的食材、專用的工具、並且清楚了解最終的擺盤風格，他就能<strong>穩定地</strong>創造出卓越、客製化的成果。</p>
<p>這就是上下文工程的精髓。我們不是只丟給模型一個指令，而是確保它在「思考」之前，就已經擁有了最優質、最相關的記憶、工具和行為準則。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：在講完 <em>Mise en Place</em> 這個比喻後，可以稍微停頓一下，讓聽眾消化這個概念。這個比喻非常關鍵。</li>
<li><strong>補充案例</strong>：可以口頭補充：「就像點餐 App 如果能記得你對香菜過敏，這就是一個簡單但有效的上下文工程應用。」</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>那麼，既然我們把上下文工程比喻為廚師的「備料」過程，大家一定很好奇，這個備料台上，到底都放了哪些關鍵的「食材」呢？下一頁，我們就來一一拆解這個複雜 payload 的組成元件。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-3" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 03 頁：What is Context Engineering

#### 【本頁重點摘要】
*   **核心定義**：上下文工程 (Context Engineering) 是為大型語言模型 (LLM) 動態組裝與管理資訊的過程。
*   **超越提示工程**：它不僅是設計靜態指令，而是動態建構一個包含使用者、歷史紀錄與外部數據的「具備狀態意識」的完整情境。
*   **最終目標**：確保模型不多不少，只獲得完成任務所需的最相關資訊。
*   **生動比喻**：就像法式料理中的 *Mise en Place* (備料)，在烹飪前備妥所有高品質的食材與工具，才能穩定產出最佳結果。

---

#### 【逐字講稿】

(開場白)
好，在上一頁我們提到了打造「有狀態」AI 的核心挑戰，以及兩個關鍵元件：Session 和 Memory。而要將這一切串連起來，我們必須先理解一個更宏觀的概念，那就是**上下文工程 (Context Engineering)**。

各位可能對「提示工程 (Prompt Engineering)」這個詞更熟悉，但我要說，提示工程只是這幅巨大藍圖中的一小部分。

##### ① 什麼是上下文工程？

簡單來說，投影片上定義了：**上下文工程，就是為大型語言模型動態組裝和管理資訊的整個過程。**

LLM 天生是健忘的，它的世界觀僅限於我們在單次 API 請求中提供給它的「上下文視窗」。所以，要讓 AI 變得聰明、能記憶、能個人化，我們就必須在每一次互動中，為它精心建構這個上下文。這就是上下文工程的核心——它是一個**動態的、有策略的建構過程**。

##### ② 它如何超越提示工程？

這也是它與傳統提示工程最大的區別。提示工程，更專注於打造一份完美的、通常是**靜態的**系統說明書或食譜。

但上下文工程處理的是**整個交付給模型的 payload**。它會根據當下的使用者是誰、對話歷史是什麼、以及需要從哪些外部資料庫檢索資訊，來**動態地**建構一個具備「狀態感知」的完整情境。它的目標是策略性地選擇、總結、並注入不同類型的資訊，最大化相關性，同時最小化雜訊。

> 正如我們在白皮書中強調的：「上下文工程的目標，是確保模型不多不少，只獲得完成其任務所需的最相關資訊。」

##### ③ 最好的比喻：法式料理的 *Mise en Place*

為了讓大家更好理解，我想請各位想像一位米其林星級大廚。

「上下文工程」就像這位大廚在烹飪前的**備料 (Mise en Place)** 過程。這是一個法文術語，指的是在開始烹飪前，把所有食材、醬料、工具全部準備妥當、井然有序地擺放好。

如果我們只給大廚一份食譜（也就是**提示**），他或許能用手邊隨便的食材做出一道還行的菜。但是，如果我們事先確保他擁有一流的食材、專用的工具、並且清楚了解最終的擺盤風格，他就能**穩定地**創造出卓越、客製化的成果。

這就是上下文工程的精髓。我們不是只丟給模型一個指令，而是確保它在「思考」之前，就已經擁有了最優質、最相關的記憶、工具和行為準則。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在講完 *Mise en Place* 這個比喻後，可以稍微停頓一下，讓聽眾消化這個概念。這個比喻非常關鍵。
*   **補充案例**：可以口頭補充：「就像點餐 App 如果能記得你對香菜過敏，這就是一個簡單但有效的上下文工程應用。」
*   **轉場橋樑 (Bridge)**：
    > 那麼，既然我們把上下文工程比喻為廚師的「備料」過程，大家一定很好奇，這個備料台上，到底都放了哪些關鍵的「食材」呢？下一頁，我們就來一一拆解這個複雜 payload 的組成元件。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-4">
            <div class="slide">
                <h2>Slide 04</h2>
                <div class="rendered-content">
                    <h1>Components of the Context Payload</h1>
<p>Context Engineering assembles a complex payload with three main categories:</p>
<ol>
<li>
<p><strong>Context to Guide Reasoning:</strong> Defines the agent's behavior.</p>
<ul>
<li>System Instructions, Tool Definitions, Few-Shot Examples.</li>
</ul>
</li>
<li>
<p><strong>Evidential &amp; Factual Data:</strong> The substantive data the agent reasons over.</p>
<ul>
<li>Long-Term Memory, External Knowledge (RAG), Tool Outputs, Sub-Agent Outputs.</li>
</ul>
</li>
<li>
<p><strong>Immediate Conversational Information:</strong> Grounds the agent in the current interaction.</p>
<ul>
<li>Conversation History, State/Scratchpad, User's Prompt.</li>
</ul>
</li>
</ol>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 04</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(4, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(4, 'raw')">Source</button>
                </div>
                <div id="note-rendered-4" class="note-content rendered-content">
                    <h3>🎙️ 第 04 頁：Components_of_the_Context_Payload</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>Context Engineering 將三類關鍵資訊組合成一個完整的「Payload」來指導 AI。</li>
<li>第一類是<strong>指導推理的背景</strong>，定義了 AI 的行為模式與能力。</li>
<li>第二類是<strong>證據與事實數據</strong>，是 AI 進行思考所依據的實質資料。</li>
<li>第三類是<strong>即時對話資訊</strong>，讓 AI 錨定在當前的互動情境中。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，我們剛剛提到了「Context Engineering」就像是為 AI 主廚準備「Mise en Place」，也就是備料。那麼，這個「料」到底包含了哪些東西呢？這就是我們這一頁要談的——<strong>情境的組合包 (Context Payload)</strong>。</p>
<p>這個組合包基本上可以分成三大類，每一類都扮演著不可或缺的角色。</p>
<h5>① 第一類：指導推理的背景 (Context to Guide Reasoning)</h5>
<p>首先，是指導 AI 如何思考與行動的指令。這部分定義了 AI 的<strong>基本行為模式</strong>與<strong>可用能力</strong>。</p>
<blockquote>
<p>它就像是 AI 的「大腦操作手冊」，告訴它「你是誰」、「你能做什麼」以及「你該怎麼做」。</p>
</blockquote>
<p>這包含了幾樣東西：</p>
<ul>
<li><strong>系統指令 (System Instructions)</strong>：這是最高層級的指示，定義了 AI 的人設、能力範圍和限制。</li>
<li><strong>工具定義 (Tool Definitions)</strong>：這就像是給 AI 的工具箱，說明了它可以使用哪些 API 或函式來與外部世界互動。</li>
<li><strong>少樣本範例 (Few-Shot Examples)</strong>：提供一些精選的問答範例，透過「情境中學習 (In-context learning)」來引導模型遵循特定的推理模式。</li>
</ul>
<h5>② 第二類：證據與事實數據 (Evidential &amp; Factual Data)</h5>
<p>接下來，是 AI 進行推理時所依據的<strong>實質內容</strong>。這部分是 AI 回應的「證據來源」。</p>
<p>這包含了：</p>
<ul>
<li><strong>長期記憶 (Long-Term Memory)</strong>：跨越多次對話、關於使用者或主題的持久化知識。</li>
<li><strong>外部知識 (External Knowledge)</strong>：通常是透過 <strong>RAG</strong> 技術從資料庫或文件中檢索出來的資訊。</li>
<li><strong>工具輸出 (Tool Outputs)</strong>：AI 呼叫工具後返回的數據或結果。</li>
<li><strong>子代理輸出 (Sub-Agent Outputs)</strong>：如果任務被委派給了其他專門的 AI，它們完成任務後回傳的結論。</li>
</ul>
<h5>③ 第三類：即時對話資訊 (Immediate Conversational Information)</h5>
<p>最後一類，也是最直觀的，就是<strong>當下這次互動</strong>的相關資訊。它將 AI 牢牢地「錨定」在目前的對話情境中。</p>
<p>這包括：</p>
<ul>
<li><strong>對話歷史 (Conversation History)</strong>：當前互動中一來一往的完整紀錄。</li>
<li><strong>狀態／暫存器 (State / Scratchpad)</strong>：AI 在思考過程中使用的臨時筆記或計算資料。</li>
<li><strong>使用者的提問 (User's Prompt)</strong>：當然，還有使用者剛剛提出的那個問題。</li>
</ul>
<p>這三類資訊組合在一起，就構成了一個完整、有狀態、能讓 AI 做出精準回應的「Context Payload」。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：講完這三類資訊後，可以稍微停頓一下，讓聽眾消化。你可以這樣總結：「所以，你可以把這三者想像成：AI 的**『操作手冊』<strong>、它能查閱的</strong>『參考資料』<strong>，以及它正在處理的</strong>『當前任務』**。」</li>
<li><strong>補充案例</strong>：如果時間允許，可以舉一個旅行預訂機器人的例子。系統指令是「你是一個友善的旅行助手」，工具是「查詢航班 API」，長期記憶是「使用者偏好靠窗座位」，而對話歷史就是「我們正在討論去巴黎的機票」。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>了解了構成 Context 的**「內容物」<strong>之後，下一頁我們將探討，在每一次對話中，這些內容物是如何被一個</strong>連續的循環**所獲取、準備、並加以利用的。這就是「情境工程的生命週期」。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-4" class="note-content note-raw" style="display: none;">
                    <pre>```markdown
### 🎙️ 第 04 頁：Components_of_the_Context_Payload

#### 【本頁重點摘要】
*   Context Engineering 將三類關鍵資訊組合成一個完整的「Payload」來指導 AI。
*   第一類是**指導推理的背景**，定義了 AI 的行為模式與能力。
*   第二類是**證據與事實數據**，是 AI 進行思考所依據的實質資料。
*   第三類是**即時對話資訊**，讓 AI 錨定在當前的互動情境中。

---

#### 【逐字講稿】

(開場白)
好的，我們剛剛提到了「Context Engineering」就像是為 AI 主廚準備「Mise en Place」，也就是備料。那麼，這個「料」到底包含了哪些東西呢？這就是我們這一頁要談的——**情境的組合包 (Context Payload)**。

這個組合包基本上可以分成三大類，每一類都扮演著不可或缺的角色。

##### ① 第一類：指導推理的背景 (Context to Guide Reasoning)
首先，是指導 AI 如何思考與行動的指令。這部分定義了 AI 的**基本行為模式**與**可用能力**。

> 它就像是 AI 的「大腦操作手冊」，告訴它「你是誰」、「你能做什麼」以及「你該怎麼做」。

這包含了幾樣東西：
*   **系統指令 (System Instructions)**：這是最高層級的指示，定義了 AI 的人設、能力範圍和限制。
*   **工具定義 (Tool Definitions)**：這就像是給 AI 的工具箱，說明了它可以使用哪些 API 或函式來與外部世界互動。
*   **少樣本範例 (Few-Shot Examples)**：提供一些精選的問答範例，透過「情境中學習 (In-context learning)」來引導模型遵循特定的推理模式。

##### ② 第二類：證據與事實數據 (Evidential & Factual Data)
接下來，是 AI 進行推理時所依據的**實質內容**。這部分是 AI 回應的「證據來源」。

這包含了：
*   **長期記憶 (Long-Term Memory)**：跨越多次對話、關於使用者或主題的持久化知識。
*   **外部知識 (External Knowledge)**：通常是透過 **RAG** 技術從資料庫或文件中檢索出來的資訊。
*   **工具輸出 (Tool Outputs)**：AI 呼叫工具後返回的數據或結果。
*   **子代理輸出 (Sub-Agent Outputs)**：如果任務被委派給了其他專門的 AI，它們完成任務後回傳的結論。

##### ③ 第三類：即時對話資訊 (Immediate Conversational Information)
最後一類，也是最直觀的，就是**當下這次互動**的相關資訊。它將 AI 牢牢地「錨定」在目前的對話情境中。

這包括：
*   **對話歷史 (Conversation History)**：當前互動中一來一往的完整紀錄。
*   **狀態／暫存器 (State / Scratchpad)**：AI 在思考過程中使用的臨時筆記或計算資料。
*   **使用者的提問 (User's Prompt)**：當然，還有使用者剛剛提出的那個問題。

這三類資訊組合在一起，就構成了一個完整、有狀態、能讓 AI 做出精準回應的「Context Payload」。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講完這三類資訊後，可以稍微停頓一下，讓聽眾消化。你可以這樣總結：「所以，你可以把這三者想像成：AI 的**『操作手冊』**、它能查閱的**『參考資料』**，以及它正在處理的**『當前任務』**。」
*   **補充案例**：如果時間允許，可以舉一個旅行預訂機器人的例子。系統指令是「你是一個友善的旅行助手」，工具是「查詢航班 API」，長期記憶是「使用者偏好靠窗座位」，而對話歷史就是「我們正在討論去巴黎的機票」。
*   **轉場橋樑 (Bridge)**：
    > 了解了構成 Context 的**「內容物」**之後，下一頁我們將探討，在每一次對話中，這些內容物是如何被一個**連續的循環**所獲取、準備、並加以利用的。這就是「情境工程的生命週期」。

```</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-5">
            <div class="slide">
                <h2>Slide 05</h2>
                <div class="rendered-content">
                    <h1>The Context Engineering Lifecycle</h1>
<p>A continuous cycle for each turn of a conversation:</p>
<ol>
<li>
<p><strong>Fetch Context:</strong></p>
<ul>
<li>Retrieve user memories, RAG documents, and recent conversation events.</li>
</ul>
</li>
<li>
<p><strong>Prepare Context:</strong></p>
<ul>
<li>Dynamically construct the full prompt for the LLM. This is a blocking, &quot;hot-path&quot; process.</li>
</ul>
</li>
<li>
<p><strong>Invoke LLM and Tools:</strong></p>
<ul>
<li>Iteratively call the LLM and any necessary tools to generate a response.</li>
</ul>
</li>
<li>
<p><strong>Upload Context:</strong></p>
<ul>
<li>Asynchronously upload new information (e.g., new memories) to persistent storage in the background.</li>
</ul>
</li>
</ol>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 05</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(5, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(5, 'raw')">Source</button>
                </div>
                <div id="note-rendered-5" class="note-content rendered-content">
                    <h3>🎙️ 第 05 頁：The_Context_Engineering_Lifecycle</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>本頁說明了「情境工程」在每一次對話中運作的連續循環。</li>
<li>這個循環包含四個關鍵階段：獲取情境、準備情境、調用模型與工具，以及上傳情境。</li>
<li>強調了哪些步驟是「阻塞式」的（必須等待完成），哪些是「非阻塞式」的（在背景中進行）。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，我們已經理解了「情境工程」就像是廚師在料理前的備料（Mise en Place）。現在，讓我們來看看這位 AI 大廚在每一次與使用者互動時，實際上是如何一步步「烹飪」的。這就是「情境工程生命週期」，一個在每次對話中不斷重複的循環。</p>
<h5>① 第一步：獲取情境 (Fetch Context)</h5>
<p>首先，是<strong>獲取情境</strong>。這就像廚師從冰箱和儲藏室裡拿出所有需要的食材。AI 代理會開始檢索所有相關的資訊，例如關於這位使用者的<strong>長期記憶</strong>、從外部知識庫（也就是 RAG）中找到的相關文件，以及我們這次對話最近的幾句交談紀錄。</p>
<blockquote>
<p>值得注意的是，這是一個動態的過程。代理會根據你當前問的問題，來決定要去檢索哪些最相關的資訊。</p>
</blockquote>
<h5>② 第二步：準備情境 (Prepare Context)</h5>
<p>拿到所有材料後，就進入了至關重要的第二步：<strong>準備情境</strong>。代理框架會將剛剛獲取的所有資訊——系統指令、工具、記憶、RAG 文件、對話歷史——動態地組合成一個完整的、結構化的提示 (Prompt)，準備提交給大型語言模型。</p>
<p>這個步驟被稱為「<strong>阻塞式 (Blocking)</strong>」或「<strong>熱路徑 (Hot-Path)</strong>」流程。這意味著代理必須等所有情境都準備就緒後，才能進行下一步。它不能在材料還沒備齊的時候就開始炒菜。</p>
<h5>③ 第三步：調用大型語言模型與工具 (Invoke LLM and Tools)</h5>
<p>情境準備好之後，就進入了執行階段。代理會<strong>迭代地 (iteratively)</strong> 調用大型語言模型和它所需要的任何工具，直到生成最終要給使用者的回覆。這個過程可能不是一次就完成的，而是來來回回的思考、調用工具、再思考，直到得出結論。所有工具和模型的輸出，又會被加回到當前的情境中。</p>
<h5>④ 第四步：上傳情境 (Upload Context)</h5>
<p>最後，當代理把回覆傳送給使用者後，它會在<strong>背景 (background)</strong> 中進行最後一個步驟：<strong>上傳情境</strong>。在這次對話中產生的任何新資訊，例如新的記憶點，都會被非同步地（asynchronously）上傳到持久化的儲存空間。</p>
<blockquote>
<p>這樣做的好處是，使用者可以立刻收到回覆，而不需要等待代理完成歸檔工作。這確保了流暢的使用者體驗，同時也讓代理的知識庫不斷成長。</p>
</blockquote>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：講完這四個步驟後，可以稍微停頓一下，讓聽眾消化這個循環的流程。這是一個核心概念。</li>
<li><strong>補充說明</strong>：可以再次使用廚師的類比來總結——「獲取食材、準備食材、烹飪、最後清洗和歸檔」。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>了解了這個為每一次互動精心準備情境的生命週期後，你可能會好奇：這整個對話的「容器」是什麼？這個管理著所有即時互動歷史的工作台，就是我們的第一個核心組件——<strong>Session</strong>。下一頁，我們就來深入探討它。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-5" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 05 頁：The_Context_Engineering_Lifecycle

#### 【本頁重點摘要】
*   本頁說明了「情境工程」在每一次對話中運作的連續循環。
*   這個循環包含四個關鍵階段：獲取情境、準備情境、調用模型與工具，以及上傳情境。
*   強調了哪些步驟是「阻塞式」的（必須等待完成），哪些是「非阻塞式」的（在背景中進行）。

---

#### 【逐字講稿】

(開場白)
好的，我們已經理解了「情境工程」就像是廚師在料理前的備料（Mise en Place）。現在，讓我們來看看這位 AI 大廚在每一次與使用者互動時，實際上是如何一步步「烹飪」的。這就是「情境工程生命週期」，一個在每次對話中不斷重複的循環。

##### ① 第一步：獲取情境 (Fetch Context)
首先，是**獲取情境**。這就像廚師從冰箱和儲藏室裡拿出所有需要的食材。AI 代理會開始檢索所有相關的資訊，例如關於這位使用者的**長期記憶**、從外部知識庫（也就是 RAG）中找到的相關文件，以及我們這次對話最近的幾句交談紀錄。

> 值得注意的是，這是一個動態的過程。代理會根據你當前問的問題，來決定要去檢索哪些最相關的資訊。

##### ② 第二步：準備情境 (Prepare Context)
拿到所有材料後，就進入了至關重要的第二步：**準備情境**。代理框架會將剛剛獲取的所有資訊——系統指令、工具、記憶、RAG 文件、對話歷史——動態地組合成一個完整的、結構化的提示 (Prompt)，準備提交給大型語言模型。

這個步驟被稱為「**阻塞式 (Blocking)**」或「**熱路徑 (Hot-Path)**」流程。這意味著代理必須等所有情境都準備就緒後，才能進行下一步。它不能在材料還沒備齊的時候就開始炒菜。

##### ③ 第三步：調用大型語言模型與工具 (Invoke LLM and Tools)
情境準備好之後，就進入了執行階段。代理會**迭代地 (iteratively)** 調用大型語言模型和它所需要的任何工具，直到生成最終要給使用者的回覆。這個過程可能不是一次就完成的，而是來來回回的思考、調用工具、再思考，直到得出結論。所有工具和模型的輸出，又會被加回到當前的情境中。

##### ④ 第四步：上傳情境 (Upload Context)
最後，當代理把回覆傳送給使用者後，它會在**背景 (background)** 中進行最後一個步驟：**上傳情境**。在這次對話中產生的任何新資訊，例如新的記憶點，都會被非同步地（asynchronously）上傳到持久化的儲存空間。

> 這樣做的好處是，使用者可以立刻收到回覆，而不需要等待代理完成歸檔工作。這確保了流暢的使用者體驗，同時也讓代理的知識庫不斷成長。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講完這四個步驟後，可以稍微停頓一下，讓聽眾消化這個循環的流程。這是一個核心概念。
*   **補充說明**：可以再次使用廚師的類比來總結——「獲取食材、準備食材、烹飪、最後清洗和歸檔」。
*   **轉場橋樑 (Bridge)**：
    > 了解了這個為每一次互動精心準備情境的生命週期後，你可能會好奇：這整個對話的「容器」是什麼？這個管理著所有即時互動歷史的工作台，就是我們的第一個核心組件——**Session**。下一頁，我們就來深入探討它。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-6">
            <div class="slide">
                <h2>Slide 06</h2>
                <div class="rendered-content">
                    <h1>Core Component 1: Sessions</h1>
<p><strong>A session encapsulates the immediate dialogue history and working memory for a single, continuous conversation.</strong></p>
<ul>
<li><strong>Self-Contained:</strong> Each session is a distinct record tied to a specific user.</li>
<li><strong>Purpose:</strong> Allows the agent to maintain context and provide coherent responses within one conversation.</li>
<li><strong>Analogy: The Workbench</strong>
<ul>
<li>A session is like a workbench for a specific project, covered in all the necessary tools and notes.</li>
<li>It's temporary and task-specific. Once the project is done, you don't store the whole messy desk; you file away the important parts (which becomes <strong>Memory</strong>).</li>
</ul>
</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 06</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(6, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(6, 'raw')">Source</button>
                </div>
                <div id="note-rendered-6" class="note-content rendered-content">
                    <h3>🎙️ 第 06 頁：Core_Component_1_Sessions</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>Session 是 AI 代理人在單次連續對話中的「短期記憶」與「工作區」。</li>
<li>它封裝了該次對話的歷史紀錄與暫存資料，並且與特定使用者綁定。</li>
<li>核心比喻：Session 如同專案的「工作台」，是暫時且任務導向的；而 Memory 則是整理歸檔後的「檔案櫃」。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，在我們深入探討如何打造有記憶、有智慧的 AI 之前，我們必須先理解一切互動的基礎——也就是「Session」。你可以把它想像成是 AI 代理人的「短期記憶」或「工作區」。</p>
<h5>① 工作台的比喻</h5>
<p>為了讓大家更容易理解，我們來用一個比喻。</p>
<blockquote>
<p>想像一下，一個 Session 就像是你為了完成一個特定專案而準備的「工作台」。</p>
</blockquote>
<p>當你正在工作時，這個工作台上會擺滿所有你需要的工具、參考資料、草稿和筆記。所有東西都隨手可得，但它們也都是暫時的，而且只跟手上的這個專案有關。</p>
<h5>② Session 的定義與目的</h5>
<p>這個比喻完美地對應到 Session 的正式定義：<strong>它封裝了單一、連續對話中的即時對話歷史和工作記憶。</strong></p>
<p>這代表每個 Session 都是一個獨立的紀錄，並且與特定的使用者綁定。這就是為什麼 AI 代理人能夠在「單次對話中」保持上下文連貫，並給出有條理的回應。一個使用者可以開啟很多個不同的 Session，但每一個之間都是互相獨立、互不干擾的。</p>
<h5>③ 從工作台到檔案櫃</h5>
<p>現在，再回到工作台的比喻。當你的專案完成後，你不會把整個凌亂的工作台直接推進儲藏室，對吧？你會開始一個整理的過程：把草稿丟掉、把多餘的筆記捨棄，只把最關鍵、最終版的成果文件，分門別類地放進一個<strong>井然有序的檔案櫃</strong>裡。</p>
<p>這個「歸檔」的過程，就是將暫時的 Session 轉化為長期知識的關鍵。而那個整齊的檔案櫃，就是我們接下來要探討的第二個核心組件——<strong>Memory (記憶)</strong>。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：在講完「工作台」的比喻後，可以稍微停頓一下，讓聽眾在腦中建立這個畫面，這有助於他們理解 Session 的暫時性與任務導向性。</li>
<li><strong>補充案例</strong>：可以口頭舉例，例如：「你跟訂票機器人的對話就是一個 Session，當你關掉視窗再重開，就是一個新的 Session，它不會記得你上次查到哪裡。」</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>了解了 Session 如同一個專案工作台後，下一頁，我們就來打開這個工作台的抽屜，看看它內部是由哪些具體的「架構」——也就是「事件」與「狀態」——所組成的。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-6" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 06 頁：Core_Component_1_Sessions

#### 【本頁重點摘要】
*   Session 是 AI 代理人在單次連續對話中的「短期記憶」與「工作區」。
*   它封裝了該次對話的歷史紀錄與暫存資料，並且與特定使用者綁定。
*   核心比喻：Session 如同專案的「工作台」，是暫時且任務導向的；而 Memory 則是整理歸檔後的「檔案櫃」。

---

#### 【逐字講稿】

(開場白)
好的，在我們深入探討如何打造有記憶、有智慧的 AI 之前，我們必須先理解一切互動的基礎——也就是「Session」。你可以把它想像成是 AI 代理人的「短期記憶」或「工作區」。

##### ① 工作台的比喻
為了讓大家更容易理解，我們來用一個比喻。

> 想像一下，一個 Session 就像是你為了完成一個特定專案而準備的「工作台」。

當你正在工作時，這個工作台上會擺滿所有你需要的工具、參考資料、草稿和筆記。所有東西都隨手可得，但它們也都是暫時的，而且只跟手上的這個專案有關。

##### ② Session 的定義與目的
這個比喻完美地對應到 Session 的正式定義：**它封裝了單一、連續對話中的即時對話歷史和工作記憶。**

這代表每個 Session 都是一個獨立的紀錄，並且與特定的使用者綁定。這就是為什麼 AI 代理人能夠在「單次對話中」保持上下文連貫，並給出有條理的回應。一個使用者可以開啟很多個不同的 Session，但每一個之間都是互相獨立、互不干擾的。

##### ③ 從工作台到檔案櫃
現在，再回到工作台的比喻。當你的專案完成後，你不會把整個凌亂的工作台直接推進儲藏室，對吧？你會開始一個整理的過程：把草稿丟掉、把多餘的筆記捨棄，只把最關鍵、最終版的成果文件，分門別類地放進一個**井然有序的檔案櫃**裡。

這個「歸檔」的過程，就是將暫時的 Session 轉化為長期知識的關鍵。而那個整齊的檔案櫃，就是我們接下來要探討的第二個核心組件——**Memory (記憶)**。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在講完「工作台」的比喻後，可以稍微停頓一下，讓聽眾在腦中建立這個畫面，這有助於他們理解 Session 的暫時性與任務導向性。
*   **補充案例**：可以口頭舉例，例如：「你跟訂票機器人的對話就是一個 Session，當你關掉視窗再重開，就是一個新的 Session，它不會記得你上次查到哪裡。」
*   **轉場橋樑 (Bridge)**：
    > 了解了 Session 如同一個專案工作台後，下一頁，我們就來打開這個工作台的抽屜，看看它內部是由哪些具體的「架構」——也就是「事件」與「狀態」——所組成的。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-7">
            <div class="slide">
                <h2>Slide 07</h2>
                <div class="rendered-content">
                    <h1>Session Architecture: Events and State</h1>
<p>Every session contains two key components:</p>
<ol>
<li>
<p><strong>Events (Chronological History):</strong></p>
<ul>
<li>The building blocks of the conversation, logged in order.</li>
<li><strong>Types:</strong> User Input, Agent Response, Tool Call, Tool Output.</li>
<li>Analogous to the list of <code>Content</code> objects passed to the Gemini API.</li>
</ul>
</li>
<li>
<p><strong>State (Working Memory):</strong></p>
<ul>
<li>A structured &quot;scratchpad&quot; holding temporary data relevant to the current conversation.</li>
<li>Example: Items in a shopping cart.</li>
</ul>
</li>
</ol>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 07</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(7, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(7, 'raw')">Source</button>
                </div>
                <div id="note-rendered-7" class="note-content rendered-content">
                    <h3>🎙️ 第 07 頁：Session_Architecture_Events_and_State</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>一個 Session 由兩個核心組件構成：<strong>事件 (Events)</strong> 與 <strong>狀態 (State)</strong>。</li>
<li><strong>事件</strong>是依時間順序記錄的對話歷史，包含使用者輸入、模型回應、工具調用與工具輸出。</li>
<li><strong>狀態</strong>是暫時性的工作記憶，用來存放當前對話的結構化資料，例如購物車裡的商品。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，我們剛剛把 Session 比喻成一個專案的工作檯。現在，讓我們來仔細看看這個工作檯上到底放了些什麼東西。一個 Session 的架構，主要由兩個關鍵部分組成：<strong>事件 (Events)</strong> 和 <strong>狀態 (State)</strong>。</p>
<h5>① 第一個是「事件」，也就是對話的時序歷史 (Chronological History)</h5>
<p>你可以把「事件」想像成這場對話的每一個積木，它們按照發生的順序一個個堆疊起來，構成完整的互動記錄。</p>
<p>這包含了幾種核心類型：</p>
<ul>
<li><strong>使用者輸入 (User Input)</strong>：就是使用者說的話、問的問題。</li>
<li><strong>模型回應 (Agent Response)</strong>：我們 AI Agent 的回覆。</li>
<li><strong>工具調用 (Tool Call)</strong>：當 Agent 決定需要使用外部工具來完成任務時，這個「決定」本身就是一個事件。</li>
<li><strong>工具輸出 (Tool Output)</strong>：外部工具回傳的資料或結果，Agent 會根據這個結果繼續下一步的思考。</li>
</ul>
<blockquote>
<p>如果你熟悉 Gemini API，這個結構就非常直觀了。它就像是傳遞給 Gemini API 的 <code>Content</code> 物件列表，每一個物件都標明了是誰 (<code>role</code>) 說了什麼 (<code>parts</code>)，完整記錄了你來我往的每一回合。</p>
</blockquote>
<h5>② 第二個是「狀態」，也就是我們所說的「工作記憶 (Working Memory)」</h5>
<p>如果說「事件」是完整的對話錄音稿，那麼「狀態」就是 Agent 放在旁邊的一張結構化的便利貼或草稿紙 (Scratchpad)。</p>
<p>它專門用來存放<strong>和當前這次對話高度相關的、暫時性的資料</strong>。最經典的例子就是購物車。當你請 AI 幫你把商品加入購物車時，「購物車裡有哪些商品」這個列表，並不是對話的一部分，但卻是完成這次購物任務至關重要的資訊。這個列表就會被存放在 Session 的「狀態」裡。當對話結束，這個暫時的狀態通常也就不再需要了。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：講完這兩個概念後，可以稍微停頓一下，確保聽眾理解「事件」是歷史紀錄，而「狀態」是暫存資料這兩個核心區別。</li>
<li><strong>補充案例</strong>：如果時間允許，可以再舉一個「狀態」的例子，例如：在一個多步驟的機票預訂流程中，「狀態」可以儲存使用者已經確認的出發地、目的地和日期，即使對話中間聊了別的話題，這些核心資訊也不會遺失。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>了解了單一 Agent 的對話是如何被結構化的，下一個問題自然就是：如果系統中不只有一個 Agent 呢？當多個 Agent 需要協同工作時，它們之間是如何共享或隔離這些對話歷史的？下一頁，我們就來探討多 Agent 系統中的 Session 管理模式。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-7" class="note-content note-raw" style="display: none;">
                    <pre>```markdown
### 🎙️ 第 07 頁：Session_Architecture_Events_and_State

#### 【本頁重點摘要】
*   一個 Session 由兩個核心組件構成：**事件 (Events)** 與 **狀態 (State)**。
*   **事件**是依時間順序記錄的對話歷史，包含使用者輸入、模型回應、工具調用與工具輸出。
*   **狀態**是暫時性的工作記憶，用來存放當前對話的結構化資料，例如購物車裡的商品。

---

#### 【逐字講稿】

(開場白)
好的，我們剛剛把 Session 比喻成一個專案的工作檯。現在，讓我們來仔細看看這個工作檯上到底放了些什麼東西。一個 Session 的架構，主要由兩個關鍵部分組成：**事件 (Events)** 和 **狀態 (State)**。

##### ① 第一個是「事件」，也就是對話的時序歷史 (Chronological History)
你可以把「事件」想像成這場對話的每一個積木，它們按照發生的順序一個個堆疊起來，構成完整的互動記錄。

這包含了幾種核心類型：
*   **使用者輸入 (User Input)**：就是使用者說的話、問的問題。
*   **模型回應 (Agent Response)**：我們 AI Agent 的回覆。
*   **工具調用 (Tool Call)**：當 Agent 決定需要使用外部工具來完成任務時，這個「決定」本身就是一個事件。
*   **工具輸出 (Tool Output)**：外部工具回傳的資料或結果，Agent 會根據這個結果繼續下一步的思考。

> 如果你熟悉 Gemini API，這個結構就非常直觀了。它就像是傳遞給 Gemini API 的 `Content` 物件列表，每一個物件都標明了是誰 (`role`) 說了什麼 (`parts`)，完整記錄了你來我往的每一回合。

##### ② 第二個是「狀態」，也就是我們所說的「工作記憶 (Working Memory)」
如果說「事件」是完整的對話錄音稿，那麼「狀態」就是 Agent 放在旁邊的一張結構化的便利貼或草稿紙 (Scratchpad)。

它專門用來存放**和當前這次對話高度相關的、暫時性的資料**。最經典的例子就是購物車。當你請 AI 幫你把商品加入購物車時，「購物車裡有哪些商品」這個列表，並不是對話的一部分，但卻是完成這次購物任務至關重要的資訊。這個列表就會被存放在 Session 的「狀態」裡。當對話結束，這個暫時的狀態通常也就不再需要了。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講完這兩個概念後，可以稍微停頓一下，確保聽眾理解「事件」是歷史紀錄，而「狀態」是暫存資料這兩個核心區別。
*   **補充案例**：如果時間允許，可以再舉一個「狀態」的例子，例如：在一個多步驟的機票預訂流程中，「狀態」可以儲存使用者已經確認的出發地、目的地和日期，即使對話中間聊了別的話題，這些核心資訊也不會遺失。
*   **轉場橋樑 (Bridge)**：
    > 了解了單一 Agent 的對話是如何被結構化的，下一個問題自然就是：如果系統中不只有一個 Agent 呢？當多個 Agent 需要協同工作時，它們之間是如何共享或隔離這些對話歷史的？下一頁，我們就來探討多 Agent 系統中的 Session 管理模式。
```</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-8">
            <div class="slide">
                <h2>Slide 08</h2>
                <div class="rendered-content">
                    <h1>Sessions in Multi-Agent Systems</h1>
<p>How do collaborating agents share information?</p>
<ol>
<li>
<p><strong>Shared, Unified History:</strong></p>
<ul>
<li>All agents read from and write to a single, central conversation log.</li>
<li><strong>Best for:</strong> Tightly coupled, collaborative tasks requiring a single source of truth.</li>
</ul>
</li>
<li>
<p><strong>Separate, Individual Histories:</strong></p>
<ul>
<li>Each agent maintains its own private log and acts as a &quot;black box.&quot;</li>
<li>Communication happens via explicit messages (e.g., Agent-as-a-Tool, A2A Protocol).</li>
<li><strong>Best for:</strong> Decoupled tasks where process privacy is desired.</li>
</ul>
</li>
</ol>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 08</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(8, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(8, 'raw')">Source</button>
                </div>
                <div id="note-rendered-8" class="note-content rendered-content">
                    <h3>🎙️ 第 08 頁：Sessions_in_Multi-Agent_Systems</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>在多代理人系統中，資訊共享主要有兩種模式：共享統一歷史紀錄與獨立個別歷史紀錄。</li>
<li><strong>共享歷史紀錄</strong>：所有代理人共用一個中央日誌，適用於需要單一事實來源的緊密協作任務。</li>
<li><strong>獨立歷史紀錄</strong>：每個代理人維護自己的私有日誌，像個「黑盒子」，適用於需要解耦和過程隱私的任務。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好，我們剛剛理解了 Session 如何為「單一」代理人保存對話狀態。但現在，讓我們把格局放大：如果我們建立的不是一個孤單的代理人，而是一個各司其職、需要互相合作的「代理人團隊」，情況會變成怎麼樣呢？它們之間該如何溝通、如何共享資訊？這就是我們這一頁要探討的核心問題。</p>
<p>在深入探討兩種主要架構之前，我們必須先釐清一個非常重要的觀念。</p>
<blockquote>
<p>請記住，代理人系統的「完整對話歷史紀錄」，不等於每一次傳送給 LLM 的「上下文」。</p>
</blockquote>
<p>完整的歷史紀錄就像是會議中一字不漏的逐字稿，記錄了所有人的發言。而上下文，則是為了讓 LLM 完成某個特定任務，從逐字稿中精心挑選、甚至重新編排的「摘要報告」。這兩者是不同的。這一頁我們關注的是，代理人團隊之間如何傳遞那份完整的「逐字稿」。</p>
<h5>① 第一種模式：共享且統一的歷史紀錄 (Shared, Unified History)</h5>
<p>首先，最直觀的方法，就是讓所有代理人共用一本帳本。在這個模式下，系統中會有一個<strong>單一的、中央的對話日誌</strong>。</p>
<p>不論是哪個代理人的訊息、工具呼叫、還是觀察結果，所有事件都會按照時間順序，被記錄到這個唯一的中央日誌裡。這就像一個團隊共同編輯一份 Google 文件，每個人都能看到完整的歷史紀錄和最新進度。</p>
<p>這種方法最適合<strong>高度耦合、需要緊密協作</strong>的任務。例如，一個多步驟的解題流程，前一個代理人的輸出，就是下一個代理人的直接輸入。在這種情況下，擁有一個單一、透明的「事實來源 (single source of truth)」至關重要。</p>
<h5>② 第二種模式：獨立且個別的歷史紀錄 (Separate, Individual Histories)</h5>
<p>另一種方法則完全相反。在這個模式中，每個代理人都維護著自己<strong>私有的對話歷史紀錄</strong>，就像一個個獨立的「黑盒子」。</p>
<p>它們內部的思考過程、中間步驟、工具使用細節，全都保存在自己的小本本裡，對其他代理人是完全不透明的。溝通只會透過「明確的訊息」來進行，也就是說，一個代理人只會告訴別人它最終的結論或產出，而不會透露它是如何得到這個結論的。</p>
<p>這種模式的實現方式，通常是把一個代理人包裝成另一個代理人的「工具」(Agent-as-a-Tool)，或是使用標準化的「代理人對代理人協議」(A2A Protocol) 來進行直接通訊。這就像團隊成員各自完成自己的報告，然後只透過 Email 把最終的 PDF 檔寄給對方，中間的草稿和修改過程別人是看不到的。</p>
<p>這種模式最適合<strong>任務可以被清楚切割、彼此之間比較獨立</strong>，而且希望保持各自運作隱私的場景。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：講完這兩種模式後，可以稍微停頓一下，讓聽眾消化「共用 Google 文件」和「互寄 Email 報告」這兩種截然不同的協作風格。</li>
<li><strong>補充案例</strong>：可以強調，這兩種模式沒有絕對的好壞，完全取決於你想解決的問題類型。緊密協作選「共享」，獨立分工選「分離」。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>無論我們選擇哪種資訊共享模式，當系統從原型走向真實世界時，都會面臨一個共同的挑戰：如何讓這些 Session 變得安全、可靠且能夠規模化？這就帶我們進入下一頁的重點：生產環境中對 Session 的考量。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-8" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 08 頁：Sessions_in_Multi-Agent_Systems

#### 【本頁重點摘要】
*   在多代理人系統中，資訊共享主要有兩種模式：共享統一歷史紀錄與獨立個別歷史紀錄。
*   **共享歷史紀錄**：所有代理人共用一個中央日誌，適用於需要單一事實來源的緊密協作任務。
*   **獨立歷史紀錄**：每個代理人維護自己的私有日誌，像個「黑盒子」，適用於需要解耦和過程隱私的任務。

---

#### 【逐字講稿】

(開場白)
好，我們剛剛理解了 Session 如何為「單一」代理人保存對話狀態。但現在，讓我們把格局放大：如果我們建立的不是一個孤單的代理人，而是一個各司其職、需要互相合作的「代理人團隊」，情況會變成怎麼樣呢？它們之間該如何溝通、如何共享資訊？這就是我們這一頁要探討的核心問題。

在深入探討兩種主要架構之前，我們必須先釐清一個非常重要的觀念。

> 請記住，代理人系統的「完整對話歷史紀錄」，不等於每一次傳送給 LLM 的「上下文」。

完整的歷史紀錄就像是會議中一字不漏的逐字稿，記錄了所有人的發言。而上下文，則是為了讓 LLM 完成某個特定任務，從逐字稿中精心挑選、甚至重新編排的「摘要報告」。這兩者是不同的。這一頁我們關注的是，代理人團隊之間如何傳遞那份完整的「逐字稿」。

##### ① 第一種模式：共享且統一的歷史紀錄 (Shared, Unified History)
首先，最直觀的方法，就是讓所有代理人共用一本帳本。在這個模式下，系統中會有一個**單一的、中央的對話日誌**。

不論是哪個代理人的訊息、工具呼叫、還是觀察結果，所有事件都會按照時間順序，被記錄到這個唯一的中央日誌裡。這就像一個團隊共同編輯一份 Google 文件，每個人都能看到完整的歷史紀錄和最新進度。

這種方法最適合**高度耦合、需要緊密協作**的任務。例如，一個多步驟的解題流程，前一個代理人的輸出，就是下一個代理人的直接輸入。在這種情況下，擁有一個單一、透明的「事實來源 (single source of truth)」至關重要。

##### ② 第二種模式：獨立且個別的歷史紀錄 (Separate, Individual Histories)
另一種方法則完全相反。在這個模式中，每個代理人都維護著自己**私有的對話歷史紀錄**，就像一個個獨立的「黑盒子」。

它們內部的思考過程、中間步驟、工具使用細節，全都保存在自己的小本本裡，對其他代理人是完全不透明的。溝通只會透過「明確的訊息」來進行，也就是說，一個代理人只會告訴別人它最終的結論或產出，而不會透露它是如何得到這個結論的。

這種模式的實現方式，通常是把一個代理人包裝成另一個代理人的「工具」(Agent-as-a-Tool)，或是使用標準化的「代理人對代理人協議」(A2A Protocol) 來進行直接通訊。這就像團隊成員各自完成自己的報告，然後只透過 Email 把最終的 PDF 檔寄給對方，中間的草稿和修改過程別人是看不到的。

這種模式最適合**任務可以被清楚切割、彼此之間比較獨立**，而且希望保持各自運作隱私的場景。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講完這兩種模式後，可以稍微停頓一下，讓聽眾消化「共用 Google 文件」和「互寄 Email 報告」這兩種截然不同的協作風格。
*   **補充案例**：可以強調，這兩種模式沒有絕對的好壞，完全取決於你想解決的問題類型。緊密協作選「共享」，獨立分工選「分離」。
*   **轉場橋樑 (Bridge)**：
    > 無論我們選擇哪種資訊共享模式，當系統從原型走向真實世界時，都會面臨一個共同的挑戰：如何讓這些 Session 變得安全、可靠且能夠規模化？這就帶我們進入下一頁的重點：生產環境中對 Session 的考量。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-9">
            <div class="slide">
                <h2>Slide 09</h2>
                <div class="rendered-content">
                    <h1>Production Considerations for Sessions</h1>
<p>Moving from prototype to production requires an enterprise-grade service.</p>
<ul>
<li>
<p><strong>Security and Privacy:</strong></p>
<ul>
<li><strong>Strict Isolation:</strong> Enforce ACLs to ensure a user can only access their own session data.</li>
<li><strong>PII Redaction:</strong> Redact Personally Identifiable Information <em>before</em> writing to storage to reduce risk.</li>
</ul>
</li>
<li>
<p><strong>Data Integrity and Lifecycle:</strong></p>
<ul>
<li><strong>TTL Policies:</strong> Automatically delete inactive sessions to manage costs.</li>
<li><strong>Deterministic Order:</strong> Guarantee events are appended chronologically.</li>
</ul>
</li>
<li>
<p><strong>Performance and Scalability:</strong></p>
<ul>
<li><strong>Low Latency:</strong> Session data is on the &quot;hot path&quot;; reads/writes must be fast.</li>
<li><strong>Compaction:</strong> Filter or compact history before transfer to reduce latency.</li>
</ul>
</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 09</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(9, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(9, 'raw')">Source</button>
                </div>
                <div id="note-rendered-9" class="note-content rendered-content">
                    <h3>🎙️ 第 09 頁：Production Considerations for Sessions</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>將 AI 應用從原型推向正式產品，必須考慮三大關鍵：安全性、資料完整性，以及效能。</li>
<li><strong>安全性</strong>：透過嚴格的用戶隔離與個資(PII)遮罩，保護用戶隱私。</li>
<li><strong>資料完整性</strong>：透過生命週期管理 (TTL) 與確保事件順序，維持對話紀錄的正確性。</li>
<li><strong>效能</strong>：Session 位於互動的「熱路徑 (hot path)」上，必須確保低延遲讀寫，並透過壓縮 (Compaction) 來優化。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好，到目前為止，我們已經了解了什麼是 Session，以及它在多代理系統中如何運作。我們的原型可能跑得非常順利。但現在，我們必須思考一個殘酷但現實的問題：當我們的應用程式從服務 10 個用戶，變成服務 10 萬、甚至 100 萬個用戶時，會發生什麼事？這就是「生產環境考量」至關重要的原因。這張投影片的三個重點，是將一個有趣的 AI 玩具，變成一個穩健、可信賴的企業級服務的基石。</p>
<h5>① 首先，也是最重要的：安全性與隱私權</h5>
<p>這絕對是不可妥協的要求。當我們處理用戶的對話時，我們手上握有的是極度敏感的資訊。</p>
<ul>
<li>
<p><strong>嚴格隔離 (Strict Isolation)</strong>：請記住，一個 Session 屬於且只屬於一個用戶。我們的系統必須像銀行金庫一樣，強制執行嚴格的存取控制 (ACLs)，確保張三永遠、絕對、不可能看到李四的對話紀錄。這聽起來是基本功，但在生產環境中，這是防止災難性資料外洩的第一道防線。</p>
</li>
<li>
<p><strong>個資遮罩 (PII Redaction)</strong>：一個更進階、但極其重要的最佳實踐是，在將任何對話資料寫入儲存體<strong>之前</strong>，就主動將個人身份資訊 (PII) 進行編輯或遮罩。</p>
</li>
</ul>
<blockquote>
<p>這是一個根本性的安全措施。想像一下，如果真的發生了資料外洩，透過像 Model Armor 這樣的工具，我們能確保攻擊者拿到的資料是不包含真實姓名、電話、地址的「無效資訊」，這將大幅降低損害的「爆炸半徑」，同時也讓我們更容易遵循像 GDPR 或 CCPA 這樣的隱私法規。</p>
</blockquote>
<h5>② 其次，是資料的完整性與生命週期</h5>
<p>一個生產系統需要有清晰的規則，來管理資料如何被儲存與維護。</p>
<ul>
<li>
<p><strong>生命週期策略 (TTL Policies)</strong>：Session 不應該「永生」。我們可以設定一個「存活時間」(Time-to-Live, TTL)，讓系統自動刪除那些長時間不活躍的 Session。這不僅能有效管理儲存成本，更能減少資料管理的負擔，保持系統的整潔。</p>
</li>
<li>
<p><strong>確定性順序 (Deterministic Order)</strong>：系統必須保證，所有對話事件都以正確的、符合時間先後的順序被記錄下來。這對維持對話紀錄的完整性至關重要。如果順序錯了，整個對話的上下文就亂了，AI 代理也會做出完全錯誤的判斷。</p>
</li>
</ul>
<h5>③ 最後，是效能與擴展性</h5>
<p>這直接關係到用戶體驗的好壞。</p>
<ul>
<li>
<p><strong>低延遲 (Low Latency)</strong>：Session 資料位於每次用戶互動的「熱路徑 (hot path)」上。這意味著每一次用戶發送訊息，系統都必須讀取 Session；每一次 AI 回應，系統都必須寫入 Session。這個讀寫過程必須快如閃電，才能確保用戶感覺不到延遲。</p>
</li>
<li>
<p><strong>壓縮 (Compaction)</strong>：我們前面提到，AI 代理通常是無狀態的，這代表每次互動，它都可能需要從資料庫重新載入<strong>整個</strong>對話歷史。當對話變長，傳輸的資料量會暴增，延遲也會飆高。因此，在將歷史紀錄傳送給代理之前，先對其進行「壓縮」或「過濾」，例如移除一些不再需要、無關緊要的工具調用結果，是降低延遲的關鍵優化手段。</p>
</li>
</ul>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：講完「安全性與隱私權」後，可以稍微停頓一下。這個部分是所有考量中最嚴肅也最容易引起共鳴的，給聽眾一點時間吸收它的重要性。</li>
<li><strong>補充案例</strong>：可以口頭舉例：「想像一下，如果你的聊天機器人記住了用戶的信用卡號，卻沒有做 PII 遮罩，那將會是一場公關災難。」</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>我們剛剛在效能部分提到了「壓縮」是管理長對話、降低延遲的關鍵。但問題來了，我們要怎麼壓縮，才能在不丟失重要上下文的前提下，又能有效縮減資料量呢？這就像打包行李，既不能什麼都塞，也不能把護照給丟了。下一頁，我們就來深入探討幾種聰明的「對話壓縮策略」。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-9" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 09 頁：Production Considerations for Sessions

#### 【本頁重點摘要】
*   將 AI 應用從原型推向正式產品，必須考慮三大關鍵：安全性、資料完整性，以及效能。
*   **安全性**：透過嚴格的用戶隔離與個資(PII)遮罩，保護用戶隱私。
*   **資料完整性**：透過生命週期管理 (TTL) 與確保事件順序，維持對話紀錄的正確性。
*   **效能**：Session 位於互動的「熱路徑 (hot path)」上，必須確保低延遲讀寫，並透過壓縮 (Compaction) 來優化。

---

#### 【逐字講稿】

(開場白)
好，到目前為止，我們已經了解了什麼是 Session，以及它在多代理系統中如何運作。我們的原型可能跑得非常順利。但現在，我們必須思考一個殘酷但現實的問題：當我們的應用程式從服務 10 個用戶，變成服務 10 萬、甚至 100 萬個用戶時，會發生什麼事？這就是「生產環境考量」至關重要的原因。這張投影片的三個重點，是將一個有趣的 AI 玩具，變成一個穩健、可信賴的企業級服務的基石。

##### ① 首先，也是最重要的：安全性與隱私權

這絕對是不可妥協的要求。當我們處理用戶的對話時，我們手上握有的是極度敏感的資訊。

*   **嚴格隔離 (Strict Isolation)**：請記住，一個 Session 屬於且只屬於一個用戶。我們的系統必須像銀行金庫一樣，強制執行嚴格的存取控制 (ACLs)，確保張三永遠、絕對、不可能看到李四的對話紀錄。這聽起來是基本功，但在生產環境中，這是防止災難性資料外洩的第一道防線。

*   **個資遮罩 (PII Redaction)**：一個更進階、但極其重要的最佳實踐是，在將任何對話資料寫入儲存體**之前**，就主動將個人身份資訊 (PII) 進行編輯或遮罩。

> 這是一個根本性的安全措施。想像一下，如果真的發生了資料外洩，透過像 Model Armor 這樣的工具，我們能確保攻擊者拿到的資料是不包含真實姓名、電話、地址的「無效資訊」，這將大幅降低損害的「爆炸半徑」，同時也讓我們更容易遵循像 GDPR 或 CCPA 這樣的隱私法規。

##### ② 其次，是資料的完整性與生命週期

一個生產系統需要有清晰的規則，來管理資料如何被儲存與維護。

*   **生命週期策略 (TTL Policies)**：Session 不應該「永生」。我們可以設定一個「存活時間」(Time-to-Live, TTL)，讓系統自動刪除那些長時間不活躍的 Session。這不僅能有效管理儲存成本，更能減少資料管理的負擔，保持系統的整潔。

*   **確定性順序 (Deterministic Order)**：系統必須保證，所有對話事件都以正確的、符合時間先後的順序被記錄下來。這對維持對話紀錄的完整性至關重要。如果順序錯了，整個對話的上下文就亂了，AI 代理也會做出完全錯誤的判斷。

##### ③ 最後，是效能與擴展性

這直接關係到用戶體驗的好壞。

*   **低延遲 (Low Latency)**：Session 資料位於每次用戶互動的「熱路徑 (hot path)」上。這意味著每一次用戶發送訊息，系統都必須讀取 Session；每一次 AI 回應，系統都必須寫入 Session。這個讀寫過程必須快如閃電，才能確保用戶感覺不到延遲。

*   **壓縮 (Compaction)**：我們前面提到，AI 代理通常是無狀態的，這代表每次互動，它都可能需要從資料庫重新載入**整個**對話歷史。當對話變長，傳輸的資料量會暴增，延遲也會飆高。因此，在將歷史紀錄傳送給代理之前，先對其進行「壓縮」或「過濾」，例如移除一些不再需要、無關緊要的工具調用結果，是降低延遲的關鍵優化手段。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講完「安全性與隱私權」後，可以稍微停頓一下。這個部分是所有考量中最嚴肅也最容易引起共鳴的，給聽眾一點時間吸收它的重要性。
*   **補充案例**：可以口頭舉例：「想像一下，如果你的聊天機器人記住了用戶的信用卡號，卻沒有做 PII 遮罩，那將會是一場公關災難。」
*   **轉場橋樑 (Bridge)**：
    > 我們剛剛在效能部分提到了「壓縮」是管理長對話、降低延遲的關鍵。但問題來了，我們要怎麼壓縮，才能在不丟失重要上下文的前提下，又能有效縮減資料量呢？這就像打包行李，既不能什麼都塞，也不能把護照給丟了。下一頁，我們就來深入探討幾種聰明的「對話壓縮策略」。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-10">
            <div class="slide">
                <h2>Slide 10</h2>
                <div class="rendered-content">
                    <h1>Managing Long Conversations: Compaction</h1>
<p><strong>Problem:</strong> As conversation history grows, it hits limits and increases cost, latency, and noise.</p>
<p><strong>Analogy: Packing a Suitcase</strong></p>
<ul>
<li>You can't stuff everything in; it becomes heavy and disorganized.</li>
<li>You can't pack too little; you'll miss essential items.</li>
<li>Success hinges on carrying <em>only what you need</em>.</li>
</ul>
<p><strong>Compaction Strategies:</strong></p>
<ul>
<li><strong>Keep Last N Turns:</strong> A simple &quot;sliding window.&quot;</li>
<li><strong>Token-Based Truncation:</strong> Cut off older messages once a token limit is reached.</li>
<li><strong>Recursive Summarization:</strong> Replace older parts of the conversation with an AI-generated summary.</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 10</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(10, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(10, 'raw')">Source</button>
                </div>
                <div id="note-rendered-10" class="note-content rendered-content">
                    <h3>🎙️ 第 10 頁：Managing_Long_Conversations_Compaction_Strategies</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li><strong>問題核心</strong>：隨著對話變長，歷史紀錄會超出模型的上下文限制，並導致成本、延遲和雜訊增加。</li>
<li><strong>核心比喻</strong>：這就像打包行李箱，成功不在於你能帶多少，而在於你只帶了你需要的東西。</li>
<li><strong>解決策略</strong>：介紹三種主要的對話歷史壓縮方法：保留最近 N 輪、基於 Token 的截斷、以及遞歸式摘要。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好，我們剛剛談完了 Session，也就是對話的「工作台」。但如果一場對話持續很久，這個工作台上就會堆滿越來越多的資料。這會引發一個非常現實的問題：我們要如何管理這些不斷增長的對話歷史呢？</p>
<p>這頁投影片的比喻非常貼切，大家可以想像一下，<strong>管理一個長對話，就像一位聰明的旅行者在為長途旅行打包行李箱。</strong></p>
<blockquote>
<p>如果你試圖把所有東西都塞進去，行李箱會變得又重又亂，很難快速找到你需要​​的東西——這就像一個超載的上下文視窗，會增加處理成本並減慢回應時間。</p>
<p>但反過來說，如果你打包得太少，你可能會漏掉像護照或外套這樣的重要物品，導致整個旅程出問題——這就像一個 Agent 失去了關鍵的上下文，給出不相關或錯誤的答案。</p>
</blockquote>
<p>所以，無論是旅行者還是 AI Agent，成功的關鍵都不在於你能攜帶多少東西，而在於 <strong>你只攜帶你真正需要的東西</strong>。這就是「壓縮 (Compaction)」的核心思想。</p>
<p>那麼，為什麼我們需要這麼費心去壓縮對話歷史呢？主要有四個原因：</p>
<h5>① Context Window 限制</h5>
<p>每個 LLM 都有它能一次處理的最大文字量，也就是「上下文視窗」。如果對話歷史超過這個限制，API 呼叫就會直接失敗。</p>
<h5>② API 成本</h5>
<p>大多數 LLM 服務是按 Token 數量收費的。更短的歷史意味著更少的 Token，也意味著每一輪對話的成本更低。</p>
<h5>③ 延遲與速度</h5>
<p>傳送給模型的文字越多，處理時間就越長，用戶等待回應的時間也就越久。壓縮能讓我們的 Agent 感覺更敏捷、反應更快速。</p>
<h5>④ 回應品質</h5>
<p>最後，當 Token 數量增加時，模型可能會因為上下文中的「雜訊」過多而表現變差，這種現象有時被稱為「上下文腐爛 (context rot)」，模型的注意力會被分散，難以聚焦在最重要的資訊上。</p>
<p>為了解決這些問題，我們有幾種核心的壓縮策略：</p>
<ul>
<li><strong>第一種，最簡單的，就是「保留最近 N 輪對話」</strong>。這就像一個滑動的視窗，我們只保留最近的幾次互動，然後把更早的內容全部丟掉。</li>
<li><strong>第二種，是「基於 Token 的截斷」</strong>。在每次發送請求前，系統會從最新的訊息開始往回計算 Token 數量，直到達到一個預設的上限（比如 4000 個 Token）。超出這個範圍的舊訊息就會被直接切掉。</li>
<li><strong>第三種，也是最精密的，叫做「遞歸式摘要」</strong>。這個方法會用另一個 LLM 呼叫，將對話中較早的部分生成一個摘要。隨著對話越來越長，系統會定期地把最舊的訊息「總結」起來，用這個濃縮後的摘要來代替原本冗長的對話紀錄。</li>
</ul>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：在講完「打包行李箱」的比喻後，可以稍微停頓一下，確保聽眾理解了這個核心概念。</li>
<li><strong>補充案例</strong>：如果時間允許，可以提到 <code>source_file</code> 中有 ADK 的程式碼範例，展示了如何用一個簡單的插件來實現「保留最近 N 輪對話」的功能，這能讓概念更具體。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>我們現在知道了如何聰明地「打包行李箱」，透過壓縮策略來管理對話的長度。但問題來了：那些被我們篩選出來、認為「至關重要」的資訊，該如何被長期保存與利用呢？這就引出了我們 Context Engineering 的第二個核心組件——<strong>Memory</strong>。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-10" class="note-content note-raw" style="display: none;">
                    <pre>```markdown
### 🎙️ 第 10 頁：Managing_Long_Conversations_Compaction_Strategies

#### 【本頁重點摘要】
*   **問題核心**：隨著對話變長，歷史紀錄會超出模型的上下文限制，並導致成本、延遲和雜訊增加。
*   **核心比喻**：這就像打包行李箱，成功不在於你能帶多少，而在於你只帶了你需要的東西。
*   **解決策略**：介紹三種主要的對話歷史壓縮方法：保留最近 N 輪、基於 Token 的截斷、以及遞歸式摘要。

---

#### 【逐字講稿】

(開場白)
好，我們剛剛談完了 Session，也就是對話的「工作台」。但如果一場對話持續很久，這個工作台上就會堆滿越來越多的資料。這會引發一個非常現實的問題：我們要如何管理這些不斷增長的對話歷史呢？

這頁投影片的比喻非常貼切，大家可以想像一下，**管理一個長對話，就像一位聰明的旅行者在為長途旅行打包行李箱。**

> 如果你試圖把所有東西都塞進去，行李箱會變得又重又亂，很難快速找到你需要​​的東西——這就像一個超載的上下文視窗，會增加處理成本並減慢回應時間。
>
> 但反過來說，如果你打包得太少，你可能會漏掉像護照或外套這樣的重要物品，導致整個旅程出問題——這就像一個 Agent 失去了關鍵的上下文，給出不相關或錯誤的答案。

所以，無論是旅行者還是 AI Agent，成功的關鍵都不在於你能攜帶多少東西，而在於 **你只攜帶你真正需要的東西**。這就是「壓縮 (Compaction)」的核心思想。

那麼，為什麼我們需要這麼費心去壓縮對話歷史呢？主要有四個原因：

##### ① Context Window 限制
每個 LLM 都有它能一次處理的最大文字量，也就是「上下文視窗」。如果對話歷史超過這個限制，API 呼叫就會直接失敗。

##### ② API 成本
大多數 LLM 服務是按 Token 數量收費的。更短的歷史意味著更少的 Token，也意味著每一輪對話的成本更低。

##### ③ 延遲與速度
傳送給模型的文字越多，處理時間就越長，用戶等待回應的時間也就越久。壓縮能讓我們的 Agent 感覺更敏捷、反應更快速。

##### ④ 回應品質
最後，當 Token 數量增加時，模型可能會因為上下文中的「雜訊」過多而表現變差，這種現象有時被稱為「上下文腐爛 (context rot)」，模型的注意力會被分散，難以聚焦在最重要的資訊上。

為了解決這些問題，我們有幾種核心的壓縮策略：

*   **第一種，最簡單的，就是「保留最近 N 輪對話」**。這就像一個滑動的視窗，我們只保留最近的幾次互動，然後把更早的內容全部丟掉。
*   **第二種，是「基於 Token 的截斷」**。在每次發送請求前，系統會從最新的訊息開始往回計算 Token 數量，直到達到一個預設的上限（比如 4000 個 Token）。超出這個範圍的舊訊息就會被直接切掉。
*   **第三種，也是最精密的，叫做「遞歸式摘要」**。這個方法會用另一個 LLM 呼叫，將對話中較早的部分生成一個摘要。隨著對話越來越長，系統會定期地把最舊的訊息「總結」起來，用這個濃縮後的摘要來代替原本冗長的對話紀錄。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在講完「打包行李箱」的比喻後，可以稍微停頓一下，確保聽眾理解了這個核心概念。
*   **補充案例**：如果時間允許，可以提到 `source_file` 中有 ADK 的程式碼範例，展示了如何用一個簡單的插件來實現「保留最近 N 輪對話」的功能，這能讓概念更具體。
*   **轉場橋樑 (Bridge)**：
    > 我們現在知道了如何聰明地「打包行李箱」，透過壓縮策略來管理對話的長度。但問題來了：那些被我們篩選出來、認為「至關重要」的資訊，該如何被長期保存與利用呢？這就引出了我們 Context Engineering 的第二個核心組件——**Memory**。
```</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-11">
            <div class="slide">
                <h2>Slide 11</h2>
                <div class="rendered-content">
                    <h1>Core Component 2: Memory</h1>
<p><strong>Memory is a snapshot of extracted, meaningful information from a conversation or data source, persisted across sessions.</strong></p>
<ul>
<li><strong>Symbiotic Relationship:</strong> Sessions are the primary source for generating memories, and memories help manage session size.</li>
<li><strong>Personalization Engine:</strong> Provides a continuous and personalized experience for the user.</li>
<li><strong>Interoperability Layer:</strong> Memory managers often use framework-agnostic data structures (strings, dicts), allowing agents built on different frameworks to share a knowledge base.</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 11</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(11, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(11, 'raw')">Source</button>
                </div>
                <div id="note-rendered-11" class="note-content rendered-content">
                    <h3>🎙️ 第 11 頁：Core_Component_2_Memory</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>記憶 (Memory) 是從對話或數據源中提取的、跨 session 持續存在的關鍵資訊快照。</li>
<li>記憶與 Session 存在共生關係：Session 是生成記憶的來源，而記憶有助於管理 Session 的大小。</li>
<li>記憶是實現個人化體驗的引擎，並作為一個互通層，讓不同框架的 Agent 能共享知識。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
各位，我們剛剛談完了 Session，那個我們比喻為「專案工作台」的組件。但想像一下，當一個專案結束後，你不會把整個凌亂不堪的工作台直接塞進儲藏室吧？你會整理它。這個「整理」的過程，就帶我們進入了 Context Engineering 的第二個核心組件：<strong>記憶 (Memory)</strong>。</p>
<h5>① 什麼是記憶？從工作台到檔案櫃</h5>
<p>如果說 Session 是那個臨時、混亂但充滿了即時資訊的<strong>工作台</strong>，那麼記憶，就是一個整理得井井有條的<strong>檔案櫃</strong>。</p>
<blockquote>
<p>你會審視工作台上的所有資料，丟掉草稿和多餘的筆記，只把最關鍵、最精華的最終文件，分門別類地放進檔案夾裡。這確保了你的檔案櫃，成為未來所有專案一個乾淨、可靠且高效的真相來源。</p>
</blockquote>
<p>這就是記憶的核心概念。它不是原始的對話紀錄，而是經過提煉的精華。</p>
<blockquote>
<p><strong>記憶，是一個從對話或數據源中提取出的、有意義的資訊快照，它會被跨 session 持久化保存。</strong></p>
</blockquote>
<p>它是一個濃縮的表示，保留了重要的上下文，以便在未來的互動中發揮作用。</p>
<h5>② 共生關係與個人化引擎</h5>
<p>記憶和 Session 之間，存在著一種深刻的<strong>共生關係</strong>。</p>
<ul>
<li>首先，<strong>Session 是生成記憶的主要數據來源</strong>。我們從即時的對話中，提煉出值得長期保存的知識。</li>
<li>反過來，<strong>記憶也是管理 Session 大小的關鍵策略</strong>。例如，透過將冗長的對話總結成一條記憶，我們就能有效縮減 Session 的長度，降低成本與延遲。</li>
</ul>
<p>而這一切最終指向的，是記憶最令人興奮的能力：它是一個<strong>個人化引擎</strong>。它讓我們的 AI Agent 不再是只有七秒記憶的金魚，而是能真正了解你的夥伴。例如，記住你最喜歡的球隊、你對某個專案的特定看法，或者你搭飛機時偏好靠窗還是走道的座位。這一切都讓互動體驗變得無比貼心和高效。</p>
<h5>③ 互通層：打破框架的壁壘</h5>
<p>最後，記憶還有一個極其重要的策略性角色：它是一個<strong>互通層 (Interoperability Layer)</strong>。</p>
<p>我們知道，市面上有許多不同的 Agent 框架，比如 ADK、LangGraph 等等。它們各自的 Session 儲存方式通常互不相容，就像是不同公司內部的檔案系統，彼此無法讀取。</p>
<p>但一個設計良好的記憶管理器，通常會使用<strong>與框架無關</strong>的數據結構，例如簡單的字串或字典（Dictionary）。這使得記憶系統就像一個中立的、共享的知識庫。不同框架開發的 Agent，都可以連接到同一個記憶儲存庫，讀取和寫入資訊。這就打破了技術壁壘，讓一個由多個異構 Agent 組成的複雜系統，能夠共享一個<strong>共同的認知資源</strong>，實現真正的協同智慧。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：在講完「工作台與檔案櫃」這個比喻後，可以稍微停頓一下，讓聽眾充分消化這個核心概念。</li>
<li><strong>補充案例</strong>：可以口頭補充：「這就像一個聰明的助理，他不會記下你說的每一句廢話，但他會記下你下週要出差，並且對花生過敏。」</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>了解了記憶是什麼，以及它在個人化與系統整合中的關鍵角色後，下一頁，我們將深入探討，一個強大的記憶系統，究竟能為我們的 AI Agent 解鎖哪些驚人的具體能力。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-11" class="note-content note-raw" style="display: none;">
                    <pre>```markdown
### 🎙️ 第 11 頁：Core_Component_2_Memory

#### 【本頁重點摘要】
*   記憶 (Memory) 是從對話或數據源中提取的、跨 session 持續存在的關鍵資訊快照。
*   記憶與 Session 存在共生關係：Session 是生成記憶的來源，而記憶有助於管理 Session 的大小。
*   記憶是實現個人化體驗的引擎，並作為一個互通層，讓不同框架的 Agent 能共享知識。

---

#### 【逐字講稿】

(開場白)
各位，我們剛剛談完了 Session，那個我們比喻為「專案工作台」的組件。但想像一下，當一個專案結束後，你不會把整個凌亂不堪的工作台直接塞進儲藏室吧？你會整理它。這個「整理」的過程，就帶我們進入了 Context Engineering 的第二個核心組件：**記憶 (Memory)**。

##### ① 什麼是記憶？從工作台到檔案櫃

如果說 Session 是那個臨時、混亂但充滿了即時資訊的**工作台**，那麼記憶，就是一個整理得井井有條的**檔案櫃**。

> 你會審視工作台上的所有資料，丟掉草稿和多餘的筆記，只把最關鍵、最精華的最終文件，分門別類地放進檔案夾裡。這確保了你的檔案櫃，成為未來所有專案一個乾淨、可靠且高效的真相來源。

這就是記憶的核心概念。它不是原始的對話紀錄，而是經過提煉的精華。

> **記憶，是一個從對話或數據源中提取出的、有意義的資訊快照，它會被跨 session 持久化保存。**

它是一個濃縮的表示，保留了重要的上下文，以便在未來的互動中發揮作用。

##### ② 共生關係與個人化引擎

記憶和 Session 之間，存在著一種深刻的**共生關係**。

*   首先，**Session 是生成記憶的主要數據來源**。我們從即時的對話中，提煉出值得長期保存的知識。
*   反過來，**記憶也是管理 Session 大小的關鍵策略**。例如，透過將冗長的對話總結成一條記憶，我們就能有效縮減 Session 的長度，降低成本與延遲。

而這一切最終指向的，是記憶最令人興奮的能力：它是一個**個人化引擎**。它讓我們的 AI Agent 不再是只有七秒記憶的金魚，而是能真正了解你的夥伴。例如，記住你最喜歡的球隊、你對某個專案的特定看法，或者你搭飛機時偏好靠窗還是走道的座位。這一切都讓互動體驗變得無比貼心和高效。

##### ③ 互通層：打破框架的壁壘

最後，記憶還有一個極其重要的策略性角色：它是一個**互通層 (Interoperability Layer)**。

我們知道，市面上有許多不同的 Agent 框架，比如 ADK、LangGraph 等等。它們各自的 Session 儲存方式通常互不相容，就像是不同公司內部的檔案系統，彼此無法讀取。

但一個設計良好的記憶管理器，通常會使用**與框架無關**的數據結構，例如簡單的字串或字典（Dictionary）。這使得記憶系統就像一個中立的、共享的知識庫。不同框架開發的 Agent，都可以連接到同一個記憶儲存庫，讀取和寫入資訊。這就打破了技術壁壘，讓一個由多個異構 Agent 組成的複雜系統，能夠共享一個**共同的認知資源**，實現真正的協同智慧。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在講完「工作台與檔案櫃」這個比喻後，可以稍微停頓一下，讓聽眾充分消化這個核心概念。
*   **補充案例**：可以口頭補充：「這就像一個聰明的助理，他不會記下你說的每一句廢話，但他會記下你下週要出差，並且對花生過敏。」
*   **轉場橋樑 (Bridge)**：
    > 了解了記憶是什麼，以及它在個人化與系統整合中的關鍵角色後，下一頁，我們將深入探討，一個強大的記憶系統，究竟能為我們的 AI Agent 解鎖哪些驚人的具體能力。

```</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-12">
            <div class="slide">
                <h2>Slide 12</h2>
                <div class="rendered-content">
                    <h1>Key Capabilities Unlocked by Memory</h1>
<p>A robust memory system transforms a chatbot into an intelligent agent.</p>
<ol>
<li>
<p><strong>Personalization:</strong></p>
<ul>
<li>Remember user preferences, facts, and past interactions to tailor responses.</li>
</ul>
</li>
<li>
<p><strong>Context Window Management:</strong></p>
<ul>
<li>Compact long histories by creating summaries or extracting key facts, reducing cost and latency.</li>
</ul>
</li>
<li>
<p><strong>Data Mining and Insight:</strong></p>
<ul>
<li>Analyze stored memories across many users (in aggregate) to identify trends and issues.</li>
</ul>
</li>
<li>
<p><strong>Agent Self-Improvement:</strong></p>
<ul>
<li>Create procedural memories about which strategies and tools led to successful outcomes.</li>
</ul>
</li>
</ol>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 12</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(12, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(12, 'raw')">Source</button>
                </div>
                <div id="note-rendered-12" class="note-content rendered-content">
                    <h3>🎙️ 第 12 頁：Key_Capabilities_Unlocked_by_Memory</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>記憶系統能將基礎的聊天機器人，轉變為真正智慧的代理人。</li>
<li>它能實現四大核心能力：個人化、上下文視窗管理、數據洞察，以及代理人自身的學習與適應。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好，我們剛剛理解了什麼是「記憶」。但一個好的記憶系統，並不僅僅是讓 AI 「記住」事情而已，它的真正價值，在於它能將一個普通的聊天機器人，徹底轉變為一個我們所說的「智慧代理人」。這頁投影片，我們就要來拆解，記憶系統到底解鎖了哪些驚人的核心能力。</p>
<h5>① 首先，也是最直覺的一點：個人化 (Personalization)</h5>
<p>這大概是大家最常想到的應用。一個具備記憶能力的代理人，可以記住你的偏好、過去的互動、甚至是你提過的一些個人事實。</p>
<blockquote>
<p>舉例來說，它可以記住你最喜歡的球隊，或你搭飛機時偏好靠窗還是走道的座位。當你下次與它互動時，它就能提供更貼心、更個人化的建議，而不是每一次都像在跟陌生人說話。</p>
</blockquote>
<p>這種體驗的轉變，是建立長期信任與實用性的第一步。</p>
<h5>② 第二點，更技術性但同樣關鍵：上下文視窗管理 (Context Window Management)</h5>
<p>我們知道，LLM 的上下文視窗是有限的。當對話越來越長，我們不可能把數千、數萬字的歷史紀錄每一次都塞給模型。這不僅成本高昂，而且會嚴重拖慢反應速度。</p>
<p>記憶系統在這裡扮演了「壓縮大師」的角色。它可以將冗長的對話歷史，<strong>濃縮成摘要或提取出關鍵事實</strong>。這樣一來，既能保留重要的上下文，又不必在每一次互動時都傳送整個對話歷史，從而有效降低了成本和延遲。</p>
<h5>③ 第三點，從數據中挖掘洞見 (Data Mining and Insight)</h5>
<p>當記憶系統被應用在大量用戶上時，它的價值就超越了個人化。透過分析<strong>經過聚合且保護隱私</strong>的用戶記憶，我們可以從海量的對話噪音中，提取出有價值的商業洞見。</p>
<blockquote>
<p>想像一個零售業的聊天機器人。如果系統發現，有大量的用戶都在詢問某個特定產品的退貨政策，這可能就標示著這個產品的說明頁面不夠清楚，或產品本身有潛在問題。這就是從記憶中挖掘出的、可指導行動的洞察。</p>
</blockquote>
<h5>④ 最後一點，也是最令人興奮的：代理人的自我改進與適應 (Agent Self-Improvement)</h5>
<p>這一點，我們稱之為「程序性記憶 (procedural memories)」。代理人不僅能記住「事實」，更能記住「方法」。</p>
<p>它會記錄下哪些策略、哪些工具組合，或哪些推理路徑，在過去成功解決了問題。這等於是讓代理人為自己建立一本<strong>成功的劇本 (playbook)</strong>。當未來遇到類似的挑戰時，它就能參考這些成功的經驗，更有效率地解決問題。這讓代理人擁有了隨著時間推移，不斷學習和進化的能力。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：講完這四點後，可以稍微停頓一下，讓聽眾消化這四個強大的能力。每一點都代表了從「聊天」到「智慧」的一大步。</li>
<li><strong>補充案例</strong>：可以強調，這些能力並非空想，而是有具體的應用場景，例如個人化助理、高效的客服系統、甚至是能自我優化的複雜任務代理人。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>講到這裡，你可能會想：「等等，你說的這些，聽起來跟我們之前提到的 RAG 有點像啊？都是從外部獲取知識。」這是一個非常好的問題！它們看似相似，但本質目標完全不同。下一頁，我們就來深入比較 Memory 和 RAG，看看一個如何讓代理人成為「用戶專家」，另一個又如何讓它成為「事實專家」。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-12" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 12 頁：Key_Capabilities_Unlocked_by_Memory

#### 【本頁重點摘要】
*   記憶系統能將基礎的聊天機器人，轉變為真正智慧的代理人。
*   它能實現四大核心能力：個人化、上下文視窗管理、數據洞察，以及代理人自身的學習與適應。

---

#### 【逐字講稿】

(開場白)
好，我們剛剛理解了什麼是「記憶」。但一個好的記憶系統，並不僅僅是讓 AI 「記住」事情而已，它的真正價值，在於它能將一個普通的聊天機器人，徹底轉變為一個我們所說的「智慧代理人」。這頁投影片，我們就要來拆解，記憶系統到底解鎖了哪些驚人的核心能力。

##### ① 首先，也是最直覺的一點：個人化 (Personalization)
這大概是大家最常想到的應用。一個具備記憶能力的代理人，可以記住你的偏好、過去的互動、甚至是你提過的一些個人事實。

> 舉例來說，它可以記住你最喜歡的球隊，或你搭飛機時偏好靠窗還是走道的座位。當你下次與它互動時，它就能提供更貼心、更個人化的建議，而不是每一次都像在跟陌生人說話。

這種體驗的轉變，是建立長期信任與實用性的第一步。

##### ② 第二點，更技術性但同樣關鍵：上下文視窗管理 (Context Window Management)
我們知道，LLM 的上下文視窗是有限的。當對話越來越長，我們不可能把數千、數萬字的歷史紀錄每一次都塞給模型。這不僅成本高昂，而且會嚴重拖慢反應速度。

記憶系統在這裡扮演了「壓縮大師」的角色。它可以將冗長的對話歷史，**濃縮成摘要或提取出關鍵事實**。這樣一來，既能保留重要的上下文，又不必在每一次互動時都傳送整個對話歷史，從而有效降低了成本和延遲。

##### ③ 第三點，從數據中挖掘洞見 (Data Mining and Insight)
當記憶系統被應用在大量用戶上時，它的價值就超越了個人化。透過分析**經過聚合且保護隱私**的用戶記憶，我們可以從海量的對話噪音中，提取出有價值的商業洞見。

> 想像一個零售業的聊天機器人。如果系統發現，有大量的用戶都在詢問某個特定產品的退貨政策，這可能就標示著這個產品的說明頁面不夠清楚，或產品本身有潛在問題。這就是從記憶中挖掘出的、可指導行動的洞察。

##### ④ 最後一點，也是最令人興奮的：代理人的自我改進與適應 (Agent Self-Improvement)
這一點，我們稱之為「程序性記憶 (procedural memories)」。代理人不僅能記住「事實」，更能記住「方法」。

它會記錄下哪些策略、哪些工具組合，或哪些推理路徑，在過去成功解決了問題。這等於是讓代理人為自己建立一本**成功的劇本 (playbook)**。當未來遇到類似的挑戰時，它就能參考這些成功的經驗，更有效率地解決問題。這讓代理人擁有了隨著時間推移，不斷學習和進化的能力。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講完這四點後，可以稍微停頓一下，讓聽眾消化這四個強大的能力。每一點都代表了從「聊天」到「智慧」的一大步。
*   **補充案例**：可以強調，這些能力並非空想，而是有具體的應用場景，例如個人化助理、高效的客服系統、甚至是能自我優化的複雜任務代理人。
*   **轉場橋樑 (Bridge)**：
    > 講到這裡，你可能會想：「等等，你說的這些，聽起來跟我們之前提到的 RAG 有點像啊？都是從外部獲取知識。」這是一個非常好的問題！它們看似相似，但本質目標完全不同。下一頁，我們就來深入比較 Memory 和 RAG，看看一個如何讓代理人成為「用戶專家」，另一個又如何讓它成為「事實專家」。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-13">
            <div class="slide">
                <h2>Slide 13</h2>
                <div class="rendered-content">
                    <h1>Memory vs. RAG</h1>
<p><strong>RAG makes an agent an expert on facts. Memory makes it an expert on the user.</strong></p>
<p>| | <strong>RAG Engines</strong> | <strong>Memory Managers</strong> |
| :--- | :--- | :--- |
| <strong>Goal</strong> | Inject external, factual knowledge | Create a personalized, stateful experience |
| <strong>Data Source</strong> | Static documents (PDFs, wikis) | Dynamic user-agent dialogue |
| <strong>Isolation</strong> | Shared, global resource | Highly isolated per-user |
| <strong>Write Pattern</strong> | Offline batch processing | Real-time, event-based processing |</p>
<p><strong>Analogy:</strong></p>
<ul>
<li><strong>RAG is the Research Librarian:</strong> An expert on the world's facts.</li>
<li><strong>Memory is the Personal Assistant:</strong> An expert on the user themselves.</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 13</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(13, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(13, 'raw')">Source</button>
                </div>
                <div id="note-rendered-13" class="note-content rendered-content">
                    <h3>🎙️ 第 13 頁：Memory vs. RAG: User Expert vs. Fact Expert</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>RAG (檢索增強生成) 與 Memory (記憶) 的核心目標不同：RAG 讓代理人成為「事實專家」，而 Memory 讓它成為「用戶專家」。</li>
<li>RAG 處理的是靜態、共享的外部知識；Memory 處理的是動態、個人化的對話歷史。</li>
<li>RAG 的知識庫通常是離線批次更新的，而 Memory 是即時、事件驅動的。</li>
<li>一個頂尖的代理人需要兩者兼備：如同既有博學的「研究館員」(RAG)，也有貼心的「個人助理」(Memory)。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，在我們深入探討記憶的內部結構之前，有一個非常重要的觀念需要釐清。很多人會把「記憶」和我們常聽到的「RAG」搞混，因為它們都涉及到從外部為模型提供資訊。但事實上，它們的目標和運作方式有著天壤之別。這頁投影片的標題，就是我們今天最想傳達的核心觀點：<strong>RAG 讓代理人成為事實專家，而 Memory 則讓它成為用戶專家。</strong></p>
<h5>① 核心類比：研究館員 vs. 個人助理</h5>
<p>為了讓大家更容易理解，我們來用一個生動的比喻。</p>
<blockquote>
<p><strong>RAG 就像是代理人專屬的「研究館員」</strong>。他坐在一座巨大的公共圖書館裡，館內藏有百科全書、技術文件、官方報告等各種權威資料。當代理人需要一個確切的事實，比如某個產品的規格或一個歷史日期時，它就會去諮詢這位館員。館員提供的資訊是靜態的、權威的、並且對所有人一視同仁。館員是世界事實的專家，但他不認識你。</p>
</blockquote>
<blockquote>
<p><strong>而 Memory，則是代理人的「個人助理」</strong>。這位助理隨時跟在你身邊，拿著一本私密的筆記本，記錄下你每一次的互動細節、你的偏好、你的目標。這本筆記是動態的、高度個人化的。當代理人需要回憶上週的專案進度，或是你最喜歡的球隊時，它就會翻閱這本筆記。助理不是事實專家，但他是<strong>關於「你」這位用戶的專家</strong>。</p>
</blockquote>
<h5>② 拆解表格中的差異</h5>
<p>這個比喻，完美地對應了我們投影片上的表格。讓我們快速看一下：</p>
<ul>
<li><strong>目標 (Goal)</strong>：RAG 的目標是注入<strong>外部事實知識</strong>，而 Memory 的目標是創造<strong>個人化的、有狀態的體驗</strong>。</li>
<li><strong>資料來源 (Data Source)</strong>：RAG 的來源是<strong>靜態文件</strong>，像是 PDF 或 Wiki；Memory 的來源則是<strong>動態的、你和代理人之間的對話</strong>。</li>
<li><strong>隔離級別 (Isolation)</strong>：RAG 的知識庫是<strong>共享的</strong>，所有用戶存取的是同一個；而 Memory 則是<strong>高度隔離的</strong>，專屬於每一個用戶，絕不混淆。</li>
<li><strong>寫入模式 (Write Pattern)</strong>：這一點非常關鍵。RAG 的知識庫通常是<strong>離線批次處理 (Offline batch processing)</strong> 的。就像圖書館一次性採購並編目一大批新書。而 Memory 則是<strong>即時的、事件驅動的 (Real-time, event-based processing)</strong>。你的個人助理是在你說話的當下，就不斷地在筆記本上寫下新的內容。</li>
</ul>
<h5>③ 結論：兩者缺一不可</h5>
<p>所以，一個真正頂尖的智慧代理人，需要同時擁有這兩位專家。它需要 RAG 這位研究館員，來提供關於世界的廣博知識；同時，它也需要 Memory 這位個人助理，來提供關於它正在服務的這位用戶的深度理解。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：這個類比非常重要，可以稍微放慢速度，確保聽眾完全理解「研究館員」和「個人助理」之間的區別。這是理解後續內容的基礎。</li>
<li><strong>互動建議</strong>：可以問聽眾：「大家覺得，在一個訂餐機器人中，查詢『菜單品項和價格』這個任務，主要是靠 RAG 還是 Memory？那如果它要記住『我對花生過敏』，這又是誰的工作呢？」引導他們思考。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>好了，既然我們已經清楚地劃分了 Memory 和 RAG 的界線，現在，就讓我們把焦點完全放在這位「個人助理」身上。他的那本神奇的筆記本裡，到底記錄了些什麼？又是如何組織的呢？下一頁，我們將深入剖析記憶的內部結構。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-13" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 13 頁：Memory vs. RAG: User Expert vs. Fact Expert

#### 【本頁重點摘要】
*   RAG (檢索增強生成) 與 Memory (記憶) 的核心目標不同：RAG 讓代理人成為「事實專家」，而 Memory 讓它成為「用戶專家」。
*   RAG 處理的是靜態、共享的外部知識；Memory 處理的是動態、個人化的對話歷史。
*   RAG 的知識庫通常是離線批次更新的，而 Memory 是即時、事件驅動的。
*   一個頂尖的代理人需要兩者兼備：如同既有博學的「研究館員」(RAG)，也有貼心的「個人助理」(Memory)。

---

#### 【逐字講稿】

(開場白)
好的，在我們深入探討記憶的內部結構之前，有一個非常重要的觀念需要釐清。很多人會把「記憶」和我們常聽到的「RAG」搞混，因為它們都涉及到從外部為模型提供資訊。但事實上，它們的目標和運作方式有著天壤之別。這頁投影片的標題，就是我們今天最想傳達的核心觀點：**RAG 讓代理人成為事實專家，而 Memory 則讓它成為用戶專家。**

##### ① 核心類比：研究館員 vs. 個人助理
為了讓大家更容易理解，我們來用一個生動的比喻。

> **RAG 就像是代理人專屬的「研究館員」**。他坐在一座巨大的公共圖書館裡，館內藏有百科全書、技術文件、官方報告等各種權威資料。當代理人需要一個確切的事實，比如某個產品的規格或一個歷史日期時，它就會去諮詢這位館員。館員提供的資訊是靜態的、權威的、並且對所有人一視同仁。館員是世界事實的專家，但他不認識你。

> **而 Memory，則是代理人的「個人助理」**。這位助理隨時跟在你身邊，拿著一本私密的筆記本，記錄下你每一次的互動細節、你的偏好、你的目標。這本筆記是動態的、高度個人化的。當代理人需要回憶上週的專案進度，或是你最喜歡的球隊時，它就會翻閱這本筆記。助理不是事實專家，但他是**關於「你」這位用戶的專家**。

##### ② 拆解表格中的差異
這個比喻，完美地對應了我們投影片上的表格。讓我們快速看一下：

*   **目標 (Goal)**：RAG 的目標是注入**外部事實知識**，而 Memory 的目標是創造**個人化的、有狀態的體驗**。
*   **資料來源 (Data Source)**：RAG 的來源是**靜態文件**，像是 PDF 或 Wiki；Memory 的來源則是**動態的、你和代理人之間的對話**。
*   **隔離級別 (Isolation)**：RAG 的知識庫是**共享的**，所有用戶存取的是同一個；而 Memory 則是**高度隔離的**，專屬於每一個用戶，絕不混淆。
*   **寫入模式 (Write Pattern)**：這一點非常關鍵。RAG 的知識庫通常是**離線批次處理 (Offline batch processing)** 的。就像圖書館一次性採購並編目一大批新書。而 Memory 則是**即時的、事件驅動的 (Real-time, event-based processing)**。你的個人助理是在你說話的當下，就不斷地在筆記本上寫下新的內容。

##### ③ 結論：兩者缺一不可
所以，一個真正頂尖的智慧代理人，需要同時擁有這兩位專家。它需要 RAG 這位研究館員，來提供關於世界的廣博知識；同時，它也需要 Memory 這位個人助理，來提供關於它正在服務的這位用戶的深度理解。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：這個類比非常重要，可以稍微放慢速度，確保聽眾完全理解「研究館員」和「個人助理」之間的區別。這是理解後續內容的基礎。
*   **互動建議**：可以問聽眾：「大家覺得，在一個訂餐機器人中，查詢『菜單品項和價格』這個任務，主要是靠 RAG 還是 Memory？那如果它要記住『我對花生過敏』，這又是誰的工作呢？」引導他們思考。
*   **轉場橋樑 (Bridge)**：
    > 好了，既然我們已經清楚地劃分了 Memory 和 RAG 的界線，現在，就讓我們把焦點完全放在這位「個人助理」身上。他的那本神奇的筆記本裡，到底記錄了些什麼？又是如何組織的呢？下一頁，我們將深入剖析記憶的內部結構。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-14">
            <div class="slide">
                <h2>Slide 14</h2>
                <div class="rendered-content">
                    <h1>Anatomy of Memory: Types &amp; Structures</h1>
<p>A memory is an atomic piece of context with two parts:</p>
<ul>
<li><strong>Content:</strong> The substance of the memory (structured JSON or unstructured text).</li>
<li><strong>Metadata:</strong> Context about the memory (ID, owner, labels).</li>
</ul>
<p>Memories can be classified by the type of knowledge they represent:</p>
<ul>
<li>
<p><strong>Declarative Memory (&quot;Knowing What&quot;):</strong></p>
<ul>
<li>Knowledge of facts, figures, and events.</li>
<li><em>Example: &quot;The user's favorite team is the Warriors.&quot;</em></li>
</ul>
</li>
<li>
<p><strong>Procedural Memory (&quot;Knowing How&quot;):</strong></p>
<ul>
<li>Knowledge of skills and workflows.</li>
<li><em>Example: A memory of the correct sequence of tool calls to book a flight.</em></li>
</ul>
</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 14</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(14, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(14, 'raw')">Source</button>
                </div>
                <div id="note-rendered-14" class="note-content rendered-content">
                    <h3>🎙️ 第 14 頁：Anatomy_of_Memory_Types_and_Structures</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>一個「記憶」單元由兩部分組成：儲存實質內容的 <strong>Content</strong>，以及描述其背景的 <strong>Metadata</strong>。</li>
<li>記憶根據其知識類型，可分為兩大類：關於事實的 <strong>Declarative Memory (陳述性記憶)</strong>，與關於方法的 <strong>Procedural Memory (程序性記憶)</strong>。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，我們已經知道記憶很重要。現在，讓我們戴上顯微鏡，來解剖一個「記憶」單元，看看它內部到底是由什麼構成的。這將幫助我們理解 AI 是如何儲存與理解資訊的。</p>
<h5>① 記憶的基本結構：Content 與 Metadata</h5>
<p>大家可以把每一個記憶，想像成一個獨立的資訊膠囊。這個膠囊基本上由兩個部分組成。</p>
<p>第一個部分是 <strong>Content (內容)</strong>，也就是這個記憶的「實質」。這是從原始對話或數據中提取出來的精華。它的形式可以很靈活：</p>
<ul>
<li>它可以是<strong>結構化的</strong>，例如一段 JSON 或字典格式的資料，像這樣：<code>{&quot;seatPreference&quot;: &quot;Window&quot;}</code>。非常精確，適合機器讀取。</li>
<li>也可以是<strong>非結構化的</strong>，也就是一句自然語言的描述，例如：「這位使用者偏好靠窗的座位。」</li>
</ul>
<p>最關鍵的是，這些內容被設計成<strong>框架無關 (framework-agnostic)</strong> 的，使用像字串或字典這種簡單的格式，確保任何代理人都能讀懂。</p>
<p>第二個部分是 <strong>Metadata (元數據)</strong>。如果說 Content 是文件裡的內容，那 Metadata 就是貼在文件夾外面的標籤。它提供了關於這個記憶的「上下文」，例如：</p>
<ul>
<li>這個記憶的唯一 <strong>ID</strong> 是什麼？</li>
<li>它的<strong>擁有者</strong>是誰？(例如 <code>user_id</code>)</li>
<li>它描述了什麼<strong>主題</strong>？(例如 <code>label: &quot;travel_preference&quot;</code>)</li>
</ul>
<p>有了這兩部分，系統才能有效地管理和檢索記憶。</p>
<h5>② 記憶的兩大類型：陳述性 vs. 程序性</h5>
<p>接下來，是更核心的分類。這個概念源自於認知科學，幫助我們理解 AI 記憶的不同功能。記憶主要分為兩大類。</p>
<blockquote>
<p>第一類，叫做 <strong>Declarative Memory (陳述性記憶)</strong>，也就是「<strong>知道什麼 (Knowing What)</strong>」。</p>
</blockquote>
<p>這類記憶儲存的是<strong>事實、數據和事件</strong>。如果一個記憶回答的是「什麼」的問題，那它就是陳-述-性-記憶。例如投影片上的例子：「使用者最喜歡的球隊是勇士隊。」這是一個可以被明確「陳述」出來的事實。</p>
<blockquote>
<p>第二類，則更進一步，叫做 <strong>Procedural Memory (程序性記憶)</strong>，也就是「<strong>知道如何做 (Knowing How)</strong>」。</p>
</blockquote>
<p>這不是關於事實，而是關於<strong>技能、流程與工作方法</strong>。它指導代理人如何正確地執行一個任務。例如，代理人記住了「要成功預訂一趟旅程，需要依序呼叫『查詢航班』、『預訂座位』、最後『確認付款』這三個工具。」這就是一個程序性記憶。它讓代理人不僅有知識，更有解決問題的「手藝」。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：「陳述性」與「程序性」是本頁最核心的觀念。在講完兩者的定義與比喻後，可以稍微停頓，確保聽眾理解了「知道什麼」和「知道如何做」之間的根本區別。</li>
<li><strong>補充案例</strong>：可以強調，我們日常生活中絕大多數與 AI 的互動，都還停留在陳述性記憶的層次。而程序性記憶，才是讓 AI 從一個「問答機器」進化為一個能自主完成複雜任務的「智慧代理人」的關鍵。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>好了，現在我們知道了單一記憶的內部構造，也理解了「事實」和「方法」這兩種根本不同的記憶類型。但問題來了，當成千上萬個這樣的記憶單元被創造出來後，我們該如何有效地組織、存放它們，才不會變成一團混亂的資訊垃圾呢？這就引出了我們下一頁要探討的主題：記憶的組織模式與儲存架構。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-14" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 14 頁：Anatomy_of_Memory_Types_and_Structures

#### 【本頁重點摘要】
*   一個「記憶」單元由兩部分組成：儲存實質內容的 **Content**，以及描述其背景的 **Metadata**。
*   記憶根據其知識類型，可分為兩大類：關於事實的 **Declarative Memory (陳述性記憶)**，與關於方法的 **Procedural Memory (程序性記憶)**。

---

#### 【逐字講稿】

(開場白)
好的，我們已經知道記憶很重要。現在，讓我們戴上顯微鏡，來解剖一個「記憶」單元，看看它內部到底是由什麼構成的。這將幫助我們理解 AI 是如何儲存與理解資訊的。

##### ① 記憶的基本結構：Content 與 Metadata

大家可以把每一個記憶，想像成一個獨立的資訊膠囊。這個膠囊基本上由兩個部分組成。

第一個部分是 **Content (內容)**，也就是這個記憶的「實質」。這是從原始對話或數據中提取出來的精華。它的形式可以很靈活：
*   它可以是**結構化的**，例如一段 JSON 或字典格式的資料，像這樣：`{"seatPreference": "Window"}`。非常精確，適合機器讀取。
*   也可以是**非結構化的**，也就是一句自然語言的描述，例如：「這位使用者偏好靠窗的座位。」

最關鍵的是，這些內容被設計成**框架無關 (framework-agnostic)** 的，使用像字串或字典這種簡單的格式，確保任何代理人都能讀懂。

第二個部分是 **Metadata (元數據)**。如果說 Content 是文件裡的內容，那 Metadata 就是貼在文件夾外面的標籤。它提供了關於這個記憶的「上下文」，例如：
*   這個記憶的唯一 **ID** 是什麼？
*   它的**擁有者**是誰？(例如 `user_id`)
*   它描述了什麼**主題**？(例如 `label: "travel_preference"`)

有了這兩部分，系統才能有效地管理和檢索記憶。

##### ② 記憶的兩大類型：陳述性 vs. 程序性

接下來，是更核心的分類。這個概念源自於認知科學，幫助我們理解 AI 記憶的不同功能。記憶主要分為兩大類。

> 第一類，叫做 **Declarative Memory (陳述性記憶)**，也就是「**知道什麼 (Knowing What)**」。

這類記憶儲存的是**事實、數據和事件**。如果一個記憶回答的是「什麼」的問題，那它就是陳-述-性-記憶。例如投影片上的例子：「使用者最喜歡的球隊是勇士隊。」這是一個可以被明確「陳述」出來的事實。

> 第二類，則更進一步，叫做 **Procedural Memory (程序性記憶)**，也就是「**知道如何做 (Knowing How)**」。

這不是關於事實，而是關於**技能、流程與工作方法**。它指導代理人如何正確地執行一個任務。例如，代理人記住了「要成功預訂一趟旅程，需要依序呼叫『查詢航班』、『預訂座位』、最後『確認付款』這三個工具。」這就是一個程序性記憶。它讓代理人不僅有知識，更有解決問題的「手藝」。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：「陳述性」與「程序性」是本頁最核心的觀念。在講完兩者的定義與比喻後，可以稍微停頓，確保聽眾理解了「知道什麼」和「知道如何做」之間的根本區別。
*   **補充案例**：可以強調，我們日常生活中絕大多數與 AI 的互動，都還停留在陳述性記憶的層次。而程序性記憶，才是讓 AI 從一個「問答機器」進化為一個能自主完成複雜任務的「智慧代理人」的關鍵。
*   **轉場橋樑 (Bridge)**：
    > 好了，現在我們知道了單一記憶的內部構造，也理解了「事實」和「方法」這兩種根本不同的記憶類型。但問題來了，當成千上萬個這樣的記憶單元被創造出來後，我們該如何有效地組織、存放它們，才不會變成一團混亂的資訊垃圾呢？這就引出了我們下一頁要探討的主題：記憶的組織模式與儲存架構。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-15">
            <div class="slide">
                <h2>Slide 15</h2>
                <div class="rendered-content">
                    <h1>Memory Organization &amp; Storage</h1>
<p><strong>Organization Patterns:</strong></p>
<ul>
<li><strong>Collections:</strong> Organizes content into multiple self-contained, natural language memories for a user.</li>
<li><strong>Structured User Profile:</strong> A set of core facts, like a continuously updated contact card.</li>
<li><strong>&quot;Rolling&quot; Summary:</strong> A single, evolving natural-language summary of the entire user-agent relationship.</li>
</ul>
<p><strong>Storage Architectures:</strong></p>
<ul>
<li><strong>Vector Databases:</strong> Most common; enables retrieval based on semantic similarity.</li>
<li><strong>Knowledge Graphs:</strong> Stores memories as a network of entities and relationships.</li>
<li><strong>Hybrid Approach:</strong> Combines both for relational and semantic search.</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 15</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(15, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(15, 'raw')">Source</button>
                </div>
                <div id="note-rendered-15" class="note-content rendered-content">
                    <h3>🎙️ 第 15 頁：Memory_Organization_and_Storage_Architectures</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>記憶的組織方式決定了它們如何相互關聯，主要有三種模式：<strong>多筆獨立的集合 (Collections)</strong>、<strong>結構化的使用者檔案 (Structured User Profile)</strong>，以及<strong>滾動式摘要 (Rolling Summary)</strong>。</li>
<li>記憶的儲存架構是實現檢索的技術基礎，主流選擇包括：用於語意搜尋的<strong>向量資料庫</strong>、用於關係推理的<strong>知識圖譜</strong>，以及結合兩者優點的<strong>混合方法</strong>。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，我們已經知道要從對話中提取有意義的資訊來建立記憶。但問題來了：這些記憶，我們該如何整理歸檔？又該把它們存放在什麼樣的「倉庫」裡，才能在需要時快速又準確地找到呢？這一頁，我們就要探討記憶的「組織模式」與「儲存架構」。</p>
<blockquote>
<p>你可以把這想像成建立一個圖書館。我們不僅要決定書本（也就是記憶）要怎麼分類上架，還要設計書架和檢索系統本身。</p>
</blockquote>
<h5>① 第一部分：組織模式 (Organization Patterns)</h5>
<p>這部分是關於「記憶之間如何關聯」。我們有三種主流的整理方法。</p>
<ul>
<li>
<p><strong>第一種是「集合 (Collections)」</strong>：這就像是為每個使用者建立一本剪貼簿或日記。每一條記憶都是一個獨立的、自成一體的自然語言紀錄，可能是某個事件、一個摘要或一個觀察。這種模式非常適合儲存大量、非結構化的資訊，讓你可以從一個大的資訊池中進行搜尋。</p>
</li>
<li>
<p><strong>第二種是「結構化使用者檔案 (Structured User Profile)」</strong>：這更像是一張會持續更新的個人名片或聯絡人檔案。它專門用來存放關於使用者的核心事實，例如姓名、偏好設定、帳號資訊等等。它的優點是查詢速度非常快，非常適合用來查找那些穩定不變的關鍵資訊。</p>
</li>
<li>
<p><strong>第三種是「滾動式摘要 (Rolling Summary)」</strong>：這種模式很特別，它不會建立很多筆獨立的記憶，而是將所有資訊，濃縮到一份不斷演進的「單一文件」裡。這份文件就像是一篇持續更新的、關於整個使用者與代理人互動歷史的自然語言摘要。這種方法也經常用來壓縮長的對話歷史，非常高效。</p>
</li>
</ul>
<h5>② 第二部分：儲存架構 (Storage Architectures)</h5>
<p>這部分，我們來談談存放這些記憶的底層技術，也就是「書架」本身。這決定了我們能多快、多聰明地找到想要的記憶。</p>
<ul>
<li>
<p><strong>首先是「向量資料庫 (Vector Databases)」</strong>：這是目前最常見的方法。它的核心思想是<strong>基於語意相似度來檢索，而不是精確的關鍵字</strong>。它會將每一條記憶轉換成一個「嵌入向量」，當你要查詢時，它會找出在概念上最接近的記憶。這對於檢索非結構化的自然語言記憶（例如我們前面提到的「集合」模式）非常有效。</p>
</li>
<li>
<p><strong>其次是「知識圖譜 (Knowledge Graphs)」</strong>：這種架構把記憶儲存成一個由「實體（節點）」和它們之間的「關係（邊）」所組成的網路。當你要檢索時，代理人可以在這個圖譜上遊走，找出直接或間接的關聯。這對於需要理解複雜連結、進行結構化查詢的場景特別理想。</p>
</li>
<li>
<p><strong>最後是「混合方法 (Hybrid Approach)」</strong>：就像它的名字一樣，這是「兩全其美」的方案。它將知識圖譜的結構化優勢與向量資料庫的語意搜尋能力結合起來。例如，在知識圖譜的實體節點上，再附加向量嵌入。這讓系統既能進行關係推理，又能做概念上的模糊搜尋，功能最為強大。</p>
</li>
</ul>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：在講完「組織模式」後，可以稍微停頓，讓聽眾區分這是「邏輯層」的分類。然後再進入「儲存架構」這個「物理層」的技術選擇。</li>
<li><strong>善用比喻</strong>：持續使用「圖書館」、「剪貼簿」、「名片」等比喻，能幫助聽眾更好地理解這些抽象的概念。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>好的，到目前為止，我們討論的組織和儲存方式，大多是圍繞著「文字」記憶。但現代的互動是多模態的，使用者可能會傳一張圖片、一段語音。那麼，當記憶的來源不是文字時，我們該如何處理？下一頁，我們就來專門探討這個越來越重要的議題：如何處理「多模態記憶」。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-15" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 15 頁：Memory_Organization_and_Storage_Architectures

#### 【本頁重點摘要】
*   記憶的組織方式決定了它們如何相互關聯，主要有三種模式：**多筆獨立的集合 (Collections)**、**結構化的使用者檔案 (Structured User Profile)**，以及**滾動式摘要 (Rolling Summary)**。
*   記憶的儲存架構是實現檢索的技術基礎，主流選擇包括：用於語意搜尋的**向量資料庫**、用於關係推理的**知識圖譜**，以及結合兩者優點的**混合方法**。

---

#### 【逐字講稿】

(開場白)
好的，我們已經知道要從對話中提取有意義的資訊來建立記憶。但問題來了：這些記憶，我們該如何整理歸檔？又該把它們存放在什麼樣的「倉庫」裡，才能在需要時快速又準確地找到呢？這一頁，我們就要探討記憶的「組織模式」與「儲存架構」。

> 你可以把這想像成建立一個圖書館。我們不僅要決定書本（也就是記憶）要怎麼分類上架，還要設計書架和檢索系統本身。

##### ① 第一部分：組織模式 (Organization Patterns)
這部分是關於「記憶之間如何關聯」。我們有三種主流的整理方法。

*   **第一種是「集合 (Collections)」**：這就像是為每個使用者建立一本剪貼簿或日記。每一條記憶都是一個獨立的、自成一體的自然語言紀錄，可能是某個事件、一個摘要或一個觀察。這種模式非常適合儲存大量、非結構化的資訊，讓你可以從一個大的資訊池中進行搜尋。

*   **第二種是「結構化使用者檔案 (Structured User Profile)」**：這更像是一張會持續更新的個人名片或聯絡人檔案。它專門用來存放關於使用者的核心事實，例如姓名、偏好設定、帳號資訊等等。它的優點是查詢速度非常快，非常適合用來查找那些穩定不變的關鍵資訊。

*   **第三種是「滾動式摘要 (Rolling Summary)」**：這種模式很特別，它不會建立很多筆獨立的記憶，而是將所有資訊，濃縮到一份不斷演進的「單一文件」裡。這份文件就像是一篇持續更新的、關於整個使用者與代理人互動歷史的自然語言摘要。這種方法也經常用來壓縮長的對話歷史，非常高效。

##### ② 第二部分：儲存架構 (Storage Architectures)
這部分，我們來談談存放這些記憶的底層技術，也就是「書架」本身。這決定了我們能多快、多聰明地找到想要的記憶。

*   **首先是「向量資料庫 (Vector Databases)」**：這是目前最常見的方法。它的核心思想是**基於語意相似度來檢索，而不是精確的關鍵字**。它會將每一條記憶轉換成一個「嵌入向量」，當你要查詢時，它會找出在概念上最接近的記憶。這對於檢索非結構化的自然語言記憶（例如我們前面提到的「集合」模式）非常有效。

*   **其次是「知識圖譜 (Knowledge Graphs)」**：這種架構把記憶儲存成一個由「實體（節點）」和它們之間的「關係（邊）」所組成的網路。當你要檢索時，代理人可以在這個圖譜上遊走，找出直接或間接的關聯。這對於需要理解複雜連結、進行結構化查詢的場景特別理想。

*   **最後是「混合方法 (Hybrid Approach)」**：就像它的名字一樣，這是「兩全其美」的方案。它將知識圖譜的結構化優勢與向量資料庫的語意搜尋能力結合起來。例如，在知識圖譜的實體節點上，再附加向量嵌入。這讓系統既能進行關係推理，又能做概念上的模糊搜尋，功能最為強大。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在講完「組織模式」後，可以稍微停頓，讓聽眾區分這是「邏輯層」的分類。然後再進入「儲存架構」這個「物理層」的技術選擇。
*   **善用比喻**：持續使用「圖書館」、「剪貼簿」、「名片」等比喻，能幫助聽眾更好地理解這些抽象的概念。
*   **轉場橋樑 (Bridge)**：
    > 好的，到目前為止，我們討論的組織和儲存方式，大多是圍繞著「文字」記憶。但現代的互動是多模態的，使用者可能會傳一張圖片、一段語音。那麼，當記憶的來源不是文字時，我們該如何處理？下一頁，我們就來專門探討這個越來越重要的議題：如何處理「多模態記憶」。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-16">
            <div class="slide">
                <h2>Slide 16</h2>
                <div class="rendered-content">
                    <h1>Handling Multimodal Memory</h1>
<p>It's crucial to distinguish between the memory's source and its content.</p>
<ol>
<li>
<p><strong>Memory from a Multimodal Source (Common):</strong></p>
<ul>
<li>The agent processes non-textual data (images, audio) but creates a <strong>textual insight</strong>.</li>
<li><em>Example: Transcribing a voice memo to create a text memory: &quot;User expressed frustration about a shipping delay.&quot;</em></li>
</ul>
</li>
<li>
<p><strong>Memory with Multimodal Content (Advanced):</strong></p>
<ul>
<li>The memory itself contains the non-textual media.</li>
<li><em>Example: A user uploads a logo design and says, &quot;Remember this.&quot; The agent stores the image file directly in the memory.</em></li>
</ul>
</li>
</ol>
<p>Most systems today focus on the first approach.</p>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 16</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(16, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(16, 'raw')">Source</button>
                </div>
                <div id="note-rendered-16" class="note-content rendered-content">
                    <h3>🎙️ 第 16 頁：Handling Multimodal Memory</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>處理非文字記憶的關鍵，在於區分記憶的「來源 (source)」與「內容 (content)」。</li>
<li><strong>常見做法</strong>：代理人能處理圖片、聲音等「多模態來源」，但最終生成的是「文字」記憶。</li>
<li><strong>進階做法</strong>：記憶的「內容」本身就是非文字的多模態媒體，例如直接儲存一張圖片。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
到目前為止，我們討論的記憶，似乎都圍繞著「文字」。但現在的 AI 這麼強大，能看、能聽，那這些非文字的資訊，像是圖片、聲音、影片，要怎麼被「記住」呢？這就是我們這一頁要探討的關鍵概念：「多模態記憶」。</p>
<p>要理解這個概念，最重要的一點，就是要先釐清一個核心區別。</p>
<blockquote>
<p>關鍵是要區分記憶的「來源 (source)」——也就是資訊是從哪裡來的，以及記憶被儲存的「內容 (content)」——也就是最終存下來的是什麼東西。</p>
</blockquote>
<p>根據這個區別，我們可以把多模態記憶分成兩種主要形式。</p>
<h5>① 第一種，也是最常見的：從「多模態來源」生成記憶</h5>
<p>這種情況下，代理人可以處理各種非文字的資料，像是圖片、音檔等等，但它最終創造出來的記憶，是一個<strong>文字形式的洞察</strong>。</p>
<ul>
<li>投影片上的例子很貼切：假設一位用戶傳來一段語音訊息，抱怨最近的貨運延遲。代理人會做什麼呢？它不會儲存那個音檔本身，而是會先將語音轉為文字，然後從中提取關鍵資訊，生成一條文字記憶，像是：<strong>「用戶對於近期的貨運延遲表示不滿。」</strong></li>
</ul>
<p>為什麼這是主流做法？因為將所有輸入都轉換成「文字」這個共通、可搜尋的格式，在技術上遠比處理和檢索各種不同的二進位檔案要簡單得多。</p>
<h5>② 第二種，更進階的：記憶本身就包含「多模態內容」</h5>
<p>這種做法就更直接了。記憶本身就直接包含了非文字的媒體檔案。</p>
<ul>
<li>例如，一位用戶上傳了一張圖片，然後說：「記住這張圖，這是我們新 Logo 的設計草稿。」 在這種情況下，代理人創建的記憶，就會<strong>直接包含那張圖片檔案</strong>，並與用戶的請求連結在一起。</li>
</ul>
<p>這種方式雖然更強大，但也更複雜，因為要對圖片或音檔這類非結構化的二進位資料進行有效的生成、儲存和檢索，需要更專門的模型、演算法和基礎設施。因此，目前大多數的記憶系統，都還是專注在第一種方法。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：講完這兩種區別後，可以稍微停頓，確保聽眾理解了「來源」和「內容」這個核心差異。這是理解多模態記憶的基礎。</li>
<li><strong>補充案例</strong>：可以提到，像白皮書中的程式碼範例就展示了，即使輸入是多模態的（例如包含圖片的 API 請求），輸出的記憶仍然是從中提取的「文字洞察」。這印證了第一種方法的普遍性。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>好的，我們現在知道了記憶可以儲存什麼（事實、流程），也知道了它可以如何處理不同類型的資料（文字、多模態）。那下一個最重要的問題就是：這些記憶，究竟是「如何被創造出來的」？這個過程並不是簡單的複製貼上。下一頁，我們將深入探討記憶生成的完整流程——一個可以被比喻為專為 AI 設計的 ETL 管道。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-16" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 16 頁：Handling Multimodal Memory

#### 【本頁重點摘要】
*   處理非文字記憶的關鍵，在於區分記憶的「來源 (source)」與「內容 (content)」。
*   **常見做法**：代理人能處理圖片、聲音等「多模態來源」，但最終生成的是「文字」記憶。
*   **進階做法**：記憶的「內容」本身就是非文字的多模態媒體，例如直接儲存一張圖片。

---

#### 【逐字講稿】

(開場白)
到目前為止，我們討論的記憶，似乎都圍繞著「文字」。但現在的 AI 這麼強大，能看、能聽，那這些非文字的資訊，像是圖片、聲音、影片，要怎麼被「記住」呢？這就是我們這一頁要探討的關鍵概念：「多模態記憶」。

要理解這個概念，最重要的一點，就是要先釐清一個核心區別。

> 關鍵是要區分記憶的「來源 (source)」——也就是資訊是從哪裡來的，以及記憶被儲存的「內容 (content)」——也就是最終存下來的是什麼東西。

根據這個區別，我們可以把多模態記憶分成兩種主要形式。

##### ① 第一種，也是最常見的：從「多模態來源」生成記憶
這種情況下，代理人可以處理各種非文字的資料，像是圖片、音檔等等，但它最終創造出來的記憶，是一個**文字形式的洞察**。

*   投影片上的例子很貼切：假設一位用戶傳來一段語音訊息，抱怨最近的貨運延遲。代理人會做什麼呢？它不會儲存那個音檔本身，而是會先將語音轉為文字，然後從中提取關鍵資訊，生成一條文字記憶，像是：**「用戶對於近期的貨運延遲表示不滿。」**

為什麼這是主流做法？因為將所有輸入都轉換成「文字」這個共通、可搜尋的格式，在技術上遠比處理和檢索各種不同的二進位檔案要簡單得多。

##### ② 第二種，更進階的：記憶本身就包含「多模態內容」
這種做法就更直接了。記憶本身就直接包含了非文字的媒體檔案。

*   例如，一位用戶上傳了一張圖片，然後說：「記住這張圖，這是我們新 Logo 的設計草稿。」 在這種情況下，代理人創建的記憶，就會**直接包含那張圖片檔案**，並與用戶的請求連結在一起。

這種方式雖然更強大，但也更複雜，因為要對圖片或音檔這類非結構化的二進位資料進行有效的生成、儲存和檢索，需要更專門的模型、演算法和基礎設施。因此，目前大多數的記憶系統，都還是專注在第一種方法。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講完這兩種區別後，可以稍微停頓，確保聽眾理解了「來源」和「內容」這個核心差異。這是理解多模態記憶的基礎。
*   **補充案例**：可以提到，像白皮書中的程式碼範例就展示了，即使輸入是多模態的（例如包含圖片的 API 請求），輸出的記憶仍然是從中提取的「文字洞察」。這印證了第一種方法的普遍性。
*   **轉場橋樑 (Bridge)**：
    > 好的，我們現在知道了記憶可以儲存什麼（事實、流程），也知道了它可以如何處理不同類型的資料（文字、多模態）。那下一個最重要的問題就是：這些記憶，究竟是「如何被創造出來的」？這個過程並不是簡單的複製貼上。下一頁，我們將深入探討記憶生成的完整流程——一個可以被比喻為專為 AI 設計的 ETL 管道。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-17">
            <div class="slide">
                <h2>Slide 17</h2>
                <div class="rendered-content">
                    <h1>Memory Generation: The ETL Pipeline for AI</h1>
<p>Memory generation is an LLM-driven <strong>ETL (Extract, Transform, Load)</strong> pipeline that turns raw data into structured insights.</p>
<p><strong>Analogy: The Gardener</strong></p>
<ul>
<li>A gardener doesn't just throw new seeds (information) onto the plot. They perform <strong>consolidation</strong>—pulling weeds (deleting conflicts), pruning branches (refining memories), and planting new saplings in the right spot.</li>
</ul>
<p><strong>High-Level Stages:</strong></p>
<ol>
<li><strong>Ingestion:</strong> Receive raw data (e.g., conversation history).</li>
<li><strong>Extraction:</strong> Use an LLM to pull out meaningful content based on topic definitions.</li>
<li><strong>Consolidation:</strong> A &quot;self-editing&quot; process to merge, update, or delete memories to resolve conflicts and deduplicate.</li>
<li><strong>Storage:</strong> Persist the curated memory to a durable database.</li>
</ol>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 17</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(17, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(17, 'raw')">Source</button>
                </div>
                <div id="note-rendered-17" class="note-content rendered-content">
                    <h3>🎙️ 第 17 頁：Memory_Generation_The_ETL_Pipeline_for_AI</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>記憶生成並非簡單的儲存，而是一個由大型語言模型 (LLM) 驅動的自動化 <strong>ETL (提取、轉換、載入)</strong> 流程。</li>
<li>這個過程就像一位園丁在整理花園：接收新種子 (資訊)、篩選並種植 (提取)、除草與修剪 (整合)、最終讓花園更茂盛 (儲存)。</li>
<li>其核心階段包含四個步驟：<strong>資料導入 (Ingestion)</strong>、<strong>資訊提取 (Extraction)</strong>、<strong>知識整合 (Consolidation)</strong>，以及 <strong>持久化儲存 (Storage)</strong>。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，我們已經知道記憶有多重要。但一個關鍵問題是：記憶是從哪裡來的？它不是被動地「存」下來的，而是被主動地「生成」出來的。這一頁，我們要介紹整個流程，也就是「記憶生成」。你可以把它想像成一個專為 AI 設計的、由 LLM 驅動的 <strong>ETL 流程——也就是提取 (Extract)、轉換 (Transform)、和載入 (Load)</strong>。</p>
<p>這個流程，將原始、雜亂的對話數據，轉化為結構化、有意義的智慧結晶。</p>
<h5>① 園丁的比喻 (The Gardener Analogy)</h5>
<p>為了讓大家更容易理解，投影片上用了一個非常貼切的比喻：<strong>一位園丁</strong>。</p>
<blockquote>
<p>想像一下，你跟 AI 的每一次對話，都像是給這位園丁一些新的種子或樹苗 (也就是新的資訊)。一個好的園丁，絕對不會把這些種子隨便亂灑。他會先進行「<strong>整合 (Consolidation)</strong>」——他會拔掉花園裡的雜草 (也就是刪除重複或衝突的舊資訊)、修剪過於茂盛的枝葉 (也就是精煉和總結現有的記憶)，然後才小心翼翼地，把新的樹苗種在最適合的位置。</p>
</blockquote>
<p>這個持續不斷、用心整理的過程，確保了花園 (也就是 AI 的知識庫) 能夠保持健康、有條理，而不是變成一片雜草叢生的荒地。而這整個園藝工作，都是在背景中<strong>非同步</strong>進行的，確保不會影響到你與 AI 的即時互動。</p>
<h5>② 記憶生成的四個階段</h5>
<p>這個園藝工作，也就是我們的 ETL 流程，主要可以拆解成四個高級階段：</p>
<ol>
<li>
<p><strong>資料導入 (Ingestion)</strong>：這是起點。系統接收到最原始的數據來源，通常就是一段完整的對話歷史紀錄。</p>
</li>
<li>
<p><strong>資訊提取 (Extraction)</strong>：這是 ETL 中的「E」。在這裡，記憶管理器會利用一個 LLM，從龐雜的對話中，<strong>提取出有意義的內容</strong>。關鍵在於，它不是什麼都提，而是根據預先定義好的「主題」，只抓取符合條件的資訊。就像園丁只挑選優良的種子一樣。</p>
</li>
<li>
<p><strong>知識整合 (Consolidation)</strong>：這是最複雜，也是最有價值的「T」(Transform) 階段。它是一個「自我編輯」的過程。LLM 會比較剛剛提取出的新資訊，與資料庫中已經存在的舊記憶。它會進行比對、解決衝突、並去除重複的內容。它可能會決定<strong>合併</strong>新舊資訊、<strong>刪除</strong>過時的記憶，或是<strong>創建</strong>一條全新的記憶。</p>
</li>
<li>
<p><strong>持久化儲存 (Storage)</strong>：最後，這是「L」(Load) 階段。經過前面層層處理後，這些乾淨、精煉、準確的記憶，會被正式存入一個持久化的資料庫中，例如向量資料庫或知識圖譜，以便在未來的互動中被快速檢索。</p>
</li>
</ol>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：在解釋園丁比喻時，可以放慢速度，讓聽眾充分吸收這個概念。這是理解記憶生成核心精神的關鍵。</li>
<li><strong>核心觀點</strong>：可以強調，正是這個自動化的 ETL 流程，讓「記憶管理器」遠遠超越了一個普通的資料庫。它是一個主動的、智慧的知識整理系統。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>了解了這個從「原始數據」到「精煉記憶」的完整流程後，你心中可能會有兩個更深入的問題：「首先，系統到底是如何判斷什麼資訊才算『有意義』，值得被<strong>提取</strong>出來的？其次，那個聽起來很神奇的『<strong>整合</strong>』步驟，它具體又是如何運作的？」這兩個問題，正好就是我們接下來兩頁要深入探討的核心。我們將先從「記憶提取」開始。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-17" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 17 頁：Memory_Generation_The_ETL_Pipeline_for_AI

#### 【本頁重點摘要】
*   記憶生成並非簡單的儲存，而是一個由大型語言模型 (LLM) 驅動的自動化 **ETL (提取、轉換、載入)** 流程。
*   這個過程就像一位園丁在整理花園：接收新種子 (資訊)、篩選並種植 (提取)、除草與修剪 (整合)、最終讓花園更茂盛 (儲存)。
*   其核心階段包含四個步驟：**資料導入 (Ingestion)**、**資訊提取 (Extraction)**、**知識整合 (Consolidation)**，以及 **持久化儲存 (Storage)**。

---

#### 【逐字講稿】

(開場白)
好的，我們已經知道記憶有多重要。但一個關鍵問題是：記憶是從哪裡來的？它不是被動地「存」下來的，而是被主動地「生成」出來的。這一頁，我們要介紹整個流程，也就是「記憶生成」。你可以把它想像成一個專為 AI 設計的、由 LLM 驅動的 **ETL 流程——也就是提取 (Extract)、轉換 (Transform)、和載入 (Load)**。

這個流程，將原始、雜亂的對話數據，轉化為結構化、有意義的智慧結晶。

##### ① 園丁的比喻 (The Gardener Analogy)
為了讓大家更容易理解，投影片上用了一個非常貼切的比喻：**一位園丁**。

> 想像一下，你跟 AI 的每一次對話，都像是給這位園丁一些新的種子或樹苗 (也就是新的資訊)。一個好的園丁，絕對不會把這些種子隨便亂灑。他會先進行「**整合 (Consolidation)**」——他會拔掉花園裡的雜草 (也就是刪除重複或衝突的舊資訊)、修剪過於茂盛的枝葉 (也就是精煉和總結現有的記憶)，然後才小心翼翼地，把新的樹苗種在最適合的位置。

這個持續不斷、用心整理的過程，確保了花園 (也就是 AI 的知識庫) 能夠保持健康、有條理，而不是變成一片雜草叢生的荒地。而這整個園藝工作，都是在背景中**非同步**進行的，確保不會影響到你與 AI 的即時互動。

##### ② 記憶生成的四個階段
這個園藝工作，也就是我們的 ETL 流程，主要可以拆解成四個高級階段：

1.  **資料導入 (Ingestion)**：這是起點。系統接收到最原始的數據來源，通常就是一段完整的對話歷史紀錄。

2.  **資訊提取 (Extraction)**：這是 ETL 中的「E」。在這裡，記憶管理器會利用一個 LLM，從龐雜的對話中，**提取出有意義的內容**。關鍵在於，它不是什麼都提，而是根據預先定義好的「主題」，只抓取符合條件的資訊。就像園丁只挑選優良的種子一樣。

3.  **知識整合 (Consolidation)**：這是最複雜，也是最有價值的「T」(Transform) 階段。它是一個「自我編輯」的過程。LLM 會比較剛剛提取出的新資訊，與資料庫中已經存在的舊記憶。它會進行比對、解決衝突、並去除重複的內容。它可能會決定**合併**新舊資訊、**刪除**過時的記憶，或是**創建**一條全新的記憶。

4.  **持久化儲存 (Storage)**：最後，這是「L」(Load) 階段。經過前面層層處理後，這些乾淨、精煉、準確的記憶，會被正式存入一個持久化的資料庫中，例如向量資料庫或知識圖譜，以便在未來的互動中被快速檢索。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在解釋園丁比喻時，可以放慢速度，讓聽眾充分吸收這個概念。這是理解記憶生成核心精神的關鍵。
*   **核心觀點**：可以強調，正是這個自動化的 ETL 流程，讓「記憶管理器」遠遠超越了一個普通的資料庫。它是一個主動的、智慧的知識整理系統。
*   **轉場橋樑 (Bridge)**：
    > 了解了這個從「原始數據」到「精煉記憶」的完整流程後，你心中可能會有兩個更深入的問題：「首先，系統到底是如何判斷什麼資訊才算『有意義』，值得被**提取**出來的？其次，那個聽起來很神奇的『**整合**』步驟，它具體又是如何運作的？」這兩個問題，正好就是我們接下來兩頁要深入探討的核心。我們將先從「記憶提取」開始。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-18">
            <div class="slide">
                <h2>Slide 18</h2>
                <div class="rendered-content">
                    <h1>Deep-dive: Memory Extraction</h1>
<p><strong>Goal: What information is meaningful enough to become a memory?</strong></p>
<ul>
<li>&quot;Meaningful&quot; is defined by the agent's specific use case.</li>
<li>The memory manager's LLM uses a system prompt with <strong>topic definitions</strong> to guide extraction.</li>
</ul>
<p><strong>Extraction Methods:</strong></p>
<ul>
<li><strong>Schema-based:</strong> The LLM fills a predefined JSON schema using information from the conversation.</li>
<li><strong>Natural Language Topics:</strong> The LLM is guided by a simple text description of the topic.</li>
<li><strong>Few-shot Prompting:</strong> The LLM is &quot;shown&quot; what to extract through examples of input text and the ideal output memory.</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 18</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(18, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(18, 'raw')">Source</button>
                </div>
                <div id="note-rendered-18" class="note-content rendered-content">
                    <h3>🎙️ 第 18 頁：Deep-dive_Memory_Extraction</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>記憶提取的目標是從對話中篩選出「有意義」的資訊，將其轉化為記憶。</li>
<li>「有意義」的標準完全取決於代理人的具體用途，需要客製化定義。</li>
<li>主要有三種提取方法：基於結構化 Schema、基於自然語言主題定義，以及透過範例學習的 Few-shot Prompting。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好，我們剛剛看完了記憶生成的整體流程，現在，我們要深入這個流程的第一個關鍵步驟：「提取 (Extraction)」。這可以說是最神奇的一步。每天，代理人跟用戶之間有無數的對話，有問候、有閒聊、有廢話。如果我們把所有東西都記下來，那記憶就只是一堆無用的噪音。</p>
<blockquote>
<p>所以，提取的核心問題就是這頁投影片上的標題：在這大量的對話中，到底哪些資訊，才真正「有意義」，值得被我們轉化為一條永久的記憶？</p>
</blockquote>
<p>這不是簡單的摘要，而是一個目標明確、高度智慧的<strong>過濾過程</strong>，目的是從噪音中分離出有價值的訊號。</p>
<h5>① 首先，我們必須定義什麼是「有意義」</h5>
<p>投影片上提到，<strong>「有意義」完全由代理人的使用場景決定</strong>。這句話非常關鍵。一個通用的標準是不存在的。</p>
<ul>
<li>想像一個<strong>電商的客服代理人</strong>，它需要記住的「有意義」資訊可能是：用戶的訂單編號、遇到的技術問題、退貨的理由。</li>
<li>但如果是一個<strong>個人健康教練代理人</strong>，它需要記住的就完全不同了，可能是：用戶的長期健身目標、最近的情緒狀態、飲食偏好。</li>
</ul>
<p>你看，同樣是「記憶」，但對「有意義」的定義天差地遠。因此，客製化代理人需要記住的內容，是打造一個真正有效代理人的第一步。</p>
<h5>② 那麼，系統是如何知道要提取什麼的？答案是：主題定義 (Topic Definitions)</h5>
<p>記憶管理器的 LLM 之所以能聰明地提取資訊，是因為我們在它的系統提示 (System Prompt) 中，給了它一套非常清晰的「遊戲規則」，也就是所謂的<strong>主題定義</strong>。這些定義就像是給 LLM 的一副特殊眼鏡，讓它在閱讀對話時，只看得到我們感興趣的特定主題。</p>
<h5>③ 實現主題定義的三種主要方法</h5>
<p>投影片列出了三種主流的技術，讓我們一一拆解：</p>
<ul>
<li>
<p><strong>第一種：基於 Schema (Schema-based)</strong>
這就像是給 LLM 一張預先設計好的 JSON 格式的「表格」。我們命令它：「從對話中找出能填進這張表格的資訊。」例如，表格裡有 <code>{&quot;seat_preference&quot;: &quot;...&quot;}</code> 這個欄位，LLM 就會去對話中尋找用戶關於座位偏好的描述，然後把「靠窗」或「走道」填進去。這非常適合提取結構化的資料。</p>
</li>
<li>
<p><strong>第二種：自然語言主題 (Natural Language Topics)</strong>
這是最直接的方法。我們直接用白話文告訴 LLM 要找什麼。例如，給它一個主題描述：「尋找用戶對於咖啡店體驗的具體回饋，包含對飲料、食物、氛圍、服務速度或清潔度的看法。」LLM 就會根據這段描述去抓取相關內容。</p>
</li>
<li>
<p><strong>第三種，也是非常強大的一種：Few-shot Prompting</strong>
這個策略的核心是「身教重於言教」。我們不只是告訴 LLM 要做什麼，而是直接<strong>做給它看</strong>。我們在提示中提供幾個「輸入對話」與「理想輸出記憶」的範例。</p>
<blockquote>
<p>例如，我們給它看一段對話：『用戶說：今天的手沖咖啡有點溫掉了，而且音樂太大聲，我幾乎聽不到朋友說話。』然後我們告訴它，這段話應該被提取成兩條記憶：『事實：用戶回報手沖咖啡是溫的』和『事實：用戶覺得店裡音樂太大聲』。</p>
</blockquote>
</li>
</ul>
<p>透過這些範例，LLM 能自己學會提取的模式與標準，這對於那些很難用簡單規則描述的、更細微的主題特別有效。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：講完這三種方法後，可以稍微停頓。這三點是本頁的技術核心，確保聽眾能理解從「給表格」、「給描述」到「給範例」的層次遞進。</li>
<li><strong>補充說明</strong>：可以補充一點，大部分的記憶平台都會內建一些通用主題（例如記住用戶偏好、關鍵事實），但真正的威力來自於開發者能夠像這樣，為自己的業務場景定義專屬的、客製化的主題。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>好了，現在我們透過這些方法，成功從對話中提取出了一批新的、有價值的資訊。但工作還沒結束。我們不能 просто 把這些新資訊丟進記憶庫裡。如果用戶說了跟以前矛盾的話怎麼辦？如果他重複提了同一件事怎麼辦？這就引出了我們記憶生成的下一個、也是更複雜的階段：「合併與整理」，也就是 <strong>Consolidation</strong>。下一頁，我們就來看看系統如何扮演一個聰明的編輯，保持記憶庫的整潔與準確。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-18" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 18 頁：Deep-dive_Memory_Extraction

#### 【本頁重點摘要】
*   記憶提取的目標是從對話中篩選出「有意義」的資訊，將其轉化為記憶。
*   「有意義」的標準完全取決於代理人的具體用途，需要客製化定義。
*   主要有三種提取方法：基於結構化 Schema、基於自然語言主題定義，以及透過範例學習的 Few-shot Prompting。

---

#### 【逐字講稿】

(開場白)
好，我們剛剛看完了記憶生成的整體流程，現在，我們要深入這個流程的第一個關鍵步驟：「提取 (Extraction)」。這可以說是最神奇的一步。每天，代理人跟用戶之間有無數的對話，有問候、有閒聊、有廢話。如果我們把所有東西都記下來，那記憶就只是一堆無用的噪音。

> 所以，提取的核心問題就是這頁投影片上的標題：在這大量的對話中，到底哪些資訊，才真正「有意義」，值得被我們轉化為一條永久的記憶？

這不是簡單的摘要，而是一個目標明確、高度智慧的**過濾過程**，目的是從噪音中分離出有價值的訊號。

##### ① 首先，我們必須定義什麼是「有意義」
投影片上提到，**「有意義」完全由代理人的使用場景決定**。這句話非常關鍵。一個通用的標準是不存在的。

*   想像一個**電商的客服代理人**，它需要記住的「有意義」資訊可能是：用戶的訂單編號、遇到的技術問題、退貨的理由。
*   但如果是一個**個人健康教練代理人**，它需要記住的就完全不同了，可能是：用戶的長期健身目標、最近的情緒狀態、飲食偏好。

你看，同樣是「記憶」，但對「有意義」的定義天差地遠。因此，客製化代理人需要記住的內容，是打造一個真正有效代理人的第一步。

##### ② 那麼，系統是如何知道要提取什麼的？答案是：主題定義 (Topic Definitions)
記憶管理器的 LLM 之所以能聰明地提取資訊，是因為我們在它的系統提示 (System Prompt) 中，給了它一套非常清晰的「遊戲規則」，也就是所謂的**主題定義**。這些定義就像是給 LLM 的一副特殊眼鏡，讓它在閱讀對話時，只看得到我們感興趣的特定主題。

##### ③ 實現主題定義的三種主要方法
投影片列出了三種主流的技術，讓我們一一拆解：

*   **第一種：基於 Schema (Schema-based)**
    這就像是給 LLM 一張預先設計好的 JSON 格式的「表格」。我們命令它：「從對話中找出能填進這張表格的資訊。」例如，表格裡有 `{"seat_preference": "..."}` 這個欄位，LLM 就會去對話中尋找用戶關於座位偏好的描述，然後把「靠窗」或「走道」填進去。這非常適合提取結構化的資料。

*   **第二種：自然語言主題 (Natural Language Topics)**
    這是最直接的方法。我們直接用白話文告訴 LLM 要找什麼。例如，給它一個主題描述：「尋找用戶對於咖啡店體驗的具體回饋，包含對飲料、食物、氛圍、服務速度或清潔度的看法。」LLM 就會根據這段描述去抓取相關內容。

*   **第三種，也是非常強大的一種：Few-shot Prompting**
    這個策略的核心是「身教重於言教」。我們不只是告訴 LLM 要做什麼，而是直接**做給它看**。我們在提示中提供幾個「輸入對話」與「理想輸出記憶」的範例。
    > 例如，我們給它看一段對話：『用戶說：今天的手沖咖啡有點溫掉了，而且音樂太大聲，我幾乎聽不到朋友說話。』然後我們告訴它，這段話應該被提取成兩條記憶：『事實：用戶回報手沖咖啡是溫的』和『事實：用戶覺得店裡音樂太大聲』。

透過這些範例，LLM 能自己學會提取的模式與標準，這對於那些很難用簡單規則描述的、更細微的主題特別有效。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講完這三種方法後，可以稍微停頓。這三點是本頁的技術核心，確保聽眾能理解從「給表格」、「給描述」到「給範例」的層次遞進。
*   **補充說明**：可以補充一點，大部分的記憶平台都會內建一些通用主題（例如記住用戶偏好、關鍵事實），但真正的威力來自於開發者能夠像這樣，為自己的業務場景定義專屬的、客製化的主題。
*   **轉場橋樑 (Bridge)**：
    > 好了，現在我們透過這些方法，成功從對話中提取出了一批新的、有價值的資訊。但工作還沒結束。我們不能 просто 把這些新資訊丟進記憶庫裡。如果用戶說了跟以前矛盾的話怎麼辦？如果他重複提了同一件事怎麼辦？這就引出了我們記憶生成的下一個、也是更複雜的階段：「合併與整理」，也就是 **Consolidation**。下一頁，我們就來看看系統如何扮演一個聰明的編輯，保持記憶庫的整潔與準確。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-19">
            <div class="slide">
                <h2>Slide 19</h2>
                <div class="rendered-content">
                    <h1>Deep-dive: Memory Consolidation</h1>
<p><strong>Goal: Integrate new information into a coherent, accurate, and evolving knowledge base.</strong></p>
<ul>
<li>This &quot;self-curation&quot; is what elevates a memory manager beyond a simple database.</li>
<li>It solves problems like information duplication, conflicts, and evolution.</li>
</ul>
<p><strong>The LLM-driven Workflow:</strong></p>
<ol>
<li>Retrieve existing memories similar to the new information.</li>
<li>An LLM analyzes both the existing and new memories.</li>
<li>The LLM decides which operation to perform:
<ul>
<li><strong>UPDATE:</strong> Modify an existing memory.</li>
<li><strong>CREATE:</strong> Create a new memory for a novel topic.</li>
<li><strong>DELETE:</strong> Invalidate an old, incorrect memory.</li>
</ul>
</li>
</ol>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 19</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(19, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(19, 'raw')">Source</button>
                </div>
                <div id="note-rendered-19" class="note-content rendered-content">
                    <h3>🎙️ 第 19 頁：Deep-dive_Memory_Consolidation</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>記憶整合 (Consolidation) 是一個「自我策展」的過程，確保記憶庫的連貫、準確與進化。</li>
<li>它解決了三大核心問題：資訊重複、資訊衝突，以及資訊的自然演變。</li>
<li>這個過程由大型語言模型 (LLM) 驅動，透過分析比對，決定執行更新 (UPDATE)、創建 (CREATE) 或刪除 (DELETE) 操作。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好，我們剛剛在上一頁談完了「提取」，也就是從對話中找出有意義的資訊。但如果我們只是把這些新資訊一股腦地塞進資料庫，那記憶庫很快就會變得雜亂無章、充滿矛盾，甚至無法使用。這就是為什麼我們需要整個流程中最精密、也最關鍵的一步，也就是我們這一頁要深入探討的——<strong>記憶整合 (Memory Consolidation)</strong>。</p>
<blockquote>
<p>這是一個「自我策展 (self-curation)」的過程。正是這個能力，才真正讓一個記憶管理器，超越了一個單純的資料庫。</p>
</blockquote>
<p>我們可以再次回到那個「花園」的比喻。如果說「提取」是園丁拿到了新的種子，那麼「整合」就是園丁實際在花園裡的工作：他要拔掉雜草、修剪老舊的樹枝，並把新的種子種在最合適的地方。</p>
<h5>① 首先，它解決了「資訊重複」的問題</h5>
<p>在不同的對話中，使用者可能會用不同的方式提到同一件事。</p>
<ul>
<li>例如，使用者第一次說：「我需要一張去 <strong>NYC</strong> 的機票。」</li>
<li>幾天後，他又說：「我正在計畫去 <strong>紐約</strong> 的旅行。」</li>
</ul>
<p>一個簡單的提取系統，可能會產生兩筆重複的記憶。但整合系統會意識到這兩者指向的是同一個意圖，並將它們<strong>合併或更新</strong>成一筆更完整的紀錄，而不是製造噪音。</p>
<h5>② 其次，它處理了「資訊衝突」與「資訊演變」</h5>
<p>人的狀態是會改變的。如果沒有整合，代理人的記憶就會充滿矛盾。</p>
<ul>
<li><strong>衝突</strong>：比如，代理人去年的記憶是「使用者對行銷感興趣」，但使用者在最近的對話中提到「我現在正在領導一個專注於第四季客戶獲取的行銷專案」。整合流程會用更具體、更新的資訊去<strong>更新 (UPDATE)</strong> 舊的、模糊的記憶。</li>
<li><strong>演變</strong>：更直接的衝突是，如果一個舊的記憶是「使用者未婚」，但新的資訊顯示「使用者正在為他的伴侶挑選結婚紀念日禮物」，那麼整合流程就必須<strong>刪除或標記 (DELETE / INVALIDATE)</strong> 那個過時的舊記憶。</li>
</ul>
<h5>③ 最後，它實現了「遺忘」</h5>
<p>不是所有的記憶都永遠有用。一個好的記憶系統必須懂得「遺忘」。整合的過程也包含了**修剪 (pruning)**那些陳舊的、無關緊要的、或信賴度低的記憶，確保整個知識庫保持高效和與時俱進。</p>
<p>那麼，這整個由 LLM 驅動的工作流程是怎麼運作的呢？其實很直觀：</p>
<ol>
<li>首先，當有新的資訊被提取出來後，系統會先去<strong>檢索</strong>現有的記憶庫，找出與新資訊主題相似的舊記憶。</li>
<li>接著，LLM 會像一位編輯或法官，同時<strong>分析</strong>這些舊記憶和新資訊。</li>
<li>最後，LLM 會做出裁決，決定要執行哪個操作：</li>
</ol>
<blockquote>
<ul>
<li><strong>UPDATE</strong>：用新資訊來更新或豐富一筆現有的記憶。</li>
<li><strong>CREATE</strong>：如果這個新資訊是全新的主題，那就創建一筆新記憶。</li>
<li><strong>DELETE</strong>：如果新資訊證明某個舊記憶已經完全過時或錯誤，那就刪除它。</li>
</ul>
</blockquote>
<p>這個自動化的策展流程，確保了代理人的知識庫是活的、會呼吸的，而不是一個堆滿垃圾的倉庫。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：在講述 UPDATE, CREATE, DELETE 時，可以加重語氣，並用手勢輔助，強調這是三個核心的動態操作。</li>
<li><strong>核心比喻</strong>：再次強調「自我策展」或「智慧編輯」的概念。這不是被動的儲存，而是主動的管理。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>講到這裡，一個關鍵問題浮現了：當 LLM 在做這些判斷時，它要如何知道該相信誰？如果一份從公司 CRM 系統來的資料說客戶是 VIP，但一次隨口的聊天記錄卻顯示他對價格很敏感，LLM 該如何解決這個衝突？它需要一個判斷資訊「可信度」的標準。這就完美地銜接到了我們的下一頁：<strong>記憶的出處與可信度 (Memory Provenance &amp; Trust)</strong>。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-19" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 19 頁：Deep-dive_Memory_Consolidation

#### 【本頁重點摘要】
*   記憶整合 (Consolidation) 是一個「自我策展」的過程，確保記憶庫的連貫、準確與進化。
*   它解決了三大核心問題：資訊重複、資訊衝突，以及資訊的自然演變。
*   這個過程由大型語言模型 (LLM) 驅動，透過分析比對，決定執行更新 (UPDATE)、創建 (CREATE) 或刪除 (DELETE) 操作。

---

#### 【逐字講稿】

(開場白)
好，我們剛剛在上一頁談完了「提取」，也就是從對話中找出有意義的資訊。但如果我們只是把這些新資訊一股腦地塞進資料庫，那記憶庫很快就會變得雜亂無章、充滿矛盾，甚至無法使用。這就是為什麼我們需要整個流程中最精密、也最關鍵的一步，也就是我們這一頁要深入探討的——**記憶整合 (Memory Consolidation)**。

> 這是一個「自我策展 (self-curation)」的過程。正是這個能力，才真正讓一個記憶管理器，超越了一個單純的資料庫。

我們可以再次回到那個「花園」的比喻。如果說「提取」是園丁拿到了新的種子，那麼「整合」就是園丁實際在花園裡的工作：他要拔掉雜草、修剪老舊的樹枝，並把新的種子種在最合適的地方。

##### ① 首先，它解決了「資訊重複」的問題
在不同的對話中，使用者可能會用不同的方式提到同一件事。

*   例如，使用者第一次說：「我需要一張去 **NYC** 的機票。」
*   幾天後，他又說：「我正在計畫去 **紐約** 的旅行。」

一個簡單的提取系統，可能會產生兩筆重複的記憶。但整合系統會意識到這兩者指向的是同一個意圖，並將它們**合併或更新**成一筆更完整的紀錄，而不是製造噪音。

##### ② 其次，它處理了「資訊衝突」與「資訊演變」
人的狀態是會改變的。如果沒有整合，代理人的記憶就會充滿矛盾。

*   **衝突**：比如，代理人去年的記憶是「使用者對行銷感興趣」，但使用者在最近的對話中提到「我現在正在領導一個專注於第四季客戶獲取的行銷專案」。整合流程會用更具體、更新的資訊去**更新 (UPDATE)** 舊的、模糊的記憶。
*   **演變**：更直接的衝突是，如果一個舊的記憶是「使用者未婚」，但新的資訊顯示「使用者正在為他的伴侶挑選結婚紀念日禮物」，那麼整合流程就必須**刪除或標記 (DELETE / INVALIDATE)** 那個過時的舊記憶。

##### ③ 最後，它實現了「遺忘」
不是所有的記憶都永遠有用。一個好的記憶系統必須懂得「遺忘」。整合的過程也包含了**修剪 (pruning)**那些陳舊的、無關緊要的、或信賴度低的記憶，確保整個知識庫保持高效和與時俱進。

那麼，這整個由 LLM 驅動的工作流程是怎麼運作的呢？其實很直觀：

1.  首先，當有新的資訊被提取出來後，系統會先去**檢索**現有的記憶庫，找出與新資訊主題相似的舊記憶。
2.  接著，LLM 會像一位編輯或法官，同時**分析**這些舊記憶和新資訊。
3.  最後，LLM 會做出裁決，決定要執行哪個操作：

> *   **UPDATE**：用新資訊來更新或豐富一筆現有的記憶。
> *   **CREATE**：如果這個新資訊是全新的主題，那就創建一筆新記憶。
> *   **DELETE**：如果新資訊證明某個舊記憶已經完全過時或錯誤，那就刪除它。

這個自動化的策展流程，確保了代理人的知識庫是活的、會呼吸的，而不是一個堆滿垃圾的倉庫。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在講述 UPDATE, CREATE, DELETE 時，可以加重語氣，並用手勢輔助，強調這是三個核心的動態操作。
*   **核心比喻**：再次強調「自我策展」或「智慧編輯」的概念。這不是被動的儲存，而是主動的管理。
*   **轉場橋樑 (Bridge)**：
    > 講到這裡，一個關鍵問題浮現了：當 LLM 在做這些判斷時，它要如何知道該相信誰？如果一份從公司 CRM 系統來的資料說客戶是 VIP，但一次隨口的聊天記錄卻顯示他對價格很敏感，LLM 該如何解決這個衝突？它需要一個判斷資訊「可信度」的標準。這就完美地銜接到了我們的下一頁：**記憶的出處與可信度 (Memory Provenance & Trust)**。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-20">
            <div class="slide">
                <h2>Slide 20</h2>
                <div class="rendered-content">
                    <h1>Memory Provenance &amp; Trust</h1>
<p><strong>&quot;Garbage in, garbage out&quot; is critical for LLMs.</strong> An agent must be able to evaluate the quality of its own memories.</p>
<ul>
<li>
<p><strong>Provenance:</strong> A detailed record of a memory's origin and history.</p>
</li>
<li>
<p><strong>Why it matters:</strong></p>
<ol>
<li><strong>Memory Consolidation:</strong> Informs conflict resolution by establishing a hierarchy of trust.</li>
<li><strong>Inference:</strong> Informs how much the agent should rely on a memory.</li>
</ol>
</li>
<li>
<p><strong>Source Types (Hierarchy of Trust):</strong></p>
<ol>
<li><strong>Bootstrapped Data (High Trust):</strong> Pre-loaded from systems like a CRM.</li>
<li><strong>Explicit User Input (High Trust):</strong> Direct commands like &quot;Remember this.&quot;</li>
<li><strong>Implicit User Input (Lower Trust):</strong> Inferred from conversation.</li>
<li><strong>Tool Output (Brittle):</strong> Best for short-term caching.</li>
</ol>
</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 20</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(20, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(20, 'raw')">Source</button>
                </div>
                <div id="note-rendered-20" class="note-content rendered-content">
                    <h3>🎙️ 第 20 頁：Memory Provenance &amp; Trustworthiness</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>記憶系統的核心原則是「垃圾進，垃圾出」，因此必須確保記憶的品質與可信度。</li>
<li><strong>記憶出處 (Provenance)</strong> 是評估記憶品質的關鍵，它記錄了記憶的來源與歷史。</li>
<li>出處之所以重要，是因為它能幫助代理人在「合併記憶」時解決衝突，並在「推理決策」時判斷該在多大程度上依賴某個記憶。</li>
<li>記憶的來源存在一個明確的「信任層級」，從最可信的內部系統數據，到最不可靠的工具輸出。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
各位，在機器學習領域有句老話：「垃圾進，垃圾出 (Garbage in, garbage out)」。這句話在大型語言模型的世界裡，變得更加關鍵，甚至可以說是「垃圾進，<strong>自信的</strong>垃圾出」。因為模型總能給出聽起來頭頭是道的答案，即使它的資訊來源是錯的。這就引出了一個核心問題：代理人要如何信任它自己的記憶？</p>
<p>這就是我們這一頁要探討的主題：<strong>記憶的出處與信任度 (Memory Provenance &amp; Trust)</strong>。</p>
<h5>① 什麼是「出處 (Provenance)」？</h5>
<p>簡單來說，「出處」就是一份記憶的<strong>身世履歷</strong>。它詳細記錄了這個記憶是從哪裡來的、什麼時候產生的、以及它是如何形成的。就像我們在評估一則新聞時，會先看它的來源是官方發布還是街頭巷訪一樣，代理人也需要用同樣的標準來審視它的記憶。</p>
<p>那為什麼這件事這麼重要呢？主要有兩個原因：</p>
<blockquote>
<p><strong>第一，它能指導「記憶合併 (Memory Consolidation)」。</strong> 當代理人發現一條新資訊與舊記憶有衝突時，它該相信哪一個？如果舊記憶來自於公司內部的 CRM 系統，而新資訊只是從一次閒聊中推斷出來的，那麼代理人就應該更相信前者。出處提供了一個解決衝突的「信任層級」。</p>
</blockquote>
<blockquote>
<p><strong>第二，它能影響「推理過程 (Inference)」。</strong> 當代理人在執行任務時，它應該在多大程度上依賴某個記憶？出處就像一個<strong>信賴分數</strong>。對於來自高度可信來源的記憶，代理人可以更果斷地使用它來做決策。</p>
</blockquote>
<h5>② 記憶來源的信任層級</h5>
<p>投影片上列出了四種主要的記憶來源，它們形成了一個從高到低的信任階梯：</p>
<ol>
<li>
<p><strong>啟動數據 (Bootstrapped Data)</strong>：這是<strong>最高信任等級</strong>的資訊。通常是從公司內部系統，比如 CRM，預先載入的權威資料。例如，用戶的帳戶類型或聯絡方式。這是最可靠的基礎事實。</p>
</li>
<li>
<p><strong>明確的用戶輸入 (Explicit User Input)</strong>：這也<strong>非常可信</strong>。就是用戶直接下指令要代理人記住的事。例如，用戶說：「記住，我的週年紀念日是 10 月 26 號。」這是用戶親口確認的事實。</p>
</li>
<li>
<p><strong>隱含的用戶輸入 (Implicit User Input)</strong>：從這裡開始，<strong>信任度就下降了</strong>。這是代理人從對話中「推斷」出來的資訊，而不是用戶直接告知的。例如，用戶說「我下週要去巴黎出差」，代理人可能會記下「用戶下週將前往巴黎」。這很可能是對的，但並非 100% 確定。</p>
</li>
<li>
<p><strong>工具輸出 (Tool Output)</strong>：這是<strong>最脆弱、最不穩定</strong>的來源，被稱為 &quot;Brittle&quot;。工具回傳的結果通常是暫時性的，很快就會過時。比如查詢當下的天氣或股價。這種資訊最好只用作短期快取，而不應成為代理人的長期記憶。</p>
</li>
</ol>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：開場的「垃圾進，自信的垃圾出」是一個很好的記憶點，可以稍微加重語氣，引起聽眾的警覺。</li>
<li><strong>生動比喻</strong>：可以將「Provenance」比喻為記憶的「身分證」或「背景調查報告」，幫助聽眾理解這個概念。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>好了，現在我們了解了如何判斷一條記憶是否值得信賴。但這引出了另一個更根本的架構問題：代理人到底「什麼時候」應該去產生記憶呢？是在每一輪對話後？還是在整個會話結束時？這個觸發的時機，將直接影響到系統的成本、效率和記憶的精確度。下一頁，我們就來探討觸發記憶生成的不同策略。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-20" class="note-content note-raw" style="display: none;">
                    <pre>```markdown
### 🎙️ 第 20 頁：Memory Provenance & Trustworthiness

#### 【本頁重點摘要】
*   記憶系統的核心原則是「垃圾進，垃圾出」，因此必須確保記憶的品質與可信度。
*   **記憶出處 (Provenance)** 是評估記憶品質的關鍵，它記錄了記憶的來源與歷史。
*   出處之所以重要，是因為它能幫助代理人在「合併記憶」時解決衝突，並在「推理決策」時判斷該在多大程度上依賴某個記憶。
*   記憶的來源存在一個明確的「信任層級」，從最可信的內部系統數據，到最不可靠的工具輸出。

---

#### 【逐字講稿】

(開場白)
各位，在機器學習領域有句老話：「垃圾進，垃圾出 (Garbage in, garbage out)」。這句話在大型語言模型的世界裡，變得更加關鍵，甚至可以說是「垃圾進，**自信的**垃圾出」。因為模型總能給出聽起來頭頭是道的答案，即使它的資訊來源是錯的。這就引出了一個核心問題：代理人要如何信任它自己的記憶？

這就是我們這一頁要探討的主題：**記憶的出處與信任度 (Memory Provenance & Trust)**。

##### ① 什麼是「出處 (Provenance)」？
簡單來說，「出處」就是一份記憶的**身世履歷**。它詳細記錄了這個記憶是從哪裡來的、什麼時候產生的、以及它是如何形成的。就像我們在評估一則新聞時，會先看它的來源是官方發布還是街頭巷訪一樣，代理人也需要用同樣的標準來審視它的記憶。

那為什麼這件事這麼重要呢？主要有兩個原因：

> **第一，它能指導「記憶合併 (Memory Consolidation)」。** 當代理人發現一條新資訊與舊記憶有衝突時，它該相信哪一個？如果舊記憶來自於公司內部的 CRM 系統，而新資訊只是從一次閒聊中推斷出來的，那麼代理人就應該更相信前者。出處提供了一個解決衝突的「信任層級」。

> **第二，它能影響「推理過程 (Inference)」。** 當代理人在執行任務時，它應該在多大程度上依賴某個記憶？出處就像一個**信賴分數**。對於來自高度可信來源的記憶，代理人可以更果斷地使用它來做決策。

##### ② 記憶來源的信任層級
投影片上列出了四種主要的記憶來源，它們形成了一個從高到低的信任階梯：

1.  **啟動數據 (Bootstrapped Data)**：這是**最高信任等級**的資訊。通常是從公司內部系統，比如 CRM，預先載入的權威資料。例如，用戶的帳戶類型或聯絡方式。這是最可靠的基礎事實。

2.  **明確的用戶輸入 (Explicit User Input)**：這也**非常可信**。就是用戶直接下指令要代理人記住的事。例如，用戶說：「記住，我的週年紀念日是 10 月 26 號。」這是用戶親口確認的事實。

3.  **隱含的用戶輸入 (Implicit User Input)**：從這裡開始，**信任度就下降了**。這是代理人從對話中「推斷」出來的資訊，而不是用戶直接告知的。例如，用戶說「我下週要去巴黎出差」，代理人可能會記下「用戶下週將前往巴黎」。這很可能是對的，但並非 100% 確定。

4.  **工具輸出 (Tool Output)**：這是**最脆弱、最不穩定**的來源，被稱為 "Brittle"。工具回傳的結果通常是暫時性的，很快就會過時。比如查詢當下的天氣或股價。這種資訊最好只用作短期快取，而不應成為代理人的長期記憶。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：開場的「垃圾進，自信的垃圾出」是一個很好的記憶點，可以稍微加重語氣，引起聽眾的警覺。
*   **生動比喻**：可以將「Provenance」比喻為記憶的「身分證」或「背景調查報告」，幫助聽眾理解這個概念。
*   **轉場橋樑 (Bridge)**：
    > 好了，現在我們了解了如何判斷一條記憶是否值得信賴。但這引出了另一個更根本的架構問題：代理人到底「什麼時候」應該去產生記憶呢？是在每一輪對話後？還是在整個會話結束時？這個觸發的時機，將直接影響到系統的成本、效率和記憶的精確度。下一頁，我們就來探討觸發記憶生成的不同策略。

```</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-21">
            <div class="slide">
                <h2>Slide 21</h2>
                <div class="rendered-content">
                    <h1>When to Trigger Memory Generation?</h1>
<p>This is a critical architectural choice, balancing data freshness against cost and latency.</p>
<p><strong>Triggering Strategies:</strong></p>
<ul>
<li><strong>Session Completion:</strong> At the end of a multi-turn session (cost-effective, lower fidelity).</li>
<li><strong>Turn Cadence:</strong> After a specific number of turns (e.g., every 5 turns).</li>
<li><strong>Real-Time:</strong> After every single turn (highest cost, highest fidelity).</li>
<li><strong>Explicit Command:</strong> Upon a direct user command (e.g., &quot;Remember this&quot;).</li>
<li><strong>Memory-as-a-Tool (Advanced):</strong> The agent itself decides when to call a <code>create_memory</code> tool.</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 21</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(21, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(21, 'raw')">Source</button>
                </div>
                <div id="note-rendered-21" class="note-content rendered-content">
                    <h3>🎙️ 第 21 頁：Triggering_Memory_Generation</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>何時觸發記憶生成，是一個在「資料新鮮度」、「成本」與「延遲」之間取得平衡的關鍵架構決策。</li>
<li>常見的觸發策略包含：在對話結束時、按固定頻率（如每 N 次對話）、即時（每次對話後）、或由使用者下達明確指令。</li>
<li>最進階的模式是「記憶即工具 (Memory-as-a-Tool)」，也就是讓代理人 (Agent) 自主判斷何時該呼叫工具來儲存記憶。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好，我們已經了解了記憶的生成是一個強大的自動化流程。但一個非常實際的問題是：這個流程到底應該「何時」啟動？這不是一個無關緊要的細節，而是一個核心的架構選擇，它直接決定了你的代理人記憶有多即時、營運成本有多高，以及使用者感受到的延遲有多久。</p>
<p>這頁投影片，我們就來探討幾種主流的觸發策略，以及它們各自的權衡。</p>
<h5>① 對話完成時 (Session Completion)</h5>
<p>這是最簡單直接的方法。我們等到一整段對話完全結束後，再把整個對話紀錄丟給記憶管理器去處理。</p>
<blockquote>
<p>這就像我們下班後，才一次性地整理今天一整天辦公桌上的所有文件。它的好處是<strong>成本效益最高</strong>，因為你只執行了一次昂貴的生成流程。但缺點是<strong>保真度 (fidelity) 較低</strong>，因為 AI 需要一次消化大量的對話，可能會遺漏掉一些細微的重點。</p>
</blockquote>
<h5>② 固定頻率觸發 (Turn Cadence)</h5>
<p>一個折衷的方案是，設定一個固定的頻率，例如每 5 次或 10 次對話，就觸發一次記憶生成。這就像是每隔一兩個小時，就稍微整理一下桌面。它在成本和資料新鮮度之間取得了不錯的平衡，對於很多應用來說，這是一個「足夠好」的選擇。</p>
<h5>③ 即時觸發 (Real-Time)</h5>
<p>接著是即時觸發，也就是在每一次對話結束後，立刻生成記憶。這種方式能提供<strong>最高的保真度和最新鮮的記憶</strong>，任何細節都會在發生的當下被捕捉。但可想而知，這也是<strong>成本最高、對系統負擔最大</strong>的選項。如果處理不當，很容易因為等待記憶寫入而增加使用者感受到的延遲。</p>
<h5>④ 使用者明確指令 (Explicit Command)</h5>
<p>我們也可以把控制權交給使用者。當使用者明確地說出「記住這件事」或類似的指令時，我們才觸發記憶生成。這個方法非常直接，而且能確保被存下來的，絕對是使用者認為重要的資訊。</p>
<h5>⑤ 記憶即工具 (Memory-as-a-Tool)</h5>
<p>最後，是我們認為最進階，也最能體現「代理人」智慧的模式：「記憶即工具」。在這種模式下，我們不再設定寫死的規則，而是把「生成記憶」這件事，包裝成一個代理人可以呼叫的工具，例如 <code>create_memory</code>。</p>
<blockquote>
<p>然後，我們<strong>授權代理人自己去分析對話</strong>，當它判斷當前的資訊具有長期保存的價值時，由它<strong>自主決定</strong>去呼叫這個工具。這就把觸發的責任，從開發者寫死的規則，轉移到了代理人自身的智慧判斷上。</p>
</blockquote>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：在介紹這幾種策略時，可以畫一個三角形，三個頂點分別是「成本」、「延遲」和「保真度」。強調每種策略都是在這個三角形上的不同取捨點。</li>
<li><strong>補充案例</strong>：可以補充說明，沒有哪種策略是絕對最好的，完全取決於應用場景。例如，一個需要高度個人化的心理諮詢代理人，可能傾向於即時觸發；而一個普通的問答機器人，或許在對話結束時觸發就足夠了。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>剛剛提到的最後一點，「記憶即工具」，是一個非常強大的設計模式。它代表了一種趨勢，也就是讓代理人從一個只會遵從指令的程式，進化成一個能自主管理其知識的智慧體。下一頁，我們將深入探討這個進階模式，看看它是如何運作的，以及它為我們帶來了哪些好處。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-21" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 21 頁：Triggering_Memory_Generation

#### 【本頁重點摘要】
*   何時觸發記憶生成，是一個在「資料新鮮度」、「成本」與「延遲」之間取得平衡的關鍵架構決策。
*   常見的觸發策略包含：在對話結束時、按固定頻率（如每 N 次對話）、即時（每次對話後）、或由使用者下達明確指令。
*   最進階的模式是「記憶即工具 (Memory-as-a-Tool)」，也就是讓代理人 (Agent) 自主判斷何時該呼叫工具來儲存記憶。

---

#### 【逐字講稿】

(開場白)
好，我們已經了解了記憶的生成是一個強大的自動化流程。但一個非常實際的問題是：這個流程到底應該「何時」啟動？這不是一個無關緊要的細節，而是一個核心的架構選擇，它直接決定了你的代理人記憶有多即時、營運成本有多高，以及使用者感受到的延遲有多久。

這頁投影片，我們就來探討幾種主流的觸發策略，以及它們各自的權衡。

##### ① 對話完成時 (Session Completion)
這是最簡單直接的方法。我們等到一整段對話完全結束後，再把整個對話紀錄丟給記憶管理器去處理。

> 這就像我們下班後，才一次性地整理今天一整天辦公桌上的所有文件。它的好處是**成本效益最高**，因為你只執行了一次昂貴的生成流程。但缺點是**保真度 (fidelity) 較低**，因為 AI 需要一次消化大量的對話，可能會遺漏掉一些細微的重點。

##### ② 固定頻率觸發 (Turn Cadence)
一個折衷的方案是，設定一個固定的頻率，例如每 5 次或 10 次對話，就觸發一次記憶生成。這就像是每隔一兩個小時，就稍微整理一下桌面。它在成本和資料新鮮度之間取得了不錯的平衡，對於很多應用來說，這是一個「足夠好」的選擇。

##### ③ 即時觸發 (Real-Time)
接著是即時觸發，也就是在每一次對話結束後，立刻生成記憶。這種方式能提供**最高的保真度和最新鮮的記憶**，任何細節都會在發生的當下被捕捉。但可想而知，這也是**成本最高、對系統負擔最大**的選項。如果處理不當，很容易因為等待記憶寫入而增加使用者感受到的延遲。

##### ④ 使用者明確指令 (Explicit Command)
我們也可以把控制權交給使用者。當使用者明確地說出「記住這件事」或類似的指令時，我們才觸發記憶生成。這個方法非常直接，而且能確保被存下來的，絕對是使用者認為重要的資訊。

##### ⑤ 記憶即工具 (Memory-as-a-Tool)
最後，是我們認為最進階，也最能體現「代理人」智慧的模式：「記憶即工具」。在這種模式下，我們不再設定寫死的規則，而是把「生成記憶」這件事，包裝成一個代理人可以呼叫的工具，例如 `create_memory`。

> 然後，我們**授權代理人自己去分析對話**，當它判斷當前的資訊具有長期保存的價值時，由它**自主決定**去呼叫這個工具。這就把觸發的責任，從開發者寫死的規則，轉移到了代理人自身的智慧判斷上。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在介紹這幾種策略時，可以畫一個三角形，三個頂點分別是「成本」、「延遲」和「保真度」。強調每種策略都是在這個三角形上的不同取捨點。
*   **補充案例**：可以補充說明，沒有哪種策略是絕對最好的，完全取決於應用場景。例如，一個需要高度個人化的心理諮詢代理人，可能傾向於即時觸發；而一個普通的問答機器人，或許在對話結束時觸發就足夠了。
*   **轉場橋樑 (Bridge)**：
    > 剛剛提到的最後一點，「記憶即工具」，是一個非常強大的設計模式。它代表了一種趨勢，也就是讓代理人從一個只會遵從指令的程式，進化成一個能自主管理其知識的智慧體。下一頁，我們將深入探討這個進階模式，看看它是如何運作的，以及它為我們帶來了哪些好處。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-22">
            <div class="slide">
                <h2>Slide 22</h2>
                <div class="rendered-content">
                    <h1>Advanced Pattern: Memory-as-a-Tool</h1>
<p><strong>A sophisticated approach where the agent decides for itself when to create or retrieve a memory.</strong></p>
<ul>
<li>
<p><strong>How it works:</strong> Memory generation and retrieval are exposed to the agent as tools (e.g., <code>create_memory</code>, <code>search_memory</code>).</p>
</li>
<li>
<p><strong>Shift in Responsibility:</strong> The agent's LLM, guided by the tool's description and the conversation's context, autonomously decides when it's appropriate to call these tools.</p>
</li>
<li>
<p><strong>Benefits:</strong></p>
<ul>
<li>More efficient and robust than triggering on every turn.</li>
<li>Allows the agent to be more deliberate about what it remembers and when it accesses that memory.</li>
</ul>
</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 22</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(22, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(22, 'raw')">Source</button>
                </div>
                <div id="note-rendered-22" class="note-content rendered-content">
                    <h3>🎙️ 第 22 頁：Memory-as-a-Tool_Pattern</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>這是一種更進階的模式，讓代理人 (Agent) 自己決定何時要建立或取回記憶。</li>
<li>作法是將記憶的生成 (<code>create_memory</code>) 與檢索 (<code>search_memory</code>) 功能，包裝成「工具」供代理人使用。</li>
<li>責任從固定的觸發機制，轉移到由代理人根據對話上下文，自主判斷何時呼叫這些工具。</li>
<li>這種模式比每一輪都觸發記憶更有效率、更穩健，也讓代理人的行為更具目的性。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好，我們剛剛討論了幾種觸發記憶生成的時機，比如在對話結束時，或是每隔幾輪就觸發一次。但這些都屬於比較「被動」或「固定」的策略。現在，我們要介紹一個更進階、更智慧的模式，叫做「Memory-as-a-Tool」，也就是「將記憶視為一種工具」。</p>
<h5>① 什麼是「將記憶視為工具」？</h5>
<p>這個概念的核心，是賦予代理人<strong>主動權</strong>。我們不再死板地規定它「何時」該去記憶或回想，而是讓它自己<strong>決定</strong>。</p>
<blockquote>
<p>這就像從一個只會「被動記錄」的學徒，進化成一個懂得「主動思考」的專家。專家不會把所有聽到的話都記下來，他會判斷：「嗯，這段資訊很重要，我得記下來」，或者「這個問題我需要查一下之前的筆記」。</p>
</blockquote>
<p>在這個模式下，代理人就是那個專家。</p>
<h5>② 它是如何運作的？</h5>
<p>運作方式非常巧妙。我們會把「建立記憶」和「搜尋記憶」這兩個動作，包裝成代理人可以呼叫的工具，就像它呼叫計算機或網路搜尋一樣。</p>
<ul>
<li>例如，我們可以建立一個叫做 <code>create_memory</code> 的工具，和一個 <code>search_memory</code> 的工具。</li>
<li>當對話進行時，代理人的大腦，也就是 LLM，會根據它對工具的描述以及當前的對話內容，來自主判斷：「現在，是時候呼叫 <code>create_memory</code> 工具來記住用戶剛剛提到的偏好了」，或者「用戶問了一個跟上次討論相關的問題，我應該呼叫 <code>search_memory</code> 工具來找回相關記憶。」</li>
</ul>
<p>這意味著，<strong>判斷「什麼資訊有意義」的責任，從外部的記憶體管理器轉移到了代理人本身</strong>。它變得更加自主和智慧。</p>
<h5>③ 這種模式有什麼好處？</h5>
<p>主要有三大好處：</p>
<ul>
<li><strong>更有效率 (Efficient)</strong>：代理人只在它認為必要的時候，才執行昂貴的記憶操作，而不是每一輪都盲目地執行，這大大節省了成本和延遲。</li>
<li><strong>更穩健 (Robust)</strong>：因為決策是基於當前的上下文，所以更加精準，避免了在不相關的時候存取記憶，反而造成干擾。</li>
<li><strong>更具目的性 (Deliberate)</strong>：代理人的行為不再是機械式的，而是有意識、有目的地去管理和使用它的知識庫。</li>
</ul>
<p>正如我們在原始資料中看到的，開發者可以透過 ADK 這樣的框架，將記憶生成的程式碼包裝成一個工具，讓代理人決定何時調用它，將整個對話內容交給 Memory Bank 進行提取和整合。甚至可以做到更精細，讓代理人自己從對話中提取出關鍵事實，然後再呼叫工具，只將這個精煉過的事實存入記憶庫。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：講到「責任轉移」時可以加重語氣，這是這個模式的核心思想——賦予代理人更大的自主權。</li>
<li><strong>補充案例</strong>：可以舉例，一個旅遊規劃代理人，在用戶提到「我對歷史古蹟很有興趣」時，它會自主決定呼叫 <code>create_memory</code> 工具記下這個偏好。當用戶後來說「幫我推薦一些景點」時，它又會自主呼叫 <code>search_memory</code> 工具，並把「歷史古蹟」作為關鍵字去搜尋。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>透過「Memory-as-a-Tool」，我們的代理人學會了「何時」去存取記憶。但這引出了下一個關鍵問題：當它決定要找回記憶時，系統應該「如何」從龐大的記憶庫中，精準地找出最相關的資訊呢？這就是我們下一頁要探討的——記憶檢索的策略。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-22" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 22 頁：Memory-as-a-Tool_Pattern

#### 【本頁重點摘要】
*   這是一種更進階的模式，讓代理人 (Agent) 自己決定何時要建立或取回記憶。
*   作法是將記憶的生成 (`create_memory`) 與檢索 (`search_memory`) 功能，包裝成「工具」供代理人使用。
*   責任從固定的觸發機制，轉移到由代理人根據對話上下文，自主判斷何時呼叫這些工具。
*   這種模式比每一輪都觸發記憶更有效率、更穩健，也讓代理人的行為更具目的性。

---

#### 【逐字講稿】

(開場白)
好，我們剛剛討論了幾種觸發記憶生成的時機，比如在對話結束時，或是每隔幾輪就觸發一次。但這些都屬於比較「被動」或「固定」的策略。現在，我們要介紹一個更進階、更智慧的模式，叫做「Memory-as-a-Tool」，也就是「將記憶視為一種工具」。

##### ① 什麼是「將記憶視為工具」？
這個概念的核心，是賦予代理人**主動權**。我們不再死板地規定它「何時」該去記憶或回想，而是讓它自己**決定**。

> 這就像從一個只會「被動記錄」的學徒，進化成一個懂得「主動思考」的專家。專家不會把所有聽到的話都記下來，他會判斷：「嗯，這段資訊很重要，我得記下來」，或者「這個問題我需要查一下之前的筆記」。

在這個模式下，代理人就是那個專家。

##### ② 它是如何運作的？
運作方式非常巧妙。我們會把「建立記憶」和「搜尋記憶」這兩個動作，包裝成代理人可以呼叫的工具，就像它呼叫計算機或網路搜尋一樣。

*   例如，我們可以建立一個叫做 `create_memory` 的工具，和一個 `search_memory` 的工具。
*   當對話進行時，代理人的大腦，也就是 LLM，會根據它對工具的描述以及當前的對話內容，來自主判斷：「現在，是時候呼叫 `create_memory` 工具來記住用戶剛剛提到的偏好了」，或者「用戶問了一個跟上次討論相關的問題，我應該呼叫 `search_memory` 工具來找回相關記憶。」

這意味著，**判斷「什麼資訊有意義」的責任，從外部的記憶體管理器轉移到了代理人本身**。它變得更加自主和智慧。

##### ③ 這種模式有什麼好處？
主要有三大好處：

*   **更有效率 (Efficient)**：代理人只在它認為必要的時候，才執行昂貴的記憶操作，而不是每一輪都盲目地執行，這大大節省了成本和延遲。
*   **更穩健 (Robust)**：因為決策是基於當前的上下文，所以更加精準，避免了在不相關的時候存取記憶，反而造成干擾。
*   **更具目的性 (Deliberate)**：代理人的行為不再是機械式的，而是有意識、有目的地去管理和使用它的知識庫。

正如我們在原始資料中看到的，開發者可以透過 ADK 這樣的框架，將記憶生成的程式碼包裝成一個工具，讓代理人決定何時調用它，將整個對話內容交給 Memory Bank 進行提取和整合。甚至可以做到更精細，讓代理人自己從對話中提取出關鍵事實，然後再呼叫工具，只將這個精煉過的事實存入記憶庫。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講到「責任轉移」時可以加重語氣，這是這個模式的核心思想——賦予代理人更大的自主權。
*   **補充案例**：可以舉例，一個旅遊規劃代理人，在用戶提到「我對歷史古蹟很有興趣」時，它會自主決定呼叫 `create_memory` 工具記下這個偏好。當用戶後來說「幫我推薦一些景點」時，它又會自主呼叫 `search_memory` 工具，並把「歷史古蹟」作為關鍵字去搜尋。
*   **轉場橋樑 (Bridge)**：
    > 透過「Memory-as-a-Tool」，我們的代理人學會了「何時」去存取記憶。但這引出了下一個關鍵問題：當它決定要找回記憶時，系統應該「如何」從龐大的記憶庫中，精準地找出最相關的資訊呢？這就是我們下一頁要探討的——記憶檢索的策略。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-23">
            <div class="slide">
                <h2>Slide 23</h2>
                <div class="rendered-content">
                    <h1>Memory Retrieval Strategies</h1>
<p><strong>Goal: Find the most pertinent memories for the current conversation.</strong></p>
<ul>
<li>
<p>A simple vector search for relevance is often not enough.</p>
</li>
<li>
<p>Advanced systems score memories across multiple dimensions:</p>
<ul>
<li><strong>Relevance (Semantic Similarity):</strong> How conceptually related is the memory?</li>
<li><strong>Recency (Time-based):</strong> How recently was the memory created?</li>
<li><strong>Importance (Significance):</strong> How critical is this memory overall?</li>
</ul>
</li>
<li>
<p><strong>Advanced Techniques (High Latency):</strong></p>
<ul>
<li><strong>Query Rewriting:</strong> Use an LLM to improve the search query itself.</li>
<li><strong>Reranking:</strong> Use an LLM to re-rank an initial broad set of results.</li>
</ul>
</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 23</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(23, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(23, 'raw')">Source</button>
                </div>
                <div id="note-rendered-23" class="note-content rendered-content">
                    <h3>🎙️ 第 23 頁：Memory_Retrieval_Strategies</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>記憶檢索的目標是為當前對話找到最相關的記憶，而不僅僅是基於向量相似度。</li>
<li>高效的檢索策略會綜合考量記憶的「相關性」、「新近性」和「重要性」等多個維度。</li>
<li>針對高精確度需求，可採用查詢重寫或重新排序等進階技術，但需權衡其帶來的延遲。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，我們已經了解了記憶是如何生成和整合的。現在，有了這些寶貴的記憶庫，下一個關鍵問題就是：我們該如何有效地「找回」它們？這頁投影片，我們將深入探討記憶檢索的策略，也就是如何從龐大的記憶中，精準地找到當前對話最需要的資訊。</p>
<h5>① 檢索的目標與挑戰</h5>
<p>記憶檢索的核心目標，是為當前的對話找到<strong>最相關、最恰當</strong>的記憶。這聽起來簡單，但實際上，如果只是單純地依賴向量搜尋來判斷「相關性」，往往是不夠的。因為單純的相似度分數，可能會找出概念上相似但卻過時或不重要的記憶，反而會混淆模型，甚至降低回應品質。</p>
<blockquote>
<p>想像一下，你問一個 AI 助理：「我上次說想買什麼書？」如果它只根據「書」這個關鍵字，給你推薦了三年前你隨口提過的一本舊書，而不是你上週才熱烈討論的新書，那體驗肯定不好。</p>
</blockquote>
<h5>② 多維度評分，提升檢索精準度</h5>
<p>因此，先進的記憶系統會超越單一的向量相似度，從多個維度來評估和排序潛在的記憶。這就像我們人類回憶事情一樣，會綜合考量：</p>
<ul>
<li><strong>相關性 (Semantic Similarity)</strong>：這條記憶與當前對話在概念上有多接近？這是最基本的判斷。</li>
<li><strong>新近性 (Recency)</strong>：這條記憶是多久以前建立的？最近發生的事情，通常對當前對話更有意義。</li>
<li><strong>重要性 (Importance)</strong>：這條記憶整體而言有多關鍵？有些記憶可能不常被提及，但其重要性卻是長期的，例如用戶的個人偏好或核心目標。</li>
</ul>
<p>最有效的策略，就是將這三個維度的分數結合起來，進行<strong>混合式評分</strong>，以確保檢索到的記憶既相關又實用。</p>
<h5>③ 進階檢索技術：精準但高延遲</h5>
<p>對於那些對精確度要求極高的應用場景，我們還可以採用一些更為精密的進階技術來優化檢索結果。這些技術雖然能顯著提升準確性，但通常會伴隨著較高的計算成本和延遲：</p>
<ul>
<li><strong>查詢重寫 (Query Rewriting)</strong>：利用另一個 LLM 來改寫或擴展用戶的原始查詢。例如，將一個模糊的提問改寫成更精確的搜尋語句，或者將單一查詢擴展為多個相關查詢，以捕捉主題的不同面向。這能讓初始搜尋的品質大幅提升。</li>
<li><strong>重新排序 (Reranking)</strong>：首先透過向量搜尋快速檢索出一批候選記憶（例如前 50 條），然後再使用一個 LLM 對這小範圍的結果進行更細緻的評估和重新排序，從中選出最優的幾條。</li>
</ul>
<p>這些技術就像是為記憶檢索加上了「精修」環節，確保最終呈現給模型的記憶是經過層層篩選的黃金資訊。然而，由於它們涉及額外的 LLM 呼叫，會增加處理時間，因此在即時性要求高的場景中需要謹慎使用。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：在解釋完多維度評分和進階技術後，可以強調「平衡」的重要性，即在精準度和效率之間找到最佳點。</li>
<li><strong>補充案例</strong>：可以舉例說明，在客服場景中，新近性可能比重要性更關鍵；而在個人助理場景中，用戶偏好（重要性）則可能優先於新近性。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>了解了如何「找」記憶之後，下一個問題自然就是：「什麼時候該找？」是每次對話都主動載入，還是等到需要時再檢索？下一頁，我們將探討記憶檢索的「時機」選擇，也就是「主動式」與「反應式」檢索策略的權衡。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-23" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 23 頁：Memory_Retrieval_Strategies

#### 【本頁重點摘要】
*   記憶檢索的目標是為當前對話找到最相關的記憶，而不僅僅是基於向量相似度。
*   高效的檢索策略會綜合考量記憶的「相關性」、「新近性」和「重要性」等多個維度。
*   針對高精確度需求，可採用查詢重寫或重新排序等進階技術，但需權衡其帶來的延遲。

---

#### 【逐字講稿】

(開場白)
好的，我們已經了解了記憶是如何生成和整合的。現在，有了這些寶貴的記憶庫，下一個關鍵問題就是：我們該如何有效地「找回」它們？這頁投影片，我們將深入探討記憶檢索的策略，也就是如何從龐大的記憶中，精準地找到當前對話最需要的資訊。

##### ① 檢索的目標與挑戰
記憶檢索的核心目標，是為當前的對話找到**最相關、最恰當**的記憶。這聽起來簡單，但實際上，如果只是單純地依賴向量搜尋來判斷「相關性」，往往是不夠的。因為單純的相似度分數，可能會找出概念上相似但卻過時或不重要的記憶，反而會混淆模型，甚至降低回應品質。

> 想像一下，你問一個 AI 助理：「我上次說想買什麼書？」如果它只根據「書」這個關鍵字，給你推薦了三年前你隨口提過的一本舊書，而不是你上週才熱烈討論的新書，那體驗肯定不好。

##### ② 多維度評分，提升檢索精準度
因此，先進的記憶系統會超越單一的向量相似度，從多個維度來評估和排序潛在的記憶。這就像我們人類回憶事情一樣，會綜合考量：

*   **相關性 (Semantic Similarity)**：這條記憶與當前對話在概念上有多接近？這是最基本的判斷。
*   **新近性 (Recency)**：這條記憶是多久以前建立的？最近發生的事情，通常對當前對話更有意義。
*   **重要性 (Importance)**：這條記憶整體而言有多關鍵？有些記憶可能不常被提及，但其重要性卻是長期的，例如用戶的個人偏好或核心目標。

最有效的策略，就是將這三個維度的分數結合起來，進行**混合式評分**，以確保檢索到的記憶既相關又實用。

##### ③ 進階檢索技術：精準但高延遲
對於那些對精確度要求極高的應用場景，我們還可以採用一些更為精密的進階技術來優化檢索結果。這些技術雖然能顯著提升準確性，但通常會伴隨著較高的計算成本和延遲：

*   **查詢重寫 (Query Rewriting)**：利用另一個 LLM 來改寫或擴展用戶的原始查詢。例如，將一個模糊的提問改寫成更精確的搜尋語句，或者將單一查詢擴展為多個相關查詢，以捕捉主題的不同面向。這能讓初始搜尋的品質大幅提升。
*   **重新排序 (Reranking)**：首先透過向量搜尋快速檢索出一批候選記憶（例如前 50 條），然後再使用一個 LLM 對這小範圍的結果進行更細緻的評估和重新排序，從中選出最優的幾條。

這些技術就像是為記憶檢索加上了「精修」環節，確保最終呈現給模型的記憶是經過層層篩選的黃金資訊。然而，由於它們涉及額外的 LLM 呼叫，會增加處理時間，因此在即時性要求高的場景中需要謹慎使用。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在解釋完多維度評分和進階技術後，可以強調「平衡」的重要性，即在精準度和效率之間找到最佳點。
*   **補充案例**：可以舉例說明，在客服場景中，新近性可能比重要性更關鍵；而在個人助理場景中，用戶偏好（重要性）則可能優先於新近性。
*   **轉場橋樑 (Bridge)**：
    > 了解了如何「找」記憶之後，下一個問題自然就是：「什麼時候該找？」是每次對話都主動載入，還是等到需要時再檢索？下一頁，我們將探討記憶檢索的「時機」選擇，也就是「主動式」與「反應式」檢索策略的權衡。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-24">
            <div class="slide">
                <h2>Slide 24</h2>
                <div class="rendered-content">
                    <h1>Timing for Retrieval: Proactive vs. Reactive</h1>
<p><strong>When should the agent retrieve memories?</strong></p>
<ol>
<li>
<p><strong>Proactive Retrieval (Eager):</strong></p>
<ul>
<li>Memories are automatically loaded at the start of <strong>every turn</strong>.</li>
<li><strong>Pros:</strong> Context is always available.</li>
<li><strong>Cons:</strong> Can introduce unnecessary latency if the turn doesn't require memory.</li>
</ul>
</li>
<li>
<p><strong>Reactive Retrieval (Lazy / Memory-as-a-Tool):</strong></p>
<ul>
<li>The agent is given a tool to query memory and decides for itself <strong>when to retrieve context</strong>.</li>
<li><strong>Pros:</strong> More efficient; latency is only incurred when needed.</li>
<li><strong>Cons:</strong> Requires an additional LLM call to decide; agent might not know relevant info exists.</li>
</ul>
</li>
</ol>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 24</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(24, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(24, 'raw')">Source</button>
                </div>
                <div id="note-rendered-24" class="note-content rendered-content">
                    <h3>🎙️ 第 24 頁：Timing_for_Retrieval_Proactive_vs._Reactive</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>記憶檢索的時機，是在「延遲」與「情境完整性」之間的關鍵權衡。</li>
<li>主要有兩種策略：主動式檢索 (Proactive)，每次都預先載入；以及反應式檢索 (Reactive)，在需要時才由代理人決定去查詢。</li>
<li>主動式雖然確保資訊隨時可用，但可能造成不必要的延遲；反應式更有效率，但增加了決策的複雜性。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好，我們已經知道如何設計一個好的檢索策略，來找出最相關的記憶。但下一個關鍵的架構問題是：<strong>到底該在「什麼時候」去執行這個檢索動作？</strong> 這是一個非常重要的決定，它會直接影響到我們代理人的反應速度和智慧程度。</p>
<p>這頁投影片就為我們展示了兩種截然不同的哲學：主動式與反應式。</p>
<h5>① 第一種策略：主動式檢索 (Proactive Retrieval)，或稱為「積極型」</h5>
<p>這個策略非常直接。它的做法是，在<strong>每一次</strong>對話開始時，就自動去記憶庫裡把相關的記憶撈出來，準備好。</p>
<blockquote>
<p>這就像一位極度殷勤的助理。在你跟任何人開會前，他都會把所有可能用到的檔案、過去的會議記錄，全部攤在桌上，以防萬一。</p>
</blockquote>
<ul>
<li><strong>優點</strong>很明顯：<strong>情境永遠都在</strong>。代理人隨時都處於「準備好」的狀態，不會因為缺少資訊而措手不及。</li>
<li>但<strong>缺點</strong>也同樣致命：<strong>延遲</strong>。在很多對話回合中，代理人根本不需要參考過去的記憶。這種「過度準備」會導致不必要的計算和等待，讓使用者感覺代理人反應很慢、很卡。</li>
</ul>
<h5>② 第二種策略：反應式檢索 (Reactive Retrieval)，也稱為「懶人型」或「工具型」</h5>
<p>這個方法就聰明多了。我們不讓系統每次都傻傻地去撈資料，而是把「查詢記憶」這件事，本身變成一個<strong>工具</strong>，交給代理人自己。</p>
<blockquote>
<p>回到剛剛的助理比喻。這次，助理不會主動攤開所有文件。他會靜靜地站在一旁，直到你問：「嘿，我們上次討論那個專案的結論是什麼？」這時，他才會去檔案櫃裡，精準地找出那份文件給你。</p>
</blockquote>
<ul>
<li><strong>優點</strong>是<strong>高效率</strong>。只有在代理人（也就是 LLM）判斷當前對話確實需要參考過去的記憶時，才會觸發檢索動作。這大大降低了不必要的延遲，讓互動更流暢。</li>
<li>但它也有<strong>挑戰</strong>。首先，代理人「決定」要使用這個工具，本身就需要一次額外的 LLM 推理，這會增加一點點成本和延遲。更重要的是，代理人<strong>可能根本不知道有相關的資訊存在</strong>。就像你如果忘了自己有做過某份會議記錄，你自然也想不到要去查詢它。不過，我們可以透過在工具的描述中，提示代理人「記憶庫裡可能有哪些類型的資訊」，來稍微緩解這個問題。</li>
</ul>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：講完這兩種策略後，可以停頓一下，強調這是一個「沒有標準答案」的設計抉擇。開發者需要根據應用的情境（例如，是需要極快反應，還是需要極度周全）來做權衡。</li>
<li><strong>強化比喻</strong>：「積極型」就像一位過度準備的助理，保證萬無一失但有點囉嗦；「反應式」則像一位更聰明、更懂得察言觀色的專家，只在關鍵時刻出手。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>所以，無論我們是積極地預先載入記憶，還是被動地等代理人自己去查詢，一旦我們拿到了這些寶貴的記憶，下一個問題就來了：我們該如何把它們呈現給模型呢？是把它當成最高機密的「系統指令」偷偷塞進去，還是把它當成一段對話，直接插入到歷史紀錄中？這兩種不同的「放置」方式，會對模型的思考產生截然不同的影響。下一頁，我們就來探討這個問題。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-24" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 24 頁：Timing_for_Retrieval_Proactive_vs._Reactive

#### 【本頁重點摘要】
*   記憶檢索的時機，是在「延遲」與「情境完整性」之間的關鍵權衡。
*   主要有兩種策略：主動式檢索 (Proactive)，每次都預先載入；以及反應式檢索 (Reactive)，在需要時才由代理人決定去查詢。
*   主動式雖然確保資訊隨時可用，但可能造成不必要的延遲；反應式更有效率，但增加了決策的複雜性。

---

#### 【逐字講稿】

(開場白)
好，我們已經知道如何設計一個好的檢索策略，來找出最相關的記憶。但下一個關鍵的架構問題是：**到底該在「什麼時候」去執行這個檢索動作？** 這是一個非常重要的決定，它會直接影響到我們代理人的反應速度和智慧程度。

這頁投影片就為我們展示了兩種截然不同的哲學：主動式與反應式。

##### ① 第一種策略：主動式檢索 (Proactive Retrieval)，或稱為「積極型」
這個策略非常直接。它的做法是，在**每一次**對話開始時，就自動去記憶庫裡把相關的記憶撈出來，準備好。

> 這就像一位極度殷勤的助理。在你跟任何人開會前，他都會把所有可能用到的檔案、過去的會議記錄，全部攤在桌上，以防萬一。

*   **優點**很明顯：**情境永遠都在**。代理人隨時都處於「準備好」的狀態，不會因為缺少資訊而措手不及。
*   但**缺點**也同樣致命：**延遲**。在很多對話回合中，代理人根本不需要參考過去的記憶。這種「過度準備」會導致不必要的計算和等待，讓使用者感覺代理人反應很慢、很卡。

##### ② 第二種策略：反應式檢索 (Reactive Retrieval)，也稱為「懶人型」或「工具型」
這個方法就聰明多了。我們不讓系統每次都傻傻地去撈資料，而是把「查詢記憶」這件事，本身變成一個**工具**，交給代理人自己。

> 回到剛剛的助理比喻。這次，助理不會主動攤開所有文件。他會靜靜地站在一旁，直到你問：「嘿，我們上次討論那個專案的結論是什麼？」這時，他才會去檔案櫃裡，精準地找出那份文件給你。

*   **優點**是**高效率**。只有在代理人（也就是 LLM）判斷當前對話確實需要參考過去的記憶時，才會觸發檢索動作。這大大降低了不必要的延遲，讓互動更流暢。
*   但它也有**挑戰**。首先，代理人「決定」要使用這個工具，本身就需要一次額外的 LLM 推理，這會增加一點點成本和延遲。更重要的是，代理人**可能根本不知道有相關的資訊存在**。就像你如果忘了自己有做過某份會議記錄，你自然也想不到要去查詢它。不過，我們可以透過在工具的描述中，提示代理人「記憶庫裡可能有哪些類型的資訊」，來稍微緩解這個問題。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講完這兩種策略後，可以停頓一下，強調這是一個「沒有標準答案」的設計抉擇。開發者需要根據應用的情境（例如，是需要極快反應，還是需要極度周全）來做權衡。
*   **強化比喻**：「積極型」就像一位過度準備的助理，保證萬無一失但有點囉嗦；「反應式」則像一位更聰明、更懂得察言觀色的專家，只在關鍵時刻出手。
*   **轉場橋樑 (Bridge)**：
    > 所以，無論我們是積極地預先載入記憶，還是被動地等代理人自己去查詢，一旦我們拿到了這些寶貴的記憶，下一個問題就來了：我們該如何把它們呈現給模型呢？是把它當成最高機密的「系統指令」偷偷塞進去，還是把它當成一段對話，直接插入到歷史紀錄中？這兩種不同的「放置」方式，會對模型的思考產生截然不同的影響。下一頁，我們就來探討這個問題。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-25">
            <div class="slide">
                <h2>Slide 25</h2>
                <div class="rendered-content">
                    <h1>Inference: Placing Memories in Context</h1>
<p>Where do retrieved memories go in the prompt?</p>
<ol>
<li>
<p><strong>In the System Instructions:</strong></p>
<ul>
<li>Memories are appended to the system prompt, giving them high authority.</li>
<li><strong>Best for:</strong> Stable, &quot;global&quot; information like a user profile.</li>
<li><strong>Cons:</strong> Risk of over-influence; incompatible with Memory-as-a-Tool.</li>
</ul>
</li>
<li>
<p><strong>In the Conversation History:</strong></p>
<ul>
<li>Memories are injected directly into the turn-by-turn dialogue (often as tool output).</li>
<li><strong>Best for:</strong> Transient, episodic memories relevant to the immediate context.</li>
<li><strong>Cons:</strong> Can be noisy; risk of model treating memory as something that was actually said.</li>
</ul>
</li>
</ol>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 25</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(25, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(25, 'raw')">Source</button>
                </div>
                <div id="note-rendered-25" class="note-content rendered-content">
                    <h3>🎙️ 第 25 頁：Inference_with_Memories_Placement_in_Context</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>成功檢索到記憶後，我們必須決定將它放在提示 (Prompt) 中的哪個位置。</li>
<li>主要有兩種策略：放入「系統指令」中，賦予其高權威性；或注入「對話歷史」中，使其更貼近當前上下文。</li>
<li>兩種方法各有優劣，最佳實踐通常是混合使用。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好，我們前面討論了如何產生記憶、如何檢索記憶。現在，我們來到了最後，也是至關重要的一步。我們已經從記憶庫中找到了最相關的資訊，但問題是... <strong>我們該把這些記憶放在提示 (Prompt) 的哪個位置，才能讓模型最有效地利用它呢？</strong> 這不是一個隨意的決定，放錯位置可能會讓模型混淆，甚至產生反效果。</p>
<p>這頁投影片，我們就來探討兩種最主流的策略。</p>
<h5>① 第一種策略：放在系統指令 (System Instructions) 中</h5>
<p>這可以說是最直接的方法。我們把檢索到的記憶，直接附加到系統指令的後面。你可以把它想像成，在 AI 開始對話前，我們先遞給它一份關於這位用戶的「個人檔案」或「背景提要」。</p>
<blockquote>
<p>這種做法會賦予這些記憶非常高的<strong>權威性</strong>。模型會把它們當作是這次互動最核心、最基礎的背景知識。這非常適合用來放置那些穩定、全域性的資訊，比如用戶的個人資料、固定的偏好設定等等。</p>
</blockquote>
<p>然而，這種方法也有幾個明顯的缺點。首先是<strong>過度影響 (over-influence)</strong> 的風險。因為這些記憶在系統指令中的地位太高，模型可能會過度解讀，試圖把所有話題都跟這些記憶扯上關係，即便有時候並不合適。其次，這個方法與我們之前提到的「記憶即工具 (Memory-as-a-Tool)」模式是<strong>不相容的</strong>。因為系統指令必須在模型決定是否要呼叫工具之前就準備好。</p>
<h5>② 第二種策略：注入對話歷史 (In the Conversation History)</h5>
<p>另一種方法，是將記憶動態地注入到一來一往的對話歷史當中。這就像是在對話進行到一半時，悄悄地遞給 AI 一張「小紙條」，提醒它一些相關的資訊。</p>
<p>這種方式最常見的實現，就是透過「工具呼叫」的結果。當代理人決定使用 <code>search_memory</code> 這個工具時，檢索到的記憶就會以工具輸出的形式，自然地出現在對話記錄裡。</p>
<blockquote>
<p>這種策略非常適合處理那些<strong>短暫的、片段式的記憶</strong>，也就是只跟當前對話上下文高度相關的資訊。</p>
</blockquote>
<p>但它的挑戰也不少。首先，它可能會讓對話歷史變得很「吵雜」，增加 token 成本。如果檢索到的記憶不夠精準，反而會干擾模型的判斷。最大的風險在於所謂的「<strong>對話注入 (dialogue injection)</strong>」，模型可能會誤以為這段記憶是某個人 (用戶或它自己) 真的在對話中說過的話，從而做出不自然的回應。</p>
<ul>
<li>舉個例子，如果你注入了一條記憶「事實：用戶喜歡喝咖啡」，模型可能會回覆：「你剛剛告訴我你喜歡喝咖啡」，這聽起來就很奇怪。因此，我們必須非常小心記憶的措辭和視角。</li>
</ul>
<p>在實務上，最好的方法往往是<strong>混合使用</strong>這兩種策略：用系統指令來存放穩定、全域的用戶輪廓；同時，用注入對話歷史的方式，來處理與當前任務相關的動態、情境式記憶。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：講完這兩種策略後，可以稍微停頓，讓聽眾思考一下這兩種方法的根本差異：「事前給予的背景檔案」 vs. 「事中傳遞的小紙條」。</li>
<li><strong>補充案例</strong>：可以舉例，像用戶的姓名、會員等級，就適合放在系統指令；而用戶上一輪對話提到的「想找藍色的外套」，就適合當作工具輸出的結果注入對話歷史。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>到目前為止，我們討論的所有記憶——無論是事實、偏好還是歷史——都屬於「陳述性記憶 (declarative memories)」，也就是「知道<strong>什麼</strong> (knowing what)」。但如果一個代理人需要記住的是「<strong>如何</strong> (knowing how)」去完成一個任務呢？這就引導我們進入一個更進階、也更強大的記憶類型。下一頁，我們就來探討「程序性記憶」。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-25" class="note-content note-raw" style="display: none;">
                    <pre>### 🎙️ 第 25 頁：Inference_with_Memories_Placement_in_Context

#### 【本頁重點摘要】
*   成功檢索到記憶後，我們必須決定將它放在提示 (Prompt) 中的哪個位置。
*   主要有兩種策略：放入「系統指令」中，賦予其高權威性；或注入「對話歷史」中，使其更貼近當前上下文。
*   兩種方法各有優劣，最佳實踐通常是混合使用。

---

#### 【逐字講稿】

(開場白)
好，我們前面討論了如何產生記憶、如何檢索記憶。現在，我們來到了最後，也是至關重要的一步。我們已經從記憶庫中找到了最相關的資訊，但問題是... **我們該把這些記憶放在提示 (Prompt) 的哪個位置，才能讓模型最有效地利用它呢？** 這不是一個隨意的決定，放錯位置可能會讓模型混淆，甚至產生反效果。

這頁投影片，我們就來探討兩種最主流的策略。

##### ① 第一種策略：放在系統指令 (System Instructions) 中
這可以說是最直接的方法。我們把檢索到的記憶，直接附加到系統指令的後面。你可以把它想像成，在 AI 開始對話前，我們先遞給它一份關於這位用戶的「個人檔案」或「背景提要」。

> 這種做法會賦予這些記憶非常高的**權威性**。模型會把它們當作是這次互動最核心、最基礎的背景知識。這非常適合用來放置那些穩定、全域性的資訊，比如用戶的個人資料、固定的偏好設定等等。

然而，這種方法也有幾個明顯的缺點。首先是**過度影響 (over-influence)** 的風險。因為這些記憶在系統指令中的地位太高，模型可能會過度解讀，試圖把所有話題都跟這些記憶扯上關係，即便有時候並不合適。其次，這個方法與我們之前提到的「記憶即工具 (Memory-as-a-Tool)」模式是**不相容的**。因為系統指令必須在模型決定是否要呼叫工具之前就準備好。

##### ② 第二種策略：注入對話歷史 (In the Conversation History)
另一種方法，是將記憶動態地注入到一來一往的對話歷史當中。這就像是在對話進行到一半時，悄悄地遞給 AI 一張「小紙條」，提醒它一些相關的資訊。

這種方式最常見的實現，就是透過「工具呼叫」的結果。當代理人決定使用 `search_memory` 這個工具時，檢索到的記憶就會以工具輸出的形式，自然地出現在對話記錄裡。

> 這種策略非常適合處理那些**短暫的、片段式的記憶**，也就是只跟當前對話上下文高度相關的資訊。

但它的挑戰也不少。首先，它可能會讓對話歷史變得很「吵雜」，增加 token 成本。如果檢索到的記憶不夠精準，反而會干擾模型的判斷。最大的風險在於所謂的「**對話注入 (dialogue injection)**」，模型可能會誤以為這段記憶是某個人 (用戶或它自己) 真的在對話中說過的話，從而做出不自然的回應。

*   舉個例子，如果你注入了一條記憶「事實：用戶喜歡喝咖啡」，模型可能會回覆：「你剛剛告訴我你喜歡喝咖啡」，這聽起來就很奇怪。因此，我們必須非常小心記憶的措辭和視角。

在實務上，最好的方法往往是**混合使用**這兩種策略：用系統指令來存放穩定、全域的用戶輪廓；同時，用注入對話歷史的方式，來處理與當前任務相關的動態、情境式記憶。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：講完這兩種策略後，可以稍微停頓，讓聽眾思考一下這兩種方法的根本差異：「事前給予的背景檔案」 vs. 「事中傳遞的小紙條」。
*   **補充案例**：可以舉例，像用戶的姓名、會員等級，就適合放在系統指令；而用戶上一輪對話提到的「想找藍色的外套」，就適合當作工具輸出的結果注入對話歷史。
*   **轉場橋樑 (Bridge)**：
    > 到目前為止，我們討論的所有記憶——無論是事實、偏好還是歷史——都屬於「陳述性記憶 (declarative memories)」，也就是「知道**什麼** (knowing what)」。但如果一個代理人需要記住的是「**如何** (knowing how)」去完成一個任務呢？這就引導我們進入一個更進階、也更強大的記憶類型。下一頁，我們就來探討「程序性記憶」。</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-26">
            <div class="slide">
                <h2>Slide 26</h2>
                <div class="rendered-content">
                    <h1>Procedural Memories: Capturing 'Knowing How'</h1>
<p>Most systems focus on declarative memory (&quot;knowing what&quot;). Procedural memory (&quot;knowing how&quot;) is different.</p>
<ul>
<li>
<p><strong>Goal:</strong> Not to retrieve a fact, but to retrieve a <strong>plan</strong> or <strong>playbook</strong> that guides the agent on how to execute a complex task.</p>
</li>
<li>
<p><strong>It is a reasoning augmentation problem, not an information retrieval problem.</strong></p>
</li>
<li>
<p><strong>Specialized Lifecycle:</strong> Requires different algorithms for extraction (distilling a strategy), consolidation (curating workflows), and retrieval (finding a plan).</p>
</li>
<li>
<p><strong>vs. Fine-Tuning:</strong></p>
<ul>
<li><strong>Fine-tuning:</strong> Slow, offline process that alters model weights.</li>
<li><strong>Procedural Memory:</strong> Fast, online adaptation via in-context learning.</li>
</ul>
</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 26</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(26, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(26, 'raw')">Source</button>
                </div>
                <div id="note-rendered-26" class="note-content rendered-content">
                    <h3>🎙️ 第 26 頁：Procedural_Memories_Capturing_'Knowing_How'</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>程序性記憶是關於「如何做」(knowing how)，而非我們之前討論的「知道什麼」(knowing what)。</li>
<li>它的目標不是檢索一個「事實」，而是檢索一個指導代理人執行複雜任務的「計畫」或「劇本」。</li>
<li>這本質上是一個「推理增強 (reasoning augmentation)」問題，而非「資訊檢索」問題。</li>
<li>與緩慢、離線的「微調 (Fine-tuning)」不同，程序性記憶透過「情境中學習 (in-context learning)」實現快速、線上的適應。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，到目前為止，我們討論的記憶，絕大多數都屬於「宣告式記憶」，也就是讓代理人記住「事實」、「數據」和「用戶偏好」——簡單來說，就是「知道什麼」。但現在，我們要探討一個更進階、也更能體現代理人智慧的概念：程序性記憶，也就是「知道如何做」。</p>
<h5>① 目標不同：檢索「計畫」，而非「事實」</h5>
<p>首先，我們必須理解程序性記憶的根本目標。它不是為了回答一個關於「是什麼」的問題，而是為了回答一個「該如何做」的問題。</p>
<blockquote>
<p>當代理人需要執行一個複雜的任務時，程序性記憶系統的目標，不是去資料庫裡找一個相關的「事實」，而是去找到一份完整的「計畫 (plan)」或「劇本 (playbook)」，這份劇本會一步步指導代理人該如何行動。</p>
</blockquote>
<p>這就像一個新手廚師，他需要的不是一本食材百科全書（宣告式記憶），而是一份詳細的食譜（程序性記憶）。</p>
<h5>② 本質差異：「推理增強」，而非「資訊檢索」</h5>
<p>這個目標上的差異，也決定了它的本質。我們必須強調，程序性記憶是一個<strong>推理增強 (reasoning augmentation)</strong> 的問題，而不是一個<strong>資訊檢索 (information retrieval)</strong> 的問題。</p>
<p>宣告式記憶，是把「資訊」檢索出來，放進上下文，讓模型去<strong>推理</strong>這些資訊。而程序性記憶，是把一個「推理的過程」或「成功的策略」本身檢索出來，讓模型去<strong>遵循</strong>這個過程。這是在更高維度上引導模型的行為。</p>
<h5>③ 專屬的生命週期</h5>
<p>正因為目標和本質都不同，管理「如何做」的知識，自然需要一套完全不同的演算法生命週期。</p>
<ul>
<li>它的<strong>提取 (Extraction)</strong>，不是捕捉事實，而是從一次成功的互動中，提煉出一個可重複使用的「策略」。</li>
<li>它的<strong>整合 (Consolidation)</strong>，不是合併相關事實，而是去管理和優化這些「工作流程」，比如更新一個既有的最佳實踐，或修補一個計畫中的瑕疵步驟。</li>
<li>它的<strong>檢索 (Retrieval)</strong>，目標也不是找最相關的資料，而是找到最適合當前任務的「計畫」。</li>
</ul>
<h5>④ 與「微調 (Fine-Tuning)」的比較</h5>
<p>談到讓代理人「學習如何做」，很多人會立刻想到「微調」。這兩者都旨在改善代理人的行為，但它們的機制和應用場景截然不同。</p>
<blockquote>
<p><strong>微調</strong>，像是一個緩慢的、離線的訓練過程，它會直接改變模型底層的權重。這就像是把一個人的性格徹底重塑，耗時且成本高昂。</p>
<p>而<strong>程序性記憶</strong>，提供的是一種快速、線上的適應能力。它不改變模型本身，而是在每次互動中，動態地將正確的「劇本」注入到提示中，透過我們之前提過的「情境中學習 (in-context learning)」，來即時指導代理人。這更像是在特定情境下，給這個人一本操作手冊，讓他立刻知道該怎麼做。</p>
</blockquote>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：這是一個比較抽象但非常重要的概念。可以放慢速度，特別是在解釋「推理增強」與「資訊檢索」的區別時，確保聽眾能跟上從「提供數據」到「提供方法」的思維轉變。</li>
<li><strong>強化類比</strong>：「廚師與食譜」或「教練給球員戰術手冊 (playbook)」的類比非常有效，可以在演說中多加利用，幫助聽眾理解。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>我們已經探討了宣告式記憶和程序性記憶這兩種強大的機制，它們分別讓代理人學會了「知道什麼」和「知道如何做」。但無論我們的記憶系統設計得多麼精巧，我們怎麼能確定它真的有效？它記住的東西是對的嗎？它能在需要時準確找到嗎？這就帶我們進入下一個至關重要的主題：如何科學地「測試與評估」我們的記憶系統。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-26" class="note-content note-raw" style="display: none;">
                    <pre>```markdown
### 🎙️ 第 26 頁：Procedural_Memories_Capturing_'Knowing_How'

#### 【本頁重點摘要】
*   程序性記憶是關於「如何做」(knowing how)，而非我們之前討論的「知道什麼」(knowing what)。
*   它的目標不是檢索一個「事實」，而是檢索一個指導代理人執行複雜任務的「計畫」或「劇本」。
*   這本質上是一個「推理增強 (reasoning augmentation)」問題，而非「資訊檢索」問題。
*   與緩慢、離線的「微調 (Fine-tuning)」不同，程序性記憶透過「情境中學習 (in-context learning)」實現快速、線上的適應。

---

#### 【逐字講稿】

(開場白)
好的，到目前為止，我們討論的記憶，絕大多數都屬於「宣告式記憶」，也就是讓代理人記住「事實」、「數據」和「用戶偏好」——簡單來說，就是「知道什麼」。但現在，我們要探討一個更進階、也更能體現代理人智慧的概念：程序性記憶，也就是「知道如何做」。

##### ① 目標不同：檢索「計畫」，而非「事實」
首先，我們必須理解程序性記憶的根本目標。它不是為了回答一個關於「是什麼」的問題，而是為了回答一個「該如何做」的問題。

> 當代理人需要執行一個複雜的任務時，程序性記憶系統的目標，不是去資料庫裡找一個相關的「事實」，而是去找到一份完整的「計畫 (plan)」或「劇本 (playbook)」，這份劇本會一步步指導代理人該如何行動。

這就像一個新手廚師，他需要的不是一本食材百科全書（宣告式記憶），而是一份詳細的食譜（程序性記憶）。

##### ② 本質差異：「推理增強」，而非「資訊檢索」
這個目標上的差異，也決定了它的本質。我們必須強調，程序性記憶是一個**推理增強 (reasoning augmentation)** 的問題，而不是一個**資訊檢索 (information retrieval)** 的問題。

宣告式記憶，是把「資訊」檢索出來，放進上下文，讓模型去**推理**這些資訊。而程序性記憶，是把一個「推理的過程」或「成功的策略」本身檢索出來，讓模型去**遵循**這個過程。這是在更高維度上引導模型的行為。

##### ③ 專屬的生命週期
正因為目標和本質都不同，管理「如何做」的知識，自然需要一套完全不同的演算法生命週期。

*   它的**提取 (Extraction)**，不是捕捉事實，而是從一次成功的互動中，提煉出一個可重複使用的「策略」。
*   它的**整合 (Consolidation)**，不是合併相關事實，而是去管理和優化這些「工作流程」，比如更新一個既有的最佳實踐，或修補一個計畫中的瑕疵步驟。
*   它的**檢索 (Retrieval)**，目標也不是找最相關的資料，而是找到最適合當前任務的「計畫」。

##### ④ 與「微調 (Fine-Tuning)」的比較
談到讓代理人「學習如何做」，很多人會立刻想到「微調」。這兩者都旨在改善代理人的行為，但它們的機制和應用場景截然不同。

> **微調**，像是一個緩慢的、離線的訓練過程，它會直接改變模型底層的權重。這就像是把一個人的性格徹底重塑，耗時且成本高昂。
>
> 而**程序性記憶**，提供的是一種快速、線上的適應能力。它不改變模型本身，而是在每次互動中，動態地將正確的「劇本」注入到提示中，透過我們之前提過的「情境中學習 (in-context learning)」，來即時指導代理人。這更像是在特定情境下，給這個人一本操作手冊，讓他立刻知道該怎麼做。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：這是一個比較抽象但非常重要的概念。可以放慢速度，特別是在解釋「推理增強」與「資訊檢索」的區別時，確保聽眾能跟上從「提供數據」到「提供方法」的思維轉變。
*   **強化類比**：「廚師與食譜」或「教練給球員戰術手冊 (playbook)」的類比非常有效，可以在演說中多加利用，幫助聽眾理解。
*   **轉場橋樑 (Bridge)**：
    > 我們已經探討了宣告式記憶和程序性記憶這兩種強大的機制，它們分別讓代理人學會了「知道什麼」和「知道如何做」。但無論我們的記憶系統設計得多麼精巧，我們怎麼能確定它真的有效？它記住的東西是對的嗎？它能在需要時準確找到嗎？這就帶我們進入下一個至關重要的主題：如何科學地「測試與評估」我們的記憶系統。
```</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-27">
            <div class="slide">
                <h2>Slide 27</h2>
                <div class="rendered-content">
                    <h1>Testing and Evaluating Memory Systems</h1>
<p>Evaluation is a multi-layered process to ensure the agent remembers the right things and uses them effectively.</p>
<ol>
<li>
<p><strong>Memory Generation Quality:</strong></p>
<ul>
<li>Is the agent remembering the right things?</li>
<li><strong>Metrics:</strong> Precision, Recall, F1-Score (compared against a &quot;golden set&quot;).</li>
</ul>
</li>
<li>
<p><strong>Memory Retrieval Performance:</strong></p>
<ul>
<li>Can the agent find the right memory at the right time?</li>
<li><strong>Metrics:</strong> Recall@K, Latency.</li>
</ul>
</li>
<li>
<p><strong>End-to-End Task Success:</strong></p>
<ul>
<li>Does memory actually help the agent perform its job better?</li>
<li><strong>Method:</strong> Use an LLM &quot;judge&quot; to compare the agent's final output to a golden answer.</li>
</ul>
</li>
</ol>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 27</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(27, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(27, 'raw')">Source</button>
                </div>
                <div id="note-rendered-27" class="note-content rendered-content">
                    <h3>🎙️ 第 27 頁：Testing_and_Evaluating_Memory_Systems</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>評估一個記憶系統是個多層次的過程，確保代理人「記住對的事」並「有效地使用它們」。</li>
<li><strong>第一層：生成品質</strong>：代理人記住的內容是否正確且相關？使用精確率 (Precision)、召回率 (Recall) 等指標來衡量。</li>
<li><strong>第二層：檢索效能</strong>：代理人能否在需要時，及時找到正確的記憶？關注召回率@K (Recall@K) 和延遲 (Latency)。</li>
<li><strong>第三層：端到端任務成功率</strong>：記憶系統是否真的幫助代理人更好地完成任務？透過 LLM「裁判」來評估最終結果。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，我們已經設計了精巧的記憶生成、整合與檢索機制。但這一切都引出了一個最根本的商業問題：「它真的有用嗎？」我們怎麼知道我們打造的不是一個昂貴又低效的數位垃圾堆？這就是「測試與評估」要回答的問題。</p>
<p>學術界可能專注於可重複的基準測試，但在業界，我們更關心記憶系統如何直接影響一個線上代理人的<strong>效能</strong>和<strong>可用性</strong>。這是一個多層次的評估過程。</p>
<h5>① 第一層：記憶生成的品質 (Memory Generation Quality)</h5>
<p>首先，我們要回答最基本的問題：「代理人記住的東西，是對的嗎？」</p>
<p>這通常是透過將代理人自動生成的記憶，與一個由人工創建的、理想的「黃金標準答案 (golden set)」進行比較來衡量的。</p>
<ul>
<li><strong>精確率 (Precision)</strong>：在所有被記住的內容中，有多少是準確且相關的？高精確率可以防止一個「過度熱心」的記憶系統，用一堆無關的雜訊污染了整個知識庫。</li>
<li><strong>召回率 (Recall)</strong>：在所有應該被記住的關鍵資訊中，代理人成功捕捉了多少？高召回率確保代理人不會錯過那些至關重要的細節。</li>
<li>最後，<strong>F1-Score</strong> 則是這兩者的平衡，提供一個綜合性的品質分數。</li>
</ul>
<h5>② 第二層：記憶檢索的效能 (Memory Retrieval Performance)</h5>
<p>光是記住正確的事情還不夠，代理人還必須能在需要的時候，<strong>及時</strong>且<strong>準確</strong>地找到它。</p>
<blockquote>
<p>這就像你擁有一個完美的圖書館，但如果圖書管理員找不到你要的那本書，或者找一本書要花三小時，那這個圖書館就沒有意義。</p>
</blockquote>
<ul>
<li><strong>Recall@K</strong>：這個指標衡量的是，當需要某個記憶時，正確的答案是否出現在檢索結果的前 'K' 個項目中？這是衡量檢索系統準確性的主要標準。</li>
<li><strong>延遲 (Latency)</strong>：這點至關重要。記憶檢索位於使用者互動的「熱路徑 (hot path)」上，整個過程必須在極其嚴格的延遲預算內完成，例如 <strong>200 毫秒以下</strong>，否則就會嚴重影響使用者體驗，讓你的代理人感覺非常遲鈍。</li>
</ul>
<h5>③ 第三層：端到端的任務成功率 (End-to-End Task Success)</h5>
<p>這是最終極的考驗，回答了那個終極問題：「所以呢？記憶系統到底有沒有幫助代理人把工作做得更好？」</p>
<p>這裡的評估方法非常有趣，我們通常會引入一個 <strong>LLM「裁判」(LLM &quot;judge&quot;)</strong>。我們會讓代理人處理一個任務，然後讓這個「裁判」模型，去比較代理人使用記憶後產生的最終答案和一個「黃金標準答案」。透過這種方式，裁判可以判斷答案是否準確，從而有效地衡量記憶系統對最終結果的貢獻度。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：這頁內容非常務實，可以把它當作一個「品質保證清單」來講解。每個層次都對應一個清晰的商業問題，可以加強語氣來凸顯。</li>
<li><strong>強化類比</strong>：可以把評估記憶系統比作評估一輛車。<strong>生成品質</strong>是檢查引擎零件是否合格；<strong>檢索效能</strong>是測試方向盤和油門是否靈敏；而<strong>端到端任務成功率</strong>，就是實際上路測試，看這輛車能否安全、快速地把你送到目的地。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>了解了如何科學地「評估」我們的記憶系統後，我們就有了衡量成功的標準。但光有標準還不夠，我們還需要確保這個系統能夠在真實世界的壓力下，穩健、安全、且大規模地運行。這就帶我們進入下一個關鍵主題：記憶系統的「生產環境考量」。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-27" class="note-content note-raw" style="display: none;">
                    <pre>```markdown
### 🎙️ 第 27 頁：Testing_and_Evaluating_Memory_Systems

#### 【本頁重點摘要】
*   評估一個記憶系統是個多層次的過程，確保代理人「記住對的事」並「有效地使用它們」。
*   **第一層：生成品質**：代理人記住的內容是否正確且相關？使用精確率 (Precision)、召回率 (Recall) 等指標來衡量。
*   **第二層：檢索效能**：代理人能否在需要時，及時找到正確的記憶？關注召回率@K (Recall@K) 和延遲 (Latency)。
*   **第三層：端到端任務成功率**：記憶系統是否真的幫助代理人更好地完成任務？透過 LLM「裁判」來評估最終結果。

---

#### 【逐字講稿】

(開場白)
好的，我們已經設計了精巧的記憶生成、整合與檢索機制。但這一切都引出了一個最根本的商業問題：「它真的有用嗎？」我們怎麼知道我們打造的不是一個昂貴又低效的數位垃圾堆？這就是「測試與評估」要回答的問題。

學術界可能專注於可重複的基準測試，但在業界，我們更關心記憶系統如何直接影響一個線上代理人的**效能**和**可用性**。這是一個多層次的評估過程。

##### ① 第一層：記憶生成的品質 (Memory Generation Quality)
首先，我們要回答最基本的問題：「代理人記住的東西，是對的嗎？」

這通常是透過將代理人自動生成的記憶，與一個由人工創建的、理想的「黃金標準答案 (golden set)」進行比較來衡量的。

*   **精確率 (Precision)**：在所有被記住的內容中，有多少是準確且相關的？高精確率可以防止一個「過度熱心」的記憶系統，用一堆無關的雜訊污染了整個知識庫。
*   **召回率 (Recall)**：在所有應該被記住的關鍵資訊中，代理人成功捕捉了多少？高召回率確保代理人不會錯過那些至關重要的細節。
*   最後，**F1-Score** 則是這兩者的平衡，提供一個綜合性的品質分數。

##### ② 第二層：記憶檢索的效能 (Memory Retrieval Performance)
光是記住正確的事情還不夠，代理人還必須能在需要的時候，**及時**且**準確**地找到它。

> 這就像你擁有一個完美的圖書館，但如果圖書管理員找不到你要的那本書，或者找一本書要花三小時，那這個圖書館就沒有意義。

*   **Recall@K**：這個指標衡量的是，當需要某個記憶時，正確的答案是否出現在檢索結果的前 'K' 個項目中？這是衡量檢索系統準確性的主要標準。
*   **延遲 (Latency)**：這點至關重要。記憶檢索位於使用者互動的「熱路徑 (hot path)」上，整個過程必須在極其嚴格的延遲預算內完成，例如 **200 毫秒以下**，否則就會嚴重影響使用者體驗，讓你的代理人感覺非常遲鈍。

##### ③ 第三層：端到端的任務成功率 (End-to-End Task Success)
這是最終極的考驗，回答了那個終極問題：「所以呢？記憶系統到底有沒有幫助代理人把工作做得更好？」

這裡的評估方法非常有趣，我們通常會引入一個 **LLM「裁判」(LLM "judge")**。我們會讓代理人處理一個任務，然後讓這個「裁判」模型，去比較代理人使用記憶後產生的最終答案和一個「黃金標準答案」。透過這種方式，裁判可以判斷答案是否準確，從而有效地衡量記憶系統對最終結果的貢獻度。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：這頁內容非常務實，可以把它當作一個「品質保證清單」來講解。每個層次都對應一個清晰的商業問題，可以加強語氣來凸顯。
*   **強化類比**：可以把評估記憶系統比作評估一輛車。**生成品質**是檢查引擎零件是否合格；**檢索效能**是測試方向盤和油門是否靈敏；而**端到端任務成功率**，就是實際上路測試，看這輛車能否安全、快速地把你送到目的地。
*   **轉場橋樑 (Bridge)**：
    > 了解了如何科學地「評估」我們的記憶系統後，我們就有了衡量成功的標準。但光有標準還不夠，我們還需要確保這個系統能夠在真實世界的壓力下，穩健、安全、且大規模地運行。這就帶我們進入下一個關鍵主題：記憶系統的「生產環境考量」。
```</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-28">
            <div class="slide">
                <h2>Slide 28</h2>
                <div class="rendered-content">
                    <h1>Production Considerations for Memory</h1>
<p>Enterprise-grade memory systems require a focus on robustness and scalability.</p>
<ul>
<li>
<p><strong>Asynchronous, Decoupled Architecture:</strong></p>
<ul>
<li>Memory generation is computationally expensive and must run as a <strong>background process</strong>.</li>
<li>The agent makes a non-blocking API call to a dedicated memory service, which handles the heavy lifting asynchronously.</li>
</ul>
</li>
<li>
<p><strong>Concurrency and Resilience:</strong></p>
<ul>
<li>The system must prevent race conditions when multiple events try to modify the same memory (e.g., using transactional operations).</li>
<li>It must be resilient to transient errors, using retry mechanisms and dead-letter queues.</li>
</ul>
</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 28</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(28, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(28, 'raw')">Source</button>
                </div>
                <div id="note-rendered-28" class="note-content rendered-content">
                    <h3>🎙️ 第 28 頁：Production_Considerations_for_Memory</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>從原型到產品，記憶體系統必須專注於「穩健性」與「可擴展性」。</li>
<li><strong>非同步架構</strong>：記憶體生成是昂貴的運算，必須作為「背景處理程序」運行，才不會影響使用者體驗。</li>
<li><strong>並行與韌性</strong>：系統必須能處理多個同時發生的請求（避免競爭條件），並能從暫時的錯誤中恢復（例如使用重試機制和死信佇列）。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，我們已經設計了一個非常聰明的記憶系統。但一個能在實驗室裡運行的酷炫原型，和一個能服務成千上萬用戶的企業級產品，是完全兩回事。現在，我們要談談將記憶系統投入生產環境時，必須面對的工程挑戰——也就是穩健性與可擴展性。</p>
<h5>① 第一個關鍵：非同步、解耦合的架構</h5>
<p>我們首先要解決一個核心問題：<strong>記憶體生成是一個非常昂貴的操作</strong>。它需要呼叫 LLM、進行資料庫寫入，這些都很花時間。如果我們讓使用者在送出訊息後，一直空等，直到記憶體寫入完成才能收到回應，那體驗將會非常糟糕。</p>
<blockquote>
<p>這就是為什麼，一個產品級的記憶體系統，<strong>必須</strong>採用非同步、解耦合的架構。記憶體的生成，應該要在「背景」中悄悄進行。</p>
</blockquote>
<p>這就像在一家高級餐廳點餐：</p>
<ol>
<li>你（代理人）向服務生（記憶體管理器）下訂單（推送原始對話資料），這是一個非阻塞的 API 呼叫。</li>
<li>服務生給你一個號碼牌，立刻確認收到訂單，然後你就可以回到座位上繼續和朋友聊天（使用者體驗不被中斷）。</li>
<li>廚房（記憶體服務的背景程序）開始非同步地進行繁重的烹飪工作：提取、整合、格式化記憶。</li>
<li>最後，烹飪好的菜餚（持久化的記憶）被送到出餐口，等待你下次需要時取用。</li>
</ol>
<p>這種架構確保了即使記憶體生成的管線出現延遲或失敗，也不會直接衝擊到使用者，讓整個系統變得更有<strong>韌性</strong>。</p>
<h5>② 第二個關鍵：並行處理與系統韌性</h5>
<p>當你的應用程式開始成長，同時有數千個請求湧入時，新的問題就出現了。</p>
<p>首先是<strong>並行處理 (Concurrency)</strong>。想像一下，兩個不同的事件，同時嘗試修改同一個用戶的記憶。這就會產生「競爭條件 (race conditions)」，可能會導致資料錯亂。產品級的系統必須透過<strong>交易式資料庫操作</strong>或<strong>樂觀鎖</strong>等機制來防止這種情況。</p>
<p>其次是<strong>系統韌性 (Resilience)</strong>。在真實世界中，錯誤無可避免。LLM 的 API 可能會暫時失敗，網路可能會抖動。我們的系統不能因此就崩潰。</p>
<ul>
<li>對於暫時性錯誤，我們需要有<strong>帶有指數退避的重試機制</strong>。</li>
<li>對於持續失敗的任務，我們不能讓它無限重試，而是應該將它發送到一個「<strong>死信佇列 (dead-letter queue)</strong>」，供工程師事後分析問題，而不會卡住整個系統。</li>
</ul>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>善用類比</strong>：「餐廳點餐」的類比非常有效，可以幫助聽眾直觀地理解為什麼「非同步背景處理」如此重要。</li>
<li><strong>強調重點</strong>：這一頁的核心訊息是，從「能用」到「好用」再到「可靠」，需要的是扎實的後端工程設計。這不只是演算法，更是架構的藝術。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>我們建立了一個穩健、可擴展的架構，確保了系統的「穩定性」。但我們儲存的「記憶」本身是什麼？是使用者的資料。這些資料可能非常敏感。這就引出了我們下一個，也是最重要的一個考量：如何處理記憶系統中的「隱私與安全風險」。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-28" class="note-content note-raw" style="display: none;">
                    <pre>```markdown
### 🎙️ 第 28 頁：Production_Considerations_for_Memory

#### 【本頁重點摘要】
*   從原型到產品，記憶體系統必須專注於「穩健性」與「可擴展性」。
*   **非同步架構**：記憶體生成是昂貴的運算，必須作為「背景處理程序」運行，才不會影響使用者體驗。
*   **並行與韌性**：系統必須能處理多個同時發生的請求（避免競爭條件），並能從暫時的錯誤中恢復（例如使用重試機制和死信佇列）。

---

#### 【逐字講稿】

(開場白)
好的，我們已經設計了一個非常聰明的記憶系統。但一個能在實驗室裡運行的酷炫原型，和一個能服務成千上萬用戶的企業級產品，是完全兩回事。現在，我們要談談將記憶系統投入生產環境時，必須面對的工程挑戰——也就是穩健性與可擴展性。

##### ① 第一個關鍵：非同步、解耦合的架構
我們首先要解決一個核心問題：**記憶體生成是一個非常昂貴的操作**。它需要呼叫 LLM、進行資料庫寫入，這些都很花時間。如果我們讓使用者在送出訊息後，一直空等，直到記憶體寫入完成才能收到回應，那體驗將會非常糟糕。

> 這就是為什麼，一個產品級的記憶體系統，**必須**採用非同步、解耦合的架構。記憶體的生成，應該要在「背景」中悄悄進行。

這就像在一家高級餐廳點餐：
1.  你（代理人）向服務生（記憶體管理器）下訂單（推送原始對話資料），這是一個非阻塞的 API 呼叫。
2.  服務生給你一個號碼牌，立刻確認收到訂單，然後你就可以回到座位上繼續和朋友聊天（使用者體驗不被中斷）。
3.  廚房（記憶體服務的背景程序）開始非同步地進行繁重的烹飪工作：提取、整合、格式化記憶。
4.  最後，烹飪好的菜餚（持久化的記憶）被送到出餐口，等待你下次需要時取用。

這種架構確保了即使記憶體生成的管線出現延遲或失敗，也不會直接衝擊到使用者，讓整個系統變得更有**韌性**。

##### ② 第二個關鍵：並行處理與系統韌性
當你的應用程式開始成長，同時有數千個請求湧入時，新的問題就出現了。

首先是**並行處理 (Concurrency)**。想像一下，兩個不同的事件，同時嘗試修改同一個用戶的記憶。這就會產生「競爭條件 (race conditions)」，可能會導致資料錯亂。產品級的系統必須透過**交易式資料庫操作**或**樂觀鎖**等機制來防止這種情況。

其次是**系統韌性 (Resilience)**。在真實世界中，錯誤無可避免。LLM 的 API 可能會暫時失敗，網路可能會抖動。我們的系統不能因此就崩潰。
*   對於暫時性錯誤，我們需要有**帶有指數退避的重試機制**。
*   對於持續失敗的任務，我們不能讓它無限重試，而是應該將它發送到一個「**死信佇列 (dead-letter queue)**」，供工程師事後分析問題，而不會卡住整個系統。

---

#### 【講者提示 & 轉場】
*   **善用類比**：「餐廳點餐」的類比非常有效，可以幫助聽眾直觀地理解為什麼「非同步背景處理」如此重要。
*   **強調重點**：這一頁的核心訊息是，從「能用」到「好用」再到「可靠」，需要的是扎實的後端工程設計。這不只是演算法，更是架構的藝術。
*   **轉場橋樑 (Bridge)**：
    > 我們建立了一個穩健、可擴展的架構，確保了系統的「穩定性」。但我們儲存的「記憶」本身是什麼？是使用者的資料。這些資料可能非常敏感。這就引出了我們下一個，也是最重要的一個考量：如何處理記憶系統中的「隱私與安全風險」。
```</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-29">
            <div class="slide">
                <h2>Slide 29</h2>
                <div class="rendered-content">
                    <h1>Privacy and Security Risks in Memory</h1>
<p>Memories contain user data and require stringent controls.</p>
<p><strong>Analogy: The Corporate Archivist</strong></p>
<ul>
<li>The archivist's job is to preserve knowledge while protecting the company.</li>
</ul>
<p><strong>Key Safeguards:</strong></p>
<ul>
<li><strong>Data Isolation:</strong> Strictly enforce user-level isolation with Access Control Lists (ACLs).</li>
<li><strong>PII Redaction:</strong> Sanitize and redact sensitive personal information <em>before</em> it is committed to memory.</li>
<li><strong>Memory Poisoning Prevention:</strong> Validate information to prevent a malicious user from corrupting the agent's knowledge via prompt injection.</li>
<li><strong>Exfiltration Risk Mitigation:</strong> Rigorously anonymize shared memories (like procedural ones) to prevent data leaks.</li>
</ul>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 29</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(29, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(29, 'raw')">Source</button>
                </div>
                <div id="note-rendered-29" class="note-content rendered-content">
                    <h3>🎙️ 第 29 頁：Privacy and Security Risks in Memory Systems</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li>記憶系統儲存著用戶數據，因此必須用最嚴格的標準來對待其隱私與安全。</li>
<li>我們可以將記憶系統比喻為一個由專業檔案管理員看管的「企業安全檔案庫」。</li>
<li>關鍵的安全措施包括：
<ul>
<li><strong>資料隔離</strong>：嚴格區分不同用戶的資料，絕不混用。</li>
<li><strong>PII 匿名化</strong>：在資訊存入記憶體「之前」，就必須清除或遮蔽個資。</li>
<li><strong>記憶汙染防範</strong>：驗證資訊來源，防止惡意用戶透過提示注入來破壞代理人的知識庫。</li>
<li><strong>資料外洩風險緩解</strong>：在共享記憶（如程序性記憶）時，必須進行嚴格的匿名化處理。</li>
</ul>
</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
好的，我們已經建立了一個強大的記憶系統，但隨之而來的是巨大的責任。因為記憶源於用戶數據，所以它需要最嚴格的隱私和安全控制。要理解這一點，最好的方式就是把我們的記憶系統想像成一個「企業的安全檔案庫」，而我們就是那位專業的檔案管理員，職責不僅是保存知識，更是保護公司。</p>
<h5>① 首要原則：資料隔離 (Data Isolation)</h5>
<p>這個檔案庫的第一條、也是最神聖的規則，就是<strong>資料隔離</strong>。</p>
<blockquote>
<p>就像檔案管理員絕對不會把不同部門的機密文件混在一起，我們的記憶系統也必須在用戶或租戶層級進行嚴格的隔離。一個為用戶 A 服務的代理人，絕對不能存取到用戶 B 的記憶。</p>
</blockquote>
<p>這必須透過嚴格的存取控制列表 (ACLs) 來強制執行。此外，用戶必須擁有對自己數據的控制權，他們應該可以選擇退出記憶生成，或要求刪除他們在檔案庫中的所有資料。</p>
<h5>② 存檔前的兩道安檢</h5>
<p>在任何文件被歸檔之前，這位盡責的檔案管理員會執行兩個關鍵的安全步驟。</p>
<p>第一，他會仔細檢查每一頁，<strong>匿名化或編輯掉所有敏感的個人資訊 (PII)</strong>。確保知識被保存下來的同時，不會製造法律或信任上的風險。</p>
<p>第二，他會<strong>驗證這份文件的真偽</strong>，丟棄那些偽造或故意誤導的資訊。這就是為了防範所謂的「記憶汙染 (Memory Poisoning)」。我們必須在資訊被提交到長期記憶之前，就對其進行驗證和清理，防止惡意用戶透過「提示注入 (prompt injection)」來汙染或操縱我們代理人的核心知識。</p>
<h5>③ 特殊風險：共享記憶的資料外洩 (Exfiltration Risk)</h5>
<p>最後，還有一種比較隱蔽的風險。當多個用戶共享同一組記憶時——最典型的例子就是我們之前提到的「程序性記憶」——就會產生資料外洩的風險。</p>
<p>想像一下，如果系統把從用戶 A 的成功經驗中學到的「操作手冊」分享給用戶 B 作為範例，那麼檔案管理員就必須先執行嚴格的<strong>匿名化處理</strong>。確保在分享知識的同時，不會意外洩露任何屬於用戶 A 的敏感資訊。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：這是一個嚴肅且至關重要的主題。演說時應帶有權威性和謹慎的語氣，強調這些不僅是技術細節，更是建立用戶信任和企業責任的基石。</li>
<li><strong>強化類比</strong>：「檔案管理員」這個類比非常貼切，可以貫穿整個演說，用它來解釋隔離、匿名化和驗證等概念，會讓聽眾更容易理解。</li>
<li><strong>轉場橋樑 (Bridge)</strong>：
<blockquote>
<p>我們已經探討了如何建構、管理，以及最重要的，如何保護我們的記憶系統。從對話的開始到知識的永久保存，我們走過了一段完整的旅程。現在，讓我們來到最後一頁，總結我們今天學到的所有關鍵概念，並再次強調 Context Engineering 的核心價值。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-29" class="note-content note-raw" style="display: none;">
                    <pre>```markdown
### 🎙️ 第 29 頁：Privacy and Security Risks in Memory Systems

#### 【本頁重點摘要】
*   記憶系統儲存著用戶數據，因此必須用最嚴格的標準來對待其隱私與安全。
*   我們可以將記憶系統比喻為一個由專業檔案管理員看管的「企業安全檔案庫」。
*   關鍵的安全措施包括：
    *   **資料隔離**：嚴格區分不同用戶的資料，絕不混用。
    *   **PII 匿名化**：在資訊存入記憶體「之前」，就必須清除或遮蔽個資。
    *   **記憶汙染防範**：驗證資訊來源，防止惡意用戶透過提示注入來破壞代理人的知識庫。
    *   **資料外洩風險緩解**：在共享記憶（如程序性記憶）時，必須進行嚴格的匿名化處理。

---

#### 【逐字講稿】

(開場白)
好的，我們已經建立了一個強大的記憶系統，但隨之而來的是巨大的責任。因為記憶源於用戶數據，所以它需要最嚴格的隱私和安全控制。要理解這一點，最好的方式就是把我們的記憶系統想像成一個「企業的安全檔案庫」，而我們就是那位專業的檔案管理員，職責不僅是保存知識，更是保護公司。

##### ① 首要原則：資料隔離 (Data Isolation)
這個檔案庫的第一條、也是最神聖的規則，就是**資料隔離**。

> 就像檔案管理員絕對不會把不同部門的機密文件混在一起，我們的記憶系統也必須在用戶或租戶層級進行嚴格的隔離。一個為用戶 A 服務的代理人，絕對不能存取到用戶 B 的記憶。

這必須透過嚴格的存取控制列表 (ACLs) 來強制執行。此外，用戶必須擁有對自己數據的控制權，他們應該可以選擇退出記憶生成，或要求刪除他們在檔案庫中的所有資料。

##### ② 存檔前的兩道安檢
在任何文件被歸檔之前，這位盡責的檔案管理員會執行兩個關鍵的安全步驟。

第一，他會仔細檢查每一頁，**匿名化或編輯掉所有敏感的個人資訊 (PII)**。確保知識被保存下來的同時，不會製造法律或信任上的風險。

第二，他會**驗證這份文件的真偽**，丟棄那些偽造或故意誤導的資訊。這就是為了防範所謂的「記憶汙染 (Memory Poisoning)」。我們必須在資訊被提交到長期記憶之前，就對其進行驗證和清理，防止惡意用戶透過「提示注入 (prompt injection)」來汙染或操縱我們代理人的核心知識。

##### ③ 特殊風險：共享記憶的資料外洩 (Exfiltration Risk)
最後，還有一種比較隱蔽的風險。當多個用戶共享同一組記憶時——最典型的例子就是我們之前提到的「程序性記憶」——就會產生資料外洩的風險。

想像一下，如果系統把從用戶 A 的成功經驗中學到的「操作手冊」分享給用戶 B 作為範例，那麼檔案管理員就必須先執行嚴格的**匿名化處理**。確保在分享知識的同時，不會意外洩露任何屬於用戶 A 的敏感資訊。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：這是一個嚴肅且至關重要的主題。演說時應帶有權威性和謹慎的語氣，強調這些不僅是技術細節，更是建立用戶信任和企業責任的基石。
*   **強化類比**：「檔案管理員」這個類比非常貼切，可以貫穿整個演說，用它來解釋隔離、匿名化和驗證等概念，會讓聽眾更容易理解。
*   **轉場橋樑 (Bridge)**：
    > 我們已經探討了如何建構、管理，以及最重要的，如何保護我們的記憶系統。從對話的開始到知識的永久保存，我們走過了一段完整的旅程。現在，讓我們來到最後一頁，總結我們今天學到的所有關鍵概念，並再次強調 Context Engineering 的核心價值。

```</pre>
                </div>
            </div>
        </div>
        
        <div class="page" id="page-30">
            <div class="slide">
                <h2>Slide 30</h2>
                <div class="rendered-content">
                    <h1>Conclusion &amp; Key Takeaways</h1>
<p><strong>Context Engineering</strong> is the discipline of building stateful, intelligent agents.</p>
<ul>
<li>
<p><strong>Sessions govern the &quot;now&quot;:</strong></p>
<ul>
<li>A low-latency, chronological container for a single conversation.</li>
<li>Requires compaction strategies to manage cost and performance.</li>
</ul>
</li>
<li>
<p><strong>Memory is the engine of long-term personalization:</strong></p>
<ul>
<li>An active, LLM-driven ETL pipeline (Extract, Consolidate, Retrieve).</li>
<li>Makes the agent an expert on the <strong>user</strong>, complementing RAG which makes it an expert on <strong>facts</strong>.</li>
<li>Must run as an asynchronous background process in production.</li>
</ul>
</li>
</ul>
<p>By mastering Sessions and Memory, developers can build trusted, adaptive assistants that truly learn and grow with the user.</p>

                </div>
            </div>
            <div class="notes">
                <h2>Notes for Slide 30</h2>
                <div class="note-toggle">
                    <button class="toggle-btn active" onclick="toggleNoteView(30, 'rendered')">Rendered</button>
                    <button class="toggle-btn" onclick="toggleNoteView(30, 'raw')">Source</button>
                </div>
                <div id="note-rendered-30" class="note-content rendered-content">
                    <h3>🎙️ 第 30 頁：Conclusion &amp; Key Takeaways</h3>
<h4>【本頁重點摘要】</h4>
<ul>
<li><strong>情境工程 (Context Engineering)</strong> 是建立有狀態、有智慧的 AI 代理人的核心準則。</li>
<li><strong>對話 (Sessions)</strong> 負責管理「當下」的互動，它是一個短暫、高速的對話容器，需要透過壓縮策略來維持效能。</li>
<li><strong>記憶 (Memory)</strong> 是實現長期個人化的引擎，它是一個由 AI 驅動的 ETL 流程，讓代理人從「事實專家」轉變為「用戶專家」。</li>
<li>掌握這兩者，我們就能打造出能與用戶一同學習和成長的、值得信賴的智慧助理。</li>
</ul>
<hr />
<h4>【逐字講稿】</h4>
<p>(開場白)
各位，我們今天的旅程即將來到終點。我們從一個最基本的問題開始：如何讓天生健忘的語言模型，擁有記憶、學會理解我們？答案，就是我們今天探討的核心——<strong>情境工程 (Context Engineering)</strong>。</p>
<h5>① 宏觀的藍圖：情境工程 (Context Engineering)</h5>
<p>情境工程，就是建立有狀態、有智慧的代理人的宏觀藍圖。它不僅僅是提示詞的藝術，更是動態地組合、管理模型所需一切資訊的科學。它確保我們的 AI 代理人，在每一次互動中，都擁有不多不少、恰到好處的上下文來完成任務。</p>
<h5>② 專注於「當下」的工作檯：對話 (Sessions)</h5>
<p>在這張藍圖下，第一個核心組件是 <strong>對話 (Sessions)</strong>。它掌管著「當下」。</p>
<blockquote>
<p>你可以把它想像成我們為了一個專案而使用的<strong>臨時工作檯</strong>。上面擺滿了所有立即需要的工具和筆記，反應迅速、專注於當前任務。</p>
</blockquote>
<p>但當對話越來越長，這個工作檯就會變得混亂。這就是為什麼我們需要像<strong>打包行李箱</strong>一樣的<strong>壓縮策略</strong>，只保留最重要的東西，確保效率與效能。</p>
<h5>③ 打造「未來」的個人助理：記憶 (Memory)</h5>
<p>而第二個、也是更具變革性的組件，就是<strong>記憶 (Memory)</strong>。如果說對話是臨時的工作檯，那記憶就是我們精心整理、可以永久查閱的<strong>檔案櫃</strong>，更是我們 AI 的<strong>專屬個人助理</strong>。</p>
<p>它讓代理人從一個博學的<strong>事實專家 (RAG)</strong>，轉變為一個懂你的<strong>用戶專家 (Memory)</strong>。這個過程並非簡單的儲存，而是一個由大型語言模型驅動的、主動的 <strong>ETL 流程</strong>——就像一位<strong>園丁</strong>，不斷地從對話中<strong>提取</strong>新芽、<strong>整合</strong>到現有花園中、並在未來需要時<strong>檢索</strong>出來。最關鍵的是，在正式產品中，這位園丁必須在<strong>背景異步工作</strong>，才不會影響到用戶的體驗。</p>
<p>(結語)
透過掌握對話與記憶這兩大支柱，我們就能打造出真正值得信賴、能夠適應、並與用戶一同成長的智慧助理。這不僅僅是技術的演進，更是通往真正個人化 AI 時代的鑰匙。</p>
<hr />
<h4>【講者提示 &amp; 轉場】</h4>
<ul>
<li><strong>節奏提醒</strong>：作為總結，語氣應充滿自信與前瞻性。可以稍微放慢速度，讓每個關鍵詞都清晰有力地傳達給聽眾。</li>
<li><strong>強化記憶點</strong>：可以快速回顧今天提到的幾個核心比喻：「工作檯」(Session)、「行李箱」(Compaction)、「個人助理」(Memory) vs. 「研究圖書館員」(RAG)，以及「園丁」(Memory Generation)，這些能幫助聽眾牢牢記住核心概念。</li>
<li><strong>結束與問答 (Ending &amp; Q&amp;A)</strong>：
<blockquote>
<p>這就是我們通往有狀態智慧代理人的路徑圖。謝謝大家。現在，我非常樂意回答各位的問題。</p>
</blockquote>
</li>
</ul>

                </div>
                <div id="note-raw-30" class="note-content note-raw" style="display: none;">
                    <pre>```markdown
### 🎙️ 第 30 頁：Conclusion & Key Takeaways

#### 【本頁重點摘要】
*   **情境工程 (Context Engineering)** 是建立有狀態、有智慧的 AI 代理人的核心準則。
*   **對話 (Sessions)** 負責管理「當下」的互動，它是一個短暫、高速的對話容器，需要透過壓縮策略來維持效能。
*   **記憶 (Memory)** 是實現長期個人化的引擎，它是一個由 AI 驅動的 ETL 流程，讓代理人從「事實專家」轉變為「用戶專家」。
*   掌握這兩者，我們就能打造出能與用戶一同學習和成長的、值得信賴的智慧助理。

---

#### 【逐字講稿】

(開場白)
各位，我們今天的旅程即將來到終點。我們從一個最基本的問題開始：如何讓天生健忘的語言模型，擁有記憶、學會理解我們？答案，就是我們今天探討的核心——**情境工程 (Context Engineering)**。

##### ① 宏觀的藍圖：情境工程 (Context Engineering)
情境工程，就是建立有狀態、有智慧的代理人的宏觀藍圖。它不僅僅是提示詞的藝術，更是動態地組合、管理模型所需一切資訊的科學。它確保我們的 AI 代理人，在每一次互動中，都擁有不多不少、恰到好處的上下文來完成任務。

##### ② 專注於「當下」的工作檯：對話 (Sessions)
在這張藍圖下，第一個核心組件是 **對話 (Sessions)**。它掌管著「當下」。

> 你可以把它想像成我們為了一個專案而使用的**臨時工作檯**。上面擺滿了所有立即需要的工具和筆記，反應迅速、專注於當前任務。

但當對話越來越長，這個工作檯就會變得混亂。這就是為什麼我們需要像**打包行李箱**一樣的**壓縮策略**，只保留最重要的東西，確保效率與效能。

##### ③ 打造「未來」的個人助理：記憶 (Memory)
而第二個、也是更具變革性的組件，就是**記憶 (Memory)**。如果說對話是臨時的工作檯，那記憶就是我們精心整理、可以永久查閱的**檔案櫃**，更是我們 AI 的**專屬個人助理**。

它讓代理人從一個博學的**事實專家 (RAG)**，轉變為一個懂你的**用戶專家 (Memory)**。這個過程並非簡單的儲存，而是一個由大型語言模型驅動的、主動的 **ETL 流程**——就像一位**園丁**，不斷地從對話中**提取**新芽、**整合**到現有花園中、並在未來需要時**檢索**出來。最關鍵的是，在正式產品中，這位園丁必須在**背景異步工作**，才不會影響到用戶的體驗。

(結語)
透過掌握對話與記憶這兩大支柱，我們就能打造出真正值得信賴、能夠適應、並與用戶一同成長的智慧助理。這不僅僅是技術的演進，更是通往真正個人化 AI 時代的鑰匙。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：作為總結，語氣應充滿自信與前瞻性。可以稍微放慢速度，讓每個關鍵詞都清晰有力地傳達給聽眾。
*   **強化記憶點**：可以快速回顧今天提到的幾個核心比喻：「工作檯」(Session)、「行李箱」(Compaction)、「個人助理」(Memory) vs. 「研究圖書館員」(RAG)，以及「園丁」(Memory Generation)，這些能幫助聽眾牢牢記住核心概念。
*   **結束與問答 (Ending & Q&A)**：
    > 這就是我們通往有狀態智慧代理人的路徑圖。謝謝大家。現在，我非常樂意回答各位的問題。
```</pre>
                </div>
            </div>
        </div>
        
    </div>

    <script>
        function toggleNoteView(pageIndex, viewType) {
            const pageElement = document.getElementById('page-' + pageIndex);
            if (!pageElement) return;

            const renderedView = pageElement.querySelector('#note-rendered-' + pageIndex);
            const rawView = pageElement.querySelector('#note-raw-' + pageIndex);
            const renderedBtn = pageElement.querySelector('button[onclick*="'rendered'"]');
            const rawBtn = pageElement.querySelector('button[onclick*="'raw'"]');

            if (viewType === 'rendered') {
                renderedView.style.display = 'block';
                rawView.style.display = 'none';
                renderedBtn.classList.add('active');
                rawBtn.classList.remove('active');
            } else {
                renderedView.style.display = 'none';
                rawView.style.display = 'block';
                rawBtn.classList.add('active');
                renderedBtn.classList.remove('active');
            }
        }
    </script>
</body>
</html>