```markdown
### 🎙️ 第 13 頁：A_Three-Layer_Defense_for_AI_Agent_Security

#### 【本頁重點摘要】
*   介紹了 Google 的安全 AI 框架 (SAIF)，為 AI 代理提供一個三層式的深度防禦策略。
*   第一層是**政策定義**，如同為代理制定一部「憲法」，確立其行為準則。
*   第二層是**強制執行**，透過輸入/輸出過濾和人工審核，建立強硬的防護邊界。
*   第三層是**持續驗證**，強調安全是一個動態過程，需要透過不斷的評估、測試和紅隊演練來鞏固。

---

#### 【逐字講稿】

(開場白)
各位，我們剛剛談到了 AI 代理因為其自主性而帶來的獨特風險，像是提示詞注入、資料外洩等等。聽起來可能有點嚇人，但好消息是，我們並非束手無策。事實上，業界已經發展出非常成熟的防禦框架，今天我們要介紹的就是一個黃金標準：一個三層式的深度防禦模型。

這套方法論主要參考了 Google 的 **Secure AI Agents** 理念以及 **SAIF (Secure AI Framework)**，它將安全性從一個模糊的概念，轉化為三個具體、可執行的防禦層次。

##### ① 第一層：政策定義與系統指令 (代理的憲法)
這一切都始於為你的 AI 代理制定一部「憲法」。這不是比喻，而是實實在在的工程。我們必須明確定義**什麼是期望的行為**，以及**什麼是絕對不允許的行為**。這些規則會被寫入代理的核心——也就是它的「系統指令」(System Instructions) 中。

> 這部憲法，就是代理所有決策和行動的最高指導原則。它告訴代理：「無論你多麼聰明，都不能逾越這些底線。」

##### ② 第二層：防護、保障與過濾 (執法層)
有了憲法，還需要強而有力的執法單位。這一層就是代理的**硬邊界**和**過濾系統**，確保所有進出的資訊都符合規範。

*   **輸入過濾**：在惡意指令接觸到代理之前就攔截它。例如，我們可以使用像 **Perspective API** 這樣的分類器服務，來分析使用者輸入的提示詞，自動阻擋那些看起來像是在進行攻擊或誘導的內容。
*   **輸出過濾**：在代理產生回應、準備送出給使用者之前，進行最後一道安檢。例如，**Vertex AI 內建的安全過濾器**就可以在這裡發揮作用，它能檢查回應中是否包含有害內容、個人隱私資訊 (PII) 或其他違反政策的言論，並在必要時直接攔截。
*   **人工介入 (Human-in-the-Loop)**：對於那些高風險或模稜兩可的指令，系統不應該擅自作主。最好的做法是**暫停執行**，並將決策權交還給人類審核員。這就像是需要法官裁決的重大案件，確保萬無一失。

##### ③ 第三層：持續驗證與測試 (動態防禦)
最後，也是最重要的一點：**安全不是一次性的設定，而是一個持續的過程**。威脅會不斷演變，我們也必須隨之進化。

*   **嚴格的評估**：每當我們對模型、工具或安全系統進行任何微小的改動，都必須重新運行一整套完整的評估流程，確保防護沒有出現漏洞。
*   **專門的 RAI 測試**：我們需要針對已知的特定風險（例如偏見、公平性）進行專門的測試，這可以透過建立專門的資料集或使用模擬代理來完成。
*   **主動的紅隊演練 (Red Teaming)**：這一步非常有趣，我們不再只是被動防守，而是要**主動攻擊**自己的系統。透過富有創意的真人手動測試，或是由 AI 驅動的、模擬各種惡意角色的攻擊演練，來找出潛在的弱點，並在它們被壞人利用之前加以修補。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：這三層防禦的資訊量較大，講完每一層後可以稍微停頓一下，讓聽眾消化吸收。特別是在講到「紅隊演練」時，可以用更生動的語氣來描述「自己攻擊自己」的概念。
*   **補充案例**：可以強調，這裡提到的 Perspective API 和 Vertex AI Safety Filters 都是真實世界中可以使用的工具，這不是純理論，而是已經在實踐中的最佳做法。
*   **轉場橋樑 (Bridge)**：
    > 透過這三層防禦，我們等於是為 AI 代理在部署前穿上了一套堅固的盔甲。但真正的戰場，是在它上線之後。當代理開始與成千上萬的真實用戶互動時，我們又該如何確保它的安全與穩定呢？下一頁，我們將進入「生產環境中的維運」，探討著名的「觀察-行動-演進」循環。

```