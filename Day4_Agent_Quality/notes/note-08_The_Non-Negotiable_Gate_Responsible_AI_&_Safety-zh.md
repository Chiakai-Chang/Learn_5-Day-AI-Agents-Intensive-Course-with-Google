### 🎙️ 第 08 頁：責任制 AI 與安全性：不容妥協的關卡

#### 【本頁重點摘要】
*   一個 Agent 即使效能再好，一旦造成傷害，就是徹底的失敗。安全性是生產環境中不容妥協的強制性關卡。
*   安全評估的三大關鍵實踐：系統性紅隊演練、自動化過濾結合人工審核、以及遵循明確的倫理準則。
*   效能指標決定 Agent 的「能力」，而安全評估決定其「應為之事」。

---

#### 【逐字講稿】

(開場白)
各位，到目前為止，我們討論了很多關於如何評估 Agent「做得好不好」的方法。但現在，我們要談一個更根本的問題：如何確保我們的 Agent「不會做壞事」。

一個 100% 有效但會造成傷害的 Agent，是一個徹底的失敗品。因此，**安全性不是一個可選的附加功能，而是進入生產環境前，一道必須通過、絕不妥協的關卡。**

那麼，具體該怎麼做呢？這裡有三個關鍵的實踐。

##### ① 系統性紅隊演練 (Systematic Red Teaming)
第一個，也是最積極主動的一步，叫做「紅隊演練」。這不是被動地等待問題發生，而是主動地、系統性地去攻擊我們自己的 Agent，試圖讓它崩潰或做出不當行為。

這包括模擬各種惡意場景，例如：
*   誘導它產生仇恨言論或有害內容。
*   嘗試讓它洩漏訓練數據中的個人隱私資訊。
*   引誘它執行一些惡意的、非預期的指令。

只有透過這種方式，我們才能在真實世界的惡意使用者發現漏洞之前，自己先找到並修補它們。

##### ② 自動化過濾與人工審核 (Automated Filters & Human Review)
第二，我們需要一個雙層防護網。

首先是 **自動化過濾器**。這就像是我們系統的免疫系統，可以即時攔截已知的風險。例如，我們可以在模型處理請求**之前**，就部署一個分類器來檢測「提示詞注入攻擊」；在模型生成回應**之後**，再用另一個工具來掃描是否包含個資 (PII)。

但自動化並非萬能。對於那些更細微、更隱晦的偏見或潛在的冒犯性內容，機器很難判斷。這就需要 **人工審核** 的介入。人類專家能夠理解文化脈絡、解讀言外之意，捕捉那些自動化系統會錯過的灰色地帶。

##### ③ 遵循準則 (Adherence to Guidelines)
最後，我們必須明確定義 Agent 的「行為準則」與「倫理邊界」，並且嚴格評估它的所有產出是否都符合這些準則。這確保了 Agent 的行為不僅合法合規，更能作為我們組織的可靠代理人，其言行與我們的核心價值觀保持一致。

> 效能指標告訴我們，這個 Agent *能不能* 完成工作。但安全評估告訴我們，它 *應不應該* 去做。

---

#### 【講者提示 & 轉場】
*   **節奏提醒**：在唸完 blockquote 中的那句總結時，請務必停頓三秒。這句話是本頁的核心，需要時間讓聽眾消化。
*   **補充案例**：可以舉例，就像一輛自動駕駛汽車，它的效能指標是「能否準時到達目的地」，但它的安全評估是「能否在遵守交通規則、不造成任何危險的情況下到達」。兩者缺一不可。
*   **轉場橋樑 (Bridge)**：
    > 我們已經定義了評估的框架，也強調了安全這個不容妥協的大門。但無論是效能評估還是安全檢查，它們都需要一個共同的基礎：數據。如果我們看不見 Agent 的決策過程，我們就無法評估它。下一頁，我們將探討如何「看見」這一切，也就是「可觀測性」(Observability)。